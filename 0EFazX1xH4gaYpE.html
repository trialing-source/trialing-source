<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-select/1.13.18/js/bootstrap-select.min.js" integrity="sha512-yDlE7vpGDP7o2eftkCiPZ+yuUyEcaBwoJoIhdXv71KZWugFqEphIS3PU60lEkFaz8RxaVsMpSvQxMBaKVwA5xg==" crossorigin="anonymous"></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>

    <script src="https://bossanova.uk/jexcel/v4/jexcel.js"></script>
    <script src="https://jsuites.net/v4/jsuites.js"></script>
    <link href="css/styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://jsuites.net/v4/jsuites.css" type="text/css" />
    <link rel="stylesheet" href="css/sheet.css">
    <link rel="stylesheet" href="css/excel.css">
    <link rel="stylesheet" href="css/darktheme.css">

    <script src="https://cdn.jsdelivr.net/npm/showdown@1.9.0/dist/showdown.min.js"></script>



    <script>
    </script>

</head>


<body>

    <div class="container-fluid">
        <div id="table-wrapper">
            <input type="text" id="myInput" onkeyup="myFunction()" placeholder="Search">
            <div id='test'>
                <div id="spreadsheet"></div>
            </div>

        </div>


        <div class="modal " id="desc_modal" tabindex="-1" role="dialog" aria-labelledby="exampleModalLabel" aria-hidden="true">
            <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                <div class="modal-content">

                    <div class="modal-body">
                        <div class="container-fluid">
                            <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>

                            <h4 class="modal-title" id='mtitle' style='padding-bottom:40px; text-align: left;'></h4>

                            <div id='mbody'></div>

                        </div>
                    </div>
                    <div class="modal-footer">
                    </div>

                </div>
            </div>
        </div>
        is


        <script>
            function myFunction() {
                // Declare variables
                sp.setData(data_all);
                var input, filter, table, tr, td, i, txtValue, data, found_names;
                input = document.getElementById("myInput");
                data = sp.getData();
                found_names = $.grep(data, function(v) {
                    return v[0].match(input.value) || v[1].match(input.value) || v[2].match(input.value);
                });
                sp.setData(found_names);
            };


            var desc = [['**Apache Airflow version**: 1.10.9\r\n\r\n\r\n**Environment**:\r\n\r\n- **Cloud provider or hardware configuration**: GCP\r\n- **OS** (e.g. from /etc/os-release): Debian\r\n\r\n\r\n**What happened**:\r\n\r\nIt has been 2 weeks that I\'m facing a strange issue and my DAG. Most of them start like this. A colleague upload manually a file into a GCS bucket, a cloud fonction is triggered at the end to launch from the API an Airflow DAG. The first task of the DAG is to transfert the file from a "landing zone" into a "save zone", then there rest of the DAG continue.\r\n\r\nI move the file from bucket A to B with the GoogleCloudStorageToGoogleCloudStorageOperator. Everything was working since 2 or 3 weeks ago. The oldest DAG is 6 months old and even if we have change some stuff it\'s on the other part of the DAG, so we never touch that part for long with correct behaviour.\r\n\r\nNow, most of the time the first task, the transfert, fail. The file is well moved, but I don\'t know what happen but I have this error 2 or 3 time in a row if I retry, and the very next try, with the EXACT same file... it\'s working. I cannot find the factor that gives me this issue. I\'m getting crazy.\r\n\r\n```\r\n[2020-10-15 09:34:50,599] {taskinstance.py:868} INFO - \r\n--------------------------------------------------------------------------------\r\n[2020-10-15 09:34:50,620] {taskinstance.py:887} INFO - Executing <Task(GoogleCloudStorageToGoogleCloudStorageOperator): transfer-landing-to-safe> on 2020-10-15T07:34:40+00:00\r\n[2020-10-15 09:34:50,626] {standard_task_runner.py:53} INFO - Started process 31555 to run task\r\n[2020-10-15 09:34:50,775] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: geometrie-preprocessing.transfer-landing-to-safe 2020-10-15T07:34:40+00:00 [running]> blablabla.internal\r\n[2020-10-15 09:34:50,860] {gcs_to_gcs.py:193} INFO - Executing copy of gs://blablabla-landing/geometrie/Track_Geometry-20201005_032915.csv to gs://blablabla-safe/geometrie/original/track_geometry_20201005_032915.csv\r\n[2020-10-15 09:34:50,861] {logging_mixin.py:112} INFO - [2020-10-15 09:34:50,860] {gcp_api_base_hook.py:146} INFO - Getting connection using `google.auth.default()` since no key file is defined for hook.\r\n[2020-10-15 09:34:50,980] {taskinstance.py:1128} ERROR - 404 POST https://storage.googleapis.com/storage/v1/b/blablabla-landing/o/geometrie%2FTrack_Geometry-20201005_032915.csv/rewriteTo/b/blablabla-safe/o/geometrie%2Foriginal%2Ftrack_geometry_20201005_032915.csv: No such object: blablabla-landing/geometrie/Track_Geometry-20201005_032915.csv\r\nTraceback (most recent call last):\r\n  File "/usr/local/lib/python3.7/dist-packages/airflow/models/taskinstance.py", line 966, in _run_raw_task\r\n    result = task_copy.execute(context=context)\r\n  File "/usr/local/lib/python3.7/dist-packages/airflow/contrib/operators/gcs_to_gcs.py", line 178, in execute\r\n    destination_object=self.destination_object)\r\n  File "/usr/local/lib/python3.7/dist-packages/airflow/contrib/operators/gcs_to_gcs.py", line 196, in _copy_single_object\r\n    self.destination_bucket, destination_object)\r\n  File "/usr/local/lib/python3.7/dist-packages/airflow/contrib/hooks/gcs_hook.py", line 135, in rewrite\r\n    source=source_object\r\n  File "/usr/local/lib/python3.7/dist-packages/google/cloud/storage/blob.py", line 2098, in rewrite\r\n    timeout=timeout,\r\n  File "/usr/local/lib/python3.7/dist-packages/google/cloud/_http.py", line 423, in api_request\r\n    raise exceptions.from_http_response(response)\r\ngoogle.api_core.exceptions.NotFound: 404 POST https://storage.googleapis.com/storage/v1/b/blablabla-landing/o/geometrie%2FTrack_Geometry-20201005_032915.csv/rewriteTo/b/blablabla-safe/o/geometrie%2Foriginal%2Ftrack_geometry_20201005_032915.csv: No such object: blablabla-landing/geometrie/Track_Geometry-20201005_032915.csv\r\n[2020-10-15 09:34:50,984] {taskinstance.py:1185} INFO - Marking task as FAILED.dag_id=geometrie-preprocessing, task_id=transfer-landing-to-safe, execution_date=20201015T073440, start_date=20201015T073450, end_date=20201015T073450\r\n[2020-10-15 09:35:00,556] {logging_mixin.py:112} INFO - [2020-10-15 09:35:00,556] {local_task_job.py:103} INFO - Task exited with return code 1\r\n```\r\n\r\nThe operator part :\r\n\r\n```\r\ntransfer_landing_to_safe = GoogleCloudStorageToGoogleCloudStorageOperator(\r\n        task_id=f"transfer-landing-to-safe{env_extension}",\r\n        source_bucket=f"blablabla-landing{env_extension}",\r\n        source_object="{{ dag_run.conf[\'file_name\'] }}",\r\n        destination_bucket=f"blablabla-safe{env_extension}",\r\n        destination_object="geometrie/original/track_geometry_{{ dag_run.conf[\'file_name\'][-19:] }}",\r\n        move_object=True,\r\n        google_cloud_storage_conn_id="gcp_conn"\r\n    )\r\n\r\n```\r\n\r\nI added  "en masse" last night 25 files from python (so using the API instead of dropping manually) and no errors.\r\n\r\nI\'m not sure if it\'s an Airflow/Operator issue or something with GCP console.'], ['Bots currently require an email-field upon creation which has no use except for referencing or searching the object. It is unnecessary to enter email field for a bot and could be instead generated using other parameters. We could instead have a bot description field for the admin to put in notes of bot usage and other specifics.\r\n\r\nRelated Discussion: https://chat.zulip.org/#narrow/stream/6-frontend/topic/Bot.20email.20field'], ['\r\n- [ ] Mention the timezone in the rendered text to clarify user that the mention time is indeed in their timezone like `Mon, Mar 22 2021, 10:30 PM IST`. We have mentioned that the timezone is in their timezone in the title, but users tend to miss that.\r\n- [ ] Mention how long in the future / past was the mentioned time in the `title`. Like "Mentioned time was 5 hours ago from now" or "Mentioned time is 5 hours from now".\r\n- [ ] Use tooltip for displaying the `title` so that user can see it more easily.\r\n\r\nSee https://zulip.com/help/format-your-message-using-markdown#mention-a-time for help on time widget.\r\n@aryanshridhar FYI if you want to take it since you recently worked in this area.'], ['#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/docs/dev/getting_started/comparison/comparison_with_spreadsheets.html\r\n\r\n#### Documentation problem\r\n\r\nFrom https://github.com/pandas-dev/pandas/pull/38554:\r\n\r\n> I teach [a class on pandas for public policy students](https://github.com/afeld/python-public-policy/blob/master/syllabus.md#readme), and for many of them, spreadsheets are the only point of reference they have for working with tabular data.\r\n\r\nThere are [a number of unofficial resources](https://www.google.com/search?q=excel+vs+pandas), but no official, living documentation to point them to.\r\n\r\n#### Suggested fix for documentation\r\n\r\nThe idea is to add a "Comparison to spreadsheets" page alongside [the other comparison pages](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/index.html). That new page already got merged in https://github.com/pandas-dev/pandas/pull/38554; creating this issue to keep a running checklist of the TODOs.\r\n\r\n- [x] start with what @rotuna did in https://github.com/pandas-dev/pandas/pull/23042\r\n- [x] link from Getting Started pages\r\n- [ ] incorporate structure from [SAS](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sas.html)/[STATA](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_stata.html) comparison pages\r\n   - [x] Data structures\r\n   - [x] Data input / output\r\n      - [x] Mention `read_excel()`\r\n   - [x] Data operations\r\n   - [x] String processing\r\n   - [x] Merging\r\n   - [ ] Missing data\r\n   - [ ] GroupBy\r\n   - [x] Other considerations\r\n- [ ] standardize datasets used for examples, for greater consistency and reduced cognitive burden on the readers\r\n   - [`tips`](https://raw.github.com/pandas-dev/pandas/master/pandas/tests/io/data/csv/tips.csv) probably works for most\r\n- [ ] add info about charting\r\n- [x] [add example showing round trip to/from Excel](https://github.com/pandas-dev/pandas/pull/38554#discussion_r549409566)\r\n- [ ] [have pivot table examples match](https://github.com/pandas-dev/pandas/pull/38554#issuecomment-752767183)\r\n- [ ] add [quick reference](https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html#quick-reference)'], ["On http://localhost:9991/help/stream-permissions, there's a \r\n\r\n![image](https://user-images.githubusercontent.com/2746074/114207094-da8afb80-9910-11eb-8b34-2db4540f186e.png)\r\n\r\nThe code is `templates/zerver/help/stream-permissions.md`.  I think we basically should just add some spans (as we can with markdown) and use CSS for alignment rather than playing games with whitespace like what I removed in 681d2a08a169039d077b74f3b5025dfd42c1d4a7.\r\n\r\nThis is a really small issue, so only claim this after you've figured out how to do it."], ['The behavior of log scales when dealing with non-positive values is a common gotcha issue.\r\n\r\nThis seems to only be documented at\r\n\r\nhttps://matplotlib.org/api/scale_api.html#matplotlib.scale.LogScale and http://matplotlib.org/devdocs/gallery/pylab_examples/log_demo.html\r\n\r\nThe log-demo should be expanded to a tutorial with prose explaining \r\n\r\n - npos clip vs mask behavior (#8623 #8056 and links there in)\r\n - `loglog` vs `semilog` vs `ax.set_scale` vs `plt.xscale`/`plt.yscale`\r\n - changing the base \r\n - how ticking works with log scale (issues around http://matplotlib.org/users/dflt_style_changes.html?highlight=default%20changes#tick-label-formatting and issue around it (#8358 #7200 #8622 #8353 #8419 #8401 #8517 #8307)\r\n  - maybe fold in the symlog demo (https://matplotlib.org/examples/pylab_examples/symlog_demo.html)\r\n\r\n'], ['Currently functions like [`ravel`]( http://docs.dask.org/en/latest/array-api.html#dask.array.Array.ravel ) and [`flatten`]( http://docs.dask.org/en/latest/array-api.html#dask.array.Array.flatten ) follow the same traversal that one would expect when using NumPy. This makes sense for users coming from NumPy, who would expect this. Also there are some Dask Array functions that rely on this behavior to match their NumPy counterparts. However this means that `ravel` and `flatten` must spread out the chunks (rechunk the array), which can have a non-negligible performance cost associated.\r\n\r\nThat said, not all cases rely on matching the same order that NumPy would provide when flattening out an array. For cases like this, a nice alternative would be to flatten chunks themselves and merely stitch together the flattened chunks into a new 1-D array. This strategy would require no rechunking and would be embarrassingly parallel. Thus it would avoid the performance penalties that `ravel` and `flatten` have today.\r\n\r\nAssuming this strategy is reasonable, there are a few ways we could go about implementing it.\r\n\r\n1. Allow some additional options to the `order` parameter to handle this need.\r\n2. Add a new parameter to toggle NumPy or chunk-based traversal strategies.\r\n3. Include a config option to enable this behavior for Dask Arrays more generally.\r\n4. Add a new function entirely for this behavior.\r\n5. ?\r\n\r\nThoughts on this?'], ['We could use a simple guide file for new contributors. We have that for `git`, but not for Python and C++.\r\n\r\n* usage of `asdf` to break the CI build intentionally\r\n* how to add a new feature (e.g. adding a `demo` so you have a "new" `main()` function)\r\n* common C++ mistakes (let\'s see if we can get those in under 100MiB :P)\r\n* redirection to the `code_style`\r\n* how to profile (python, cython, c++)\r\n  * python (`openage.util.profiler.Profiler`)\r\n  * cython ([activate profiling in cython code](https://cython.readthedocs.io/en/latest/src/tutorial/profiling_tutorial.html), then start profiling with the `openage.util.profiler.Profiler`)\r\n  * C++ (hit `F12` to start and stop profiling, then you\'ll get a report-PDF)\r\n* summary of\r\n  * useful openage functions (incl redirection to testing/demo docs and others)\r\n    * `ENSURE` macro\r\n    *  `log` system introduction\r\n    * `error::Error` and `MSG` usage\r\n    * `libopenage/util/fslike` introduction\r\n    * `libopenage/util/timer`\r\n    * `libopenage/util/strings`\r\n  * make people aware of existing custom datastructures\r\n    * `PairingHeap`\r\n    * ...\r\n  * how to add cython files\r\n  * how to add c++ files\r\n  * how to add python files\r\n'], ['Part of these files are moved to `@superset-ui/chart-controls` but still incomplete.\r\n\r\n- [superset-frontend/src/explore/controlPanels/sections.jsx](https://github.com/apache/superset/blob/2a587f6cb1e598ef2d90ff3e60e97094ceec2ca1/superset-frontend/src/explore/controlPanels/sections.jsx) -> [packages/superset-ui-chart-controls/src/sections.tsx](https://github.com/apache-superset/superset-ui/blob/59283de7b0c9110c5dea64ac0ff4ef8d4554456d/packages/superset-ui-chart-controls/src/sections.tsx)\r\n- [superset-frontend/src/explore/controls.jsx](https://github.com/apache/superset/blob/3c62069bbb0d07ff0194fbb159c2e04fd98fafc2/superset-frontend/src/explore/controls.jsx#L281) -> [packages/superset-ui-chart-controls/src/shared-controls/index.tsx](https://github.com/apache-superset/superset-ui/blob/bd4d592299ff31c9f036feb93f93b529eb66fb8e/packages/superset-ui-chart-controls/src/shared-controls/index.tsx)\r\n\r\nDiverging changes in these almost identical files have caused [confusion](https://github.com/apache/superset/discussions/13404) and bugs. We should clean up the version in `superset-frontend` and port any [subsequent changes](https://github.com/apache/superset/commits/master/superset-frontend/src/explore/controlPanels/sections.jsx) after the file was initially copied over to `sueprst-ui/chart-controls`.\r\n\r\n'], ["NOTE: we'll need to verify that this is indeed what we want the behavior to be before doing any work on this.\r\n\r\nConsider the following sequence of actions a user might take\r\n1. Populate a `config.toml` file by running `streamlit config show > ~/.streamlit/config.toml`\r\n2. In the file, change the `server.port` config option to something other than 8501 (the default), let's say it's changed to 8502\r\n3. Start up streamlit: `streamlit run my_app.py`\r\n4. See the following message being logged and wonder why the port change apparently didn't take effect (note the server will be serving on http://localhost:8502)\r\n```\r\n  You can now view your Streamlit app in your browser.\r\n\r\n  Local URL: http://localhost:8501\r\n  Network URL: http://192.168.0.19:8501\r\n```\r\n\r\nThe cause of this confusing behavior is that `streamlit config show` prints the current values of config options when run, but this can cause weird interactions with config options with default values when using the command to initially populate a `config.toml` file.\r\n\r\nIn the example above, the default value for `server.port` is 8501, and the default value for `browser.serverPort` is to match the value of `server.port`. `streamlit config show` prints the following\r\n```toml\r\n[server]\r\n# Default: 8501\r\nport = 8501\r\n# ...\r\n\r\n[browser]\r\n# Default: whatever value is set in server.port.\r\nserverPort = 8501\r\n# ...\r\n```\r\nWhen the user uses the output of the command to initially populate a config file, then later changes `server.port`, the value of `browser.serverPort` is still explicitly set to 8501 within the config file, resulting in confusion. Most likely, the desired behavior was to have `browser.serverPort` use the default value instead of unintentionally fixing it in `config.toml`\r\n\r\nIf we instead print something like\r\n```toml\r\n[server]\r\n# Default: 8501\r\n# port = 8501\r\n# ...\r\n\r\n[browser]\r\n# Default: whatever value is set in server.port.\r\n# serverPort = 8501\r\n# ...\r\n```\r\nthere's no chance that some options are accidentally fixed within a config file when the intention was to use the default."], ['From [this comment](https://github.com/saltstack/salt/issues/18312#issuecomment-70873249):\n\n> Also, it looks like the color of the patch now is in green (see attached screenshot). It used to be yellow along with other changes. Is that a separate issue or related?\n'], ['MLflow seems to have a length limit of 5000 when setting tags (see below).\r\n\r\n``` bash\r\n[...]\r\n  File "/home/smay/miniconda3/envs/py38/lib/python3.8/site-packages/mlflow/utils/validation.py", line 136, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Tag value \'[0.8562690322984875, 0.8544098885636596, 0.8544098885636596, 0.8544098885636596, 0.8544098885636596, 0.859181214773054, 0.86273086038245, 0.86273086038245, 0.86273086038245, 0.86273086038245, 0.86273086038245, 0.8562690322984875, 0.8544098885636596, \' had length 5276, which exceeded length limit of 5000\r\n```\r\nSince you do not want to increase this limit (see #2892). What about an option to cut the strings if they are longer then 5000 characters and just log a warning instead of throwing an exception?\r\n\r\nWhat do you think? I am willing to contribute...'], ['`GCSToLocalFilesystemOperator` in `airflow/providers/google/cloud/transfers/gcs_to_local.py` checks the file size if `store_to_xcom_key` is `True`.\r\n\r\nhttps://github.com/apache/airflow/blob/b40dffa08547b610162f8cacfa75847f3c4ca364/airflow/providers/google/cloud/transfers/gcs_to_local.py#L137-L142\r\n\r\nHow it checks size is to download the object as `bytes` then check the size. This unnecessarily downloads the objects. `google.cloud.storage.blob.Blob` itself already has a `size` property ([documentation reference](https://googleapis.dev/python/storage/1.30.0/blobs.html#google.cloud.storage.blob.Blob.size)), and it should be used instead.\r\n\r\nIn extreme cases, if the object is big size, it adds unnecessary burden on the instance resources.\r\n\r\nA new method, `object_size()`, can be added to `GCSHook`, then this can be addressed in `GCSToLocalFilesystemOperator`.'], ['### Description of Issue\r\n<!-- Note: Please direct questions to the salt-users google group. Only post issues and feature requests here -->\r\n\r\nSaltStack strips / character from APT URIs and Google package adds an entry with this extra character resulting in duplicate entry and apt upgrade failures. Settings as per SaltStack manual for pkg.managed\r\n\r\n### Setup\r\n(Please provide relevant configs and/or SLS files (Be sure to remove sensitive info).)\r\n\r\nDeploy Google Chrome through SaltStack package to Ubuntu. On next package upgrde from Google, machine will fail to upgrade due to duplicate entry in /etc/apt/source.d file\r\n\r\n### Steps to Reproduce Issue\r\n(Include debug logs if possible and relevant.)\r\n\r\n### Versions Report\r\n(Provided by running `salt --versions-report`. Please also mention any differences in master/minion versions.)\r\n\r\nsalt 2018.3.4\r\nany recent Google Chrome version.'], ['Hi,\r\n\r\nI have a heatmap style plot based on the [les_mis.py](https://docs.bokeh.org/en/latest/docs/gallery/les_mis.html) example.\r\n\r\nI am configuring the `LinearColorMapper` with a `low_color` value and the plot shows it correctly, but the `ColorBar` does not, even if I explicitly give it a `FixedTicker`\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport bokeh.palettes as palettes\r\nfrom bokeh.plotting import figure\r\nfrom bokeh.plotting import show\r\nfrom bokeh.models import LinearColorMapper, ColorBar, FixedTicker\r\nfrom bokeh.transform import transform\r\n\r\ndata = pd.DataFrame(\r\n    np.arange(0.1, 1.0, 0.1).reshape(3, 3),\r\n    index=["a", "b", "c"], columns=["x", "y", "z"]\r\n)\r\n\r\nii, jj = np.meshgrid(data.columns, data.index, indexing="ij")\r\nii = ii.flatten()\r\njj = jj.flatten()\r\ncc = [data.loc[j, i] for i, j in zip(ii, jj)]\r\nplot_data = dict(\r\n    xname=ii,\r\n    yname=jj,\r\n    colors=cc,\r\n    data=cc,\r\n)\r\n\r\ncolormapper = LinearColorMapper(\r\n    palette=palettes.magma(7)[::-1], low=0.3, high=1.0, low_color="white"\r\n)\r\n\r\nfig = figure(\r\n    x_axis_location="above",\r\n    tools="hover",\r\n    x_range=list(data.columns),\r\n    y_range=list(reversed(data.index)),\r\n    tooltips=[\r\n        (\'names\', \'@yname, @xname\'), (\'data\', \'@data{0.0}\')\r\n    ],\r\n)\r\n\r\nfig.rect(\r\n    \'xname\',\r\n    \'yname\',\r\n    1,\r\n    1,\r\n    source=plot_data,\r\n    color=transform("colors", colormapper),\r\n)\r\n\r\n# ticks = [0.0]\r\n# ticks.extend(np.arange(0.3, 1.0, 0.1))\r\ncolor_bar = ColorBar(\r\n    color_mapper=colormapper,\r\n    location=(0, 0),\r\n#     ticker=FixedTicker(ticks=ticks)\r\n)\r\n\r\nfig.add_layout(color_bar, \'right\')\r\n\r\nshow(fig)\r\n```\r\n\r\nThis is the figure I get with the above code:\r\n\r\n![plot showing issue](https://user-images.githubusercontent.com/4750021/89425938-7dabc580-d731-11ea-84ab-73137d504b8b.png)\r\n\r\nIs this the expected behaviour or a bug?\r\n\r\nI\'ve tried this on bokeh 2.1.1 with python 3.7.2\r\n\r\nThanks\r\n'], ['\r\n![butterworth_order_5](https://user-images.githubusercontent.com/971634/36656313-e6f34556-1b1b-11e8-8aab-a0eb71e1fe82.png)\r\n\r\n\r\nI am using a slightly modified code form [scipy-cookbook here](http://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html). The parameters are the default ones in the code below (fs = 200,lowcut = 1.0,highcut = 2.0) . I am wondering if the "order-5" curve is correct?!\r\n\r\n\r\n### Reproducing code example:\r\n```\r\nfrom scipy.signal import butter, lfilter\r\n\r\ndef butter_bandpass(lowcut, highcut, fs, order=5):\r\n    nyq = 0.5 * fs\r\n    low = lowcut / nyq\r\n    high = highcut / nyq\r\n    b, a = butter(order, [low, high], btype=\'band\')\r\n    return b, a\r\n\r\n\r\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\r\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\r\n    y = lfilter(b, a, data)\r\n    return y\r\n\r\ndef get_freqz(fs = 200,lowcut = 1.0,highcut = 2.0, order = [1,2,3,4,5]):\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    from scipy.signal import freqz,freqs\r\n    plt.style.use(\'bmh\')\r\n\r\n    plt.figure(figsize=(4,4),dpi = 150)\r\n    \r\n    for o in order:\r\n        b, a = butter_bandpass(lowcut, highcut, fs, order=o)\r\n        w, h = freqz(b, a, worN=2000)\r\n   \r\n        plt.plot((fs * 0.5 / np.pi) * w, abs(h), linewidth=1,label="order = %d" % o)\r\n        plt.xlim((0,3))\r\n\r\n        plt.xlabel(\'Frequency (Hz)\')\r\n        plt.ylabel(\'Gain\')\r\n        plt.legend(loc=\'best\')\r\n        \r\n    plt.show()\r\n\r\n\r\n    \r\nget_freqz()   \r\n```\r\n\r\n### Scipy/Numpy/Python version information:\r\n```\r\n\r\nscipy.__version__ = \'0.19.1\'\r\nnumpy.__version__ = \'1.13.1\'\r\nsys.version_info = sys.version_info(major=2, minor=7, micro=13, releaselevel=\'final\', serial=0)\r\n```\r\n\r\n'], ['## Description\r\n\r\nSafari Version: 14.0.3 (16610.4.3.1.4)\r\nSuperset Commit hash: c9a755f25\r\n\r\nAs seen in the below video, after changing any chart type or running an invalid query, the height of \'explore-container\' div gets larger than \'app\' div. Hence, the screen becomes ineffective if we want to run another query since the controls like \'Run\' and \'Save\' gets hidden.\r\n\r\nhttps://user-images.githubusercontent.com/57817186/110589449-4835e300-812b-11eb-86b5-2225f70b3291.mp4\r\n\r\nTested with Google Chrome and Firefox where it works fine. \r\n\r\n#### Expected Screen:\r\n\r\nGoogle Chrome:\r\n\r\n<img width="1434" alt="Preset_io_Dataset_expected" src="https://user-images.githubusercontent.com/57817186/110590155-43256380-812c-11eb-994c-810a02e48c9d.png">\r\n \r\n\r\n## Design input\r\n[describe any input/collaboration you\'d like from designers, and\r\ntag accordingly. For design review, add the\r\nlabel `design:review`. If this includes a design proposal,\r\ninclude the label `design:suggest`]\r\n'], ['## 🚀 Feature\r\n`reload_dataloaders_every_epoch` reloads dataloader every epoch. We can change it to reload dataloaders after every n epochs. It will accept both bool and int values. `True` will be treated as 1 and `False` will be treated as 0 (means no reload).\r\n\r\n### Motivation\r\nRecently a request came up on slack regarding reloading dataloaders after some epochs. Also since we have enhanced `fast_dev_run` like this, we can update this too.\r\n\r\n### Alternatives\r\nCan be done manually by calling the method to reload dataloaders with a simple conditional statement by setting `reload_dataloaders_every_epoch=False`, but we can let this attribute accept an integer to avoid manual work.\r\n\r\n'], ['### What is the problem?\r\nI\'m calling tune.run, but no file is being output in the local_dir. \r\n\r\nray: 0.8.6\r\ntensorflow: 2.2.0\r\nArch Linux\r\n\r\n### Reproduction \r\n\r\nTune it and ask for the h5\r\n```python\r\nfrom ray import tune\r\nfrom ray.rllib.agents.ppo import PPOTrainer\r\ntune.run(PPOTrainer,\r\n         config={"env": "CartPole-v0"},\r\n         export_formats=[\'h5\'],\r\n         local_dir=\'cart_outputs\',\r\n         stop={"training_iteration": 1}\r\n)\r\n```\r\n\r\nSee outputs in local_dir\r\n```bash\r\n> tree cart_outputs\r\ncart_outputs\r\n└── PPO\r\n    ├── experiment_state-2020-07-06_09-17-52.json\r\n    └── PPO_CartPole-v0_0_2020-07-06_09-17-52g7nd9506\r\n        ├── events.out.tfevents.1594041472.refindly-arch\r\n        ├── params.json\r\n        ├── params.pkl\r\n        ├── progress.csv\r\n        └── result.json\r\n\r\n2 directories, 6 files\r\n```\r\n\r\n'], ["Due to personal experience I'd really like to replace jinja2 with mako templates instead.\r\n\r\nWe only use jinja in `openage/codegen/coord.py`.\r\n\r\nReally not high priority :)"], ['If you are using Airflow at the company you work with, add the name of your company to the list of Airflow Users at https://github.com/apache/airflow/blob/master/INTHEWILD.md\r\n\r\nCheck an example PR: https://github.com/apache/airflow/pull/11995\r\n\r\nAdd the name of organization and GitHub handle to https://github.com/apache/airflow/blob/master/INTHEWILD.md as an easy first commit :) Would love to discover who some of our new Airflow users are.\r\n\r\n**Note**: The list is alphabetically ordered, so please follow the order or else static checks in the CI would fail.\r\n\r\nThis issue does not need to be assigned to anyone, hence this issue will remain open forever :)\r\n\r\n'], ['When running projects on Databricks, we should filter out the uid / guid / username / group name when tarring projects for upload to DBFS, since we use the hash of the tarfile to determine whether to reupload a project to DBFS. This could be done by updating the [helper method](https://github.com/mlflow/mlflow/blob/master/mlflow/utils/file_utils.py#L246) used to modify file metadata [(TarInfos)](https://docs.python.org/3/library/tarfile.html#tarfile.TarInfo.uid) while creating the archive. '], ['The action "Copy partition to clipboard" on the left side of SQL Lab doesn\'t work.\r\n\r\n### Expected results\r\n\r\nrecord is in clipboard\r\n\r\n### Actual results\r\n\r\nnothing (old value remains, even though bottom of the page says "copied to clipboard!"\r\n\r\n#### Screenshots\r\n\r\n![Screen Shot 2021-03-01 at 8 51 50 AM](https://user-images.githubusercontent.com/61221714/109530453-95e78700-7a6b-11eb-9719-0629ceda5c58.png)\r\n![image (7)](https://user-images.githubusercontent.com/61221714/109530460-9aac3b00-7a6b-11eb-8c62-c66548a950c3.png)\r\n\r\n\r\n#### How to reproduce the bug\r\n\r\n1. Go to \'SQL Lab\'\r\n2. In left panel select schema and table\r\n3. Click down arrow to see details of the table\r\n4. Click on "copy" next to latest partition info\r\n5. See error\r\n\r\n### Environment\r\n\r\n(please complete the following information):\r\n\r\n- superset version: `master`\r\n### Checklist\r\n\r\nMake sure to follow these steps before submitting your issue - thank you!\r\n\r\n- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.\r\n- [x] I have reproduced the issue with at least the latest released version of superset.\r\n- [x] I have checked the issue tracker for the same issue and I haven\'t found one similar.\r\n\r\n'], ['In the UI it would be nice to duplicate the selected connection, in the menu "With Selected". \r\n\r\nThe copy would be set with the same name plus some suffix (_copy, _1, whatever) and it is useful for when you have a connection with all equal fields except something in a particular one. '], ['\n* Numbers should be right-justified, everything else left-justified. \n* Column headers should be justified with their contents.'], ['The documentation for class `ElasticNetCV()` reads:\r\n\r\n    y : array-like, shape (n_samples,) or (n_samples, n_targets)\r\n\r\nHowever, when I try to pass a 2D array I get the error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-33-597417f0a72a> in <module>()\r\n      2 \r\n      3 enet = ElasticNetCV()\r\n----> 4 enet.fit(X_train, y_train)\r\n\r\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py in fit(self, X, y)\r\n   1091             if y.ndim > 1 and y.shape[1] > 1:\r\n   1092                 raise ValueError("For multi-task outputs, use "\r\n-> 1093                                  "MultiTask%sCV" % (model_str))\r\n   1094             y = column_or_1d(y, warn=True)\r\n   1095         else:\r\n\r\nValueError: For multi-task outputs, use MultiTaskElasticNetCV\r\n```\r\n\r\n'], ['<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Describe your feature request\r\n\r\nFor feature requests or questions, post on our Discussion page instead: https://discuss.ray.io/\r\n\r\nWhen you first call `ray.init()`, you get a dictionary of cluster information including head node IP, etc:\r\n\r\n```\r\n>>> ray.init("auto")\r\n2021-03-29 13:56:08,644 INFO worker.py:653 -- Connecting to existing Ray cluster at address: 172.31.61.153:6379\r\n{\'node_ip_address\': \'172.31.61.153\', \'raylet_ip_address\': \'172.31.61.153\', \'redis_address\': \'172.31.61.153:6379\', \'object_store_address\': \'/tmp/ray/session_2021-03-29_13-56-02_184304_17642/sockets/plasma_store\', \'raylet_socket_name\': \'/tmp/ray/session_2021-03-29_13-56-02_184304_17642/sockets/raylet\', \'webui_url\': \'127.0.0.1:8265\', \'session_dir\': \'/tmp/ray/session_2021-03-29_13-56-02_184304_17642\', \'metrics_export_port\': 8080, \'node_id\': \'06873d5ae668b9bf8af405b7d32821c78dc56282aa71daee6adfa335\'}\r\n```\r\n\r\nHowever there is no way to get this dictionary _after_ initialization using public API. Some information, e.g. the head node address, can be obtained via private API like `ray.worker._global_node.address`, but it would be great if there is a public API for it.\r\n\r\ncc @rkooo567 @edoakes @richardliaw '], ["See  https://github.com/ray-project/ray/pull/5626 -- the refactored DDPG somehow doesn't work on certain tasks such as MountainCar."], ['**Description**\r\n\r\nA clear and concise description of what the bug is.\r\n\r\n**Setup**\r\n\r\nCreate/import a GPG key.\r\n\r\n**Steps to Reproduce the behavior**\r\n\r\nTry to encrypt some text:\r\n\r\n```\r\n[root@master1 salt]# salt-call gpg.encrypt text=foo recipients=2CFCA2B538DD0FAC75838096A4E89A0D3328D483 output=/tmp/encypted_foo.txt user=salt\r\nlocal:\r\n    ----------\r\n    comment:\r\n        Encrypted data has been written to /tmp/encypted_foo.txt\r\n    res:\r\n        True\r\n[root@master1 salt]# cat /tmp/encypted_foo.txt\r\ncat: /tmp/encypted_foo.txt: No such file or directory\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe file should be created.\r\n\r\n**Versions Report**\r\n<details><summary>salt --versions-report</summary>\r\n(Provided by running salt --versions-report. Please also mention any differences in master/minion versions.) \r\n\r\n```\r\n# salt-call test.versions_report\r\nlocal:\r\n    Salt Version:\r\n               Salt: 3000.1\r\n     \r\n    Dependency Versions:\r\n               cffi: Not Installed\r\n           cherrypy: Not Installed\r\n           dateutil: Not Installed\r\n          docker-py: Not Installed\r\n              gitdb: Not Installed\r\n          gitpython: Not Installed\r\n             Jinja2: 2.7.2\r\n            libgit2: Not Installed\r\n           M2Crypto: Not Installed\r\n               Mako: Not Installed\r\n       msgpack-pure: Not Installed\r\n     msgpack-python: 0.6.2\r\n       mysql-python: Not Installed\r\n          pycparser: Not Installed\r\n           pycrypto: 2.6.1\r\n       pycryptodome: Not Installed\r\n             pygit2: Not Installed\r\n             Python: 2.7.5 (default, Aug  7 2019, 00:51:29)\r\n       python-gnupg: 0.4.3\r\n             PyYAML: 3.11\r\n              PyZMQ: 15.3.0\r\n              smmap: Not Installed\r\n            timelib: Not Installed\r\n            Tornado: 4.5.3\r\n                ZMQ: 4.1.4\r\n     \r\n    System Versions:\r\n               dist: centos 7.6.1810 Core\r\n             locale: UTF-8\r\n            machine: x86_64\r\n            release: 5.3.0-46-generic\r\n             system: Linux\r\n            version: CentOS Linux 7.6.1810 Core\r\n```\r\n</details>\r\n'], ['A minor cleanup for the future.\r\n\r\nAt the moment [we use this in `http11.py`](https://github.com/scrapy/scrapy/pull/4897/files/2eabfb60e8112a85f4f1c6ce16c4a789856e3ead#r578344512):\r\n\r\n```\r\nwith suppress(AttributeError):\r\n    txresponse._transport._producer.loseConnection()\r\n```\r\n\r\n[Once we drop support for Twisted < 18.4.0](https://github.com/twisted/twisted/commit/38559237ec584e41d2c8667c3b9573865ddff19e), we should be able to switch to:\r\n\r\n```\r\ntxresponse._transport.loseConnection()\r\n```'], ["During the PyCon Hatchery Mentories sprint, it has been mentioned that currently There's only 3 default theme and it would be great to have more."], ['Hatches in the fill_between function are not rendered with the Agg backend.\r\n\r\nFor example, the following code:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport matplotlib as mpl\r\nmpl.use("agg")\r\nimport matplotlib.pyplot as plt\r\n\r\ncolor = \'blue\'\r\nxgrid = np.linspace(0,10,100)\r\nerr68up = 2*xgrid\r\nerr68down = 3*xgrid\r\nhatch = "///"\r\nalpha=0.3\r\n\r\nplt.figure()\r\nplt.fill_between(xgrid, err68up, err68down, color=\'None\', alpha=alpha,\r\n                    edgecolor=color, hatch=hatch, zorder=1)\r\nplt.savefig("test.png")\r\n```\r\nresults in an empty figure. However it works as I expect in the pdf or Qt4Agg backends.\r\n\r\nI am using the conda-forge matplotlib 1.5.3 package on python 3.5.\r\n\r\n\r\n\r\n'], ["Hi\r\nPlease add some method work with children nodes same as lxml.etree.getchildren()\r\n\r\nCurrently, I'm using requests-html but sometime I must using lxml.etree to load html from Element, after that, using etree to get children :(\r\n\r\nThanks so much. This is awesome project."], ['Grep the code for "deprecated", etc. and remove relevant code/interfaces. Be sure to include news fragments for each change.'], ["### FOLLOW THESE INSTRUCTIONS CAREFULLY\r\n\r\n*ISSUES THAT DO NOT CONTAIN NECESSARY INFORMATION MAY BE CLOSED, IMMEDIATELY*\r\n\r\nThe issue tracker is NOT the place for general support. For questions and technical assistance, come join the [Bokeh Project Discourse](https://discourse.bokeh.org).\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nSphinx extension bokeh_plot directive resources defaulted to cdn which is an issue when working on computers that are not connected to the internet\r\n\r\n**Describe the solution you'd like**\r\nA workaround for cdn resources\r\n\r\n**Describe alternatives you've considered**\r\nMonkey patch on conf.py file as below:\r\n\r\n```python\r\ndef get_sphinx_resources(include Bokehjs api-false):\r\n    from bokeh.resources import INLINE\r\n    return INLINE\r\n\r\nimport bokeh.sphinxext.util as util\r\nutil.get_sphinx_resources = get_sphinx_resources\r\n```\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n"], ['Hi there! This issue is a permanent issue dedicated as a placeholder to for documentation contributions. If you have questions talk to the community on our [discord server](https://discordapp.com/invite/awxPgve).\r\n\r\nIf you have a question about a particular issue, or pull rqeuest just tag @python-poetry/triage in a comment.\r\n\r\n## Reference\r\n- [Contribution Guide](https://github.com/python-poetry/poetry/blob/master/CONTRIBUTING.md#contributing-to-documentation)\r\n- [Open Documentation Issues](https://github.com/python-poetry/poetry/labels/Documentation)\r\n- [Documentation Source](https://github.com/python-poetry/poetry/tree/master/docs)\r\n- [Opportunities to add an FAQ](https://github.com/python-poetry/poetry/issues?q=is%3Aissue+label%3Acandidate%2Ffaq+)\r\n- [FAQ Source](https://github.com/python-poetry/poetry/blob/master/docs/docs/faq.md)'], ['# 🚀 Feature request\r\n\r\nThis is a feature request to add Wav2Vec2 Pretraining functionality to the transformers library. This is a "Good Second Issue" feature request, which means that interested contributors should have some experience with the transformers library and ideally also with training/fine-tuning Wav2Vec2.\r\n\r\n## Motivation\r\n\r\nThe popular [Wav2Vec2](https://huggingface.co/models?filter=wav2vec2) model cannot be pretrained using the Hugging Face library yet. During the fine-tuning week, multiple people have reported improved results by pretraining wav2vec2 directly on the target language before fine-tuning it.\r\n\r\n## Your contribution\r\n\r\nI am happy to give an interesting contributor guidance throughout the PR and answer all relevant questions.\r\n\r\n## How to start\r\n\r\n1) To begin with, one should run a pretraining forward pass using the official Wav2Vec2 repository. The forward pass can be found here: https://github.com/pytorch/fairseq/blob/436166a00c2ecd1215df258f022608947cca2aa8/fairseq/models/wav2vec/wav2vec2.py#L474. \r\nIt is important that the argument `features_only` is set to `False` in the [`forward`](https://github.com/pytorch/fairseq/blob/436166a00c2ecd1215df258f022608947cca2aa8/fairseq/models/wav2vec/wav2vec2.py#L474) function.\r\nSuccesfully running a forward pass with fairseq is important to ensure the correctness of the hugging face implementation by comparing the two outputs.\r\n\r\nThis is probably the most difficult part of the PR.\r\n\r\n**Note:** this also means that the loaded fairseq wav2vec2 checkpoint should include weights for the `GumbelVectorQuantizer` quantizer, see: https://github.com/pytorch/fairseq/blob/436166a00c2ecd1215df258f022608947cca2aa8/fairseq/models/wav2vec/wav2vec2.py#L277\r\n\r\nThe easiest checkpoint to try out pretraining with is probably the wav2vec2 2.0 Base - No fine-tuning [here](https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt)\r\n\r\n[Here](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#train-a-wav2vec-20-base-model) is the official Fairseq recipe on how to do so.\r\n\r\n2) Having run a forward pass successfully, the methods can now be implemented into transformers [here](https://github.com/huggingface/transformers/blob/653076ca307520ee85fd5f5de6918019f8521bb5/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L966) as a new class that could roughly look as follows:\r\n\r\n```python \r\n\r\nclass Wav2Vec2ForPretraining:\r\n\r\ndef init(self, config):\r\n      self.wav2vec2 = Wav2Vec2Model(config)\r\n      self.quantizer = ...\r\n      self.project_q = ...\r\n\r\ndef forward(...):\r\n\r\n        outputs = self.wav2vec2(\r\n            input_values,\r\n            attention_mask=attention_mask,\r\n            output_attentions=output_attentions,\r\n            output_hidden_states=output_hidden_states,\r\n            return_dict=return_dict,\r\n        )\r\n\r\n        # ... all the pretraining logic here\r\n\r\n```\r\n\r\nHaving implemented the class it should be made sure that a forward pass of `Wav2Vec2ForPretraining` works.\r\n\r\n3) Convert the pretrained checkpoints correctly\r\n\r\nAfter `Wav2Vec2ForPretraining` was succesfully added, a "non-finuted" checkpoint, e.g., the wav2vec2 2.0 Base - No fine-tuning [here](https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt) should be converted to the hugging face models. One will probably have to slightly adapt the conversion script as well: https://github.com/huggingface/transformers/blob/master/src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py\r\n\r\nHaving converted the checkpoint it can be uploaded to the hub and checked whether the checkpoint yields the same outputs as the official wav2vec2 pretraining functionality.\r\n\r\n4) Add tests\r\n\r\nNext, a couple of tests should be implemented that make sure that the behavior stays correct in the future. This included both fast and "slow" integration tests (fast tests are "normal" tests); "slow" integration tests include a "real" checkpoint and test its output against a hardcoded expected output tensor slice as it\'s done, *e.g.*, [here](https://github.com/huggingface/transformers/blob/653076ca307520ee85fd5f5de6918019f8521bb5/tests/test_modeling_big_bird.py#L823).\r\n\r\n## Ask for help\r\n\r\nFor questions regarding how to finish 1), they can be directly asked on this issue; I will try to answer them as best as I can. Also gently pinging @cceyda here as I think she has already succesfully pretrained a wav2vec2 model using fairseq. Hope it\'s fine to ping you here 😅) - in case you have some good tips on how to pretrained wav2vec2 with fairseq, it would be amazing to share some tips here. \r\n\r\nFor questions when doing 2), 3) & 4) please directly asked on the PR you have opened to implement the model.\r\n\r\n'], ['Docs fail if we add them.\r\n\r\nProbably the TYPE_CHECKING variable needs to be injected\r\n\r\n### Context\r\n\r\n> Try this:\r\n\r\nthat is what I already had in f9959a3abcdc659d21a3e65b85cafe686b6f0ed5 but as I say docs were failing\r\n\r\n_Originally posted by @Borda in https://github.com/PyTorchLightning/pytorch-lightning/issues/5919#issuecomment-777447287_'], ['I need to understand how files works during the crawling and how they are used in the crawler. Like "requests.seen", or "queue dir" or "activity.json" and so on. I had some problem with the crawler and i would try to understand how this files works. \r\n\r\nShould be useful to know the interaction between this kind of files with the crawler, maybe in the documentation.'], ['I noticed that `test-documentation` doesn\'t check that the user\'s development environment is setup, even though it does need our Python virtualenv to work, os it misses out on the things we do to avoid weird "missing dependency" error messages:\r\n\r\n```\r\n$ ./tools/test-documentation \r\n/bin/sh: 1: sphinx-build: not found\r\nMakefile:19: recipe for target \'clean\' failed\r\nmake: *** [clean] Error 127\r\n```\r\n\r\nWe should fix this to use , which probably involves converting that script to Python like its cousin `test-help-documentation` already is.\r\n\r\n(While working on this you\'ll probably notice https://github.com/zulip/zulip/issues/17928; the right fix is `test-documentation --skip-external-links` for now)\r\n\r\nWe should also look through https://zulip.readthedocs.io/en/latest/testing/testing.html and also `test-all` and check that there aren\'t any others with this issue.'], ["<!--\r\n\r\nWelcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.\r\nDon't worry if they're not all applicable; just try to include what you can :-)\r\n\r\nIf you need to include code snippets or logs, please put them in fenced code\r\nblocks.  If they're super-long, please use the details tag like\r\n<details><summary>super-long log</summary> lots of stuff </details>\r\n\r\nPlease delete these comment blocks before submitting the issue.\r\n\r\n-->\r\n\r\n**Description**\r\nCurrently, GoogleAdsHook only supports the `service account authentication flow`. It would be nice to have a way to authenticate with `OAuth2 installed application flow`.\r\n<!-- A short description of your feature -->\r\n\r\n**Use case / motivation**\r\n[AdWords API Doc](https://developers.google.com/adwords/api/docs/guides/authentication#an_alternative_to_service_accounts) \r\n> Because of the complexity of setting up service account access for the AdWords API, we recommend a simpler alternative that achieves the same goal, which is to use the OAuth2 installed application flow\r\n\r\nTo quote Google, service account flow is a hassle, and they recommend `OAuth2 installed application flow`. With this feature, GoogleAds operator users will be able to choose which authentication flow to use.\r\n\r\n<!-- What do you want to happen?\r\n\r\nRather than telling us how you might implement this solution, try to take a\r\nstep back and describe what you are trying to achieve.\r\n\r\n-->\r\n\r\n**Are you willing to submit a PR?**\r\nYes!\r\n\r\n<!--- We accept contributions! -->\r\n\r\n**Related Issues**\r\nAIRFLOW-6732\r\n\r\n<!-- Is there currently another issue associated with this? -->\r\n"], ['Create an [incoming webhook](https://zulip.com/api/incoming-webhooks-overview) integration for Open Collective. The main use case is getting notifications when new financial contributors sign up.\r\n\r\nOpen Collective integrations documentation: https://docs.opencollective.com/help/collectives/integrations\r\n\r\nNow that [Zulip has an Open Collective page](https://opencollective.com/zulip), it would be great to be able to monitor new sign-ups!'], ["### Describe your feature request\r\nI've been bitten by this at least twice now and it delays PRs.\r\n\r\nCI runs an extra lint step (or at least with different arguments) than `scripts/format.sh` (or `scripts/format.sh --all`). \r\nIt also runs `./scripts/check-git-clang-format-output.sh` -- and why this is different than the clang-format run in `--all` is unclear to me.\r\n\r\nThis is most notable in linting `*.proto` files, but it's worth an audit of what else clang-format is missing between the two\r\n\r\nThere are two subissues here:\r\n1) Audit the discrepancies between the two scripts (and the necessity of the latter)\r\n2) Consider adding a proto-specific lint step instead of using `clang-format` -- probably [buf from Uber](https://github.com/bufbuild/buf) -- which we could set up in linting mode for now.\r\n\r\nLonger term (or if you feel like scripting it), `buf` would allow us to check for breaking changes to protos, which requires a little more design work (sometimes we want to break, when iterating on a new feature) but bringing the tool into our build here, ahead of time, simply to lint seems like a win for now and is one less step for the future\r\n"], ['##### ISSUE TYPE\r\n - Bug Report\r\n\r\n##### SUMMARY\r\nConfusing UI if you cancel a job while it is still in "pending" state, caused by API behavior.\r\n\r\n##### ENVIRONMENT\r\n* AWX version: 4.0.0\r\n* AWX install method: openshift, minishift, docker on linux, docker for mac, boot2docker\r\n* Ansible version:  X.Y.Z\r\n* Operating System:\r\n* Web Browser:\r\n\r\n##### STEPS TO REPRODUCE\r\n\r\nLaunch a job that is blocked by some other job, so it remains in "pending" (or send to an instance group with no capacity), and then cancel it.\r\n\r\n##### EXPECTED RESULTS\r\n\r\nMostly null fields related to started, finished, etc.\r\n\r\nThe job never started and never finished.\r\n\r\n##### ACTUAL RESULTS\r\n\r\nThe UI shows that the job finished, see 7844\r\n\r\n![Screen Shot 2019-06-03 at 8 50 50 AM](https://user-images.githubusercontent.com/1385596/58803296-12baa800-85dd-11e9-999f-a678ce860c2e.png)\r\n\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nCaused by the API fields.\r\n\r\n```json\r\n    "scm_type": "git",\r\n    "scm_url": "https://github.com/ansible/test-playbooks.git",\r\n    "scm_branch": "",\r\n    "scm_clean": false,\r\n    "scm_delete_on_update": false,\r\n    "credential": null,\r\n    "timeout": 0,\r\n    "unified_job_template": 3838,\r\n    "launch_type": "manual",\r\n    "status": "canceled",\r\n    "failed": true,\r\n    "started": null,\r\n    "finished": "2019-06-03T12:39:05.272329Z",\r\n    "elapsed": 0.0,\r\n    "job_args": "",\r\n    "job_cwd": "",\r\n    "job_env": {},\r\n    "job_explanation": "",\r\n    "execution_node": "",\r\n    "result_traceback": "",\r\n    "event_processing_finished": true,\r\n    "project": 3838,\r\n    "job_type": "check",\r\n```\r\n\r\nExpectation is that the `finished` field should remain null.\r\n\r\nPreviously @ryanpetrello made the suggestion\r\n\r\n> Seems like if we really wanted to have accurate data, we\'d want another column to record the "canceled_on" datestamp.\r\n\r\nThis issue also makes the argument for an independent field to track canceled time. (but we may prefer to track that as a separate enhancement)'], ["sometimes we just want to temporarily check some info and don't like to make our console dirty. like `%whos` mentioned in this [stackoverflow question](http://stackoverflow.com/questions/5740835/how-to-use-pipe-in-ipython). another case can be someone types `%hist` to remind himself. So I suggest add an option to those magics such that users can display the output in pager. \n"], ['Symtoms:\r\n    Causes clipped nodes and non-transparent background.  \r\n\r\nRecreate issue:\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport networkx as nx\r\nimport matplotlib.pyplot as plt\r\n \r\n# Build a dataframe with your connections\r\ndf = pd.DataFrame({ \'from\':[\'A\', \'B\', \'C\',\'A\'], \'to\':[\'D\', \'A\', \'E\',\'C\'] })\r\ndf\r\n \r\n# Build your graph\r\nG=nx.from_pandas_edgelist(df, \'from\', \'to\', create_using=nx.Graph() )\r\n \r\n# Custom the nodes:\r\nfig = plt.figure()\r\npos = nx.spectral_layout(G)\r\nnx.draw_networkx_nodes(G, pos=pos, with_labels=True, node_color=\'skyblue\', node_size=1500, edge_color=\'white\')\r\nfig.set_facecolor("#00000F")\r\n\r\nplt.show()\r\n```\r\n\r\nIn `networkx/drawing/nx_pylab.py` the `draw_networkx_nodes` function \r\n\r\ncurrently:\r\n```python\r\nif ax is None:\r\n    ax = plt.gca()\r\n```\r\n\r\nshould be something like:\r\n```python\r\nif ax is None:\r\n    cf = plt.gcf()\r\n    if cf._axstack() is None:\r\n        ax = cf.add_axes((0, 0, 1, 1))\r\n    else:\r\n        ax = cf.gca()\r\nelse:\r\n    cf = ax.get_figure()\r\ncf.set_facecolor(\'w\')\r\nax.set_axis_off()\r\n```\r\n\r\nWorkaround:\r\n    Add "ax == None" logic from the `should be something like` section above and pass the ax key word arg to `draw_networkx_nodes`\r\n\r\n*edit:  added complete example of issue.  '], ['According to [user.present documentation](https://docs.saltstack.com/en/latest/ref/states/all/salt.states.user.html#salt.states.user.present) the `expire` property of `user.present` should be specified as an integer as\r\n> Date that account expires, represented in days since epoch (January 1, 1970)\r\n\r\nTo make it more human-readable I want to be able to specify it as a string with an ISO 8601 date (e.g. "2020-01-27").\r\n\r\nInstead of:\r\n```\r\nfoo-expires:\r\n  user.present:\r\n    - name: foo\r\n    - expire: 18288\r\n```\r\n\r\nI would like to be able to have:\r\n```\r\nfoo-expires:\r\n  user.present:\r\n    - name: foo\r\n    - expire: 2020-01-27\r\n```\r\n\r\nThat would make it much easier to understand when a user expires.\r\n'], ['[An answer on my recent SO post](https://stackoverflow.com/a/65085536/4107349) used `get_indexer`, which worked great, but I soon learned if the indexer doesn\'t find a match it seems to use the last entry without raising an error:\r\n\r\n```\r\ndf = pd.DataFrame({"ID": [6, 2, 4],\r\n                   "to ignore": ["foo", "whatever", "idk"],\r\n                   "value": ["A", "B", "asdf"],\r\n                   })\r\n\r\ndf2 = pd.DataFrame({"ID_number": [1, 2, 3, 4, 5, 6],\r\n                    "A": [0.91, 0.42, 0.85, 0.84, 0.81, 0.88],\r\n                    "B": [0.11, 0.22, 0.45, 0.38, 0.01, 0.18],\r\n                    })\r\n\r\ndf2 = df2.set_index(\'ID_number\')\r\ndf[\'new_col\'] = df2.values[df2.index.get_indexer(df[\'ID\']), df2.columns.get_indexer(df[\'value\'])]\r\n```\r\n\r\nI presumed the row with "asdf" would have raised an error, not returned the value for "B". This was problematic because I unwittingly processed data incorrectly and I enjoy being employed.\r\n\r\nMy interpretation of the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.get_indexer.html) was that if `method` was not supplied it would be "default: exact matches only". Supplying `method = \'default\'` and `tolerance = 0` was also not accepted when not using another method.\r\n\r\nAlso, I\'m no serious programmer so I have likely misunderstood something and am only trying to help. Please feel free to correct / tell me to go away'], ["This issue tracks Superset's migration from JavaScript to TypeScript (as started in [SIP-36](https://github.com/apache/incubator-superset/issues/9101)). If you'd like to help with the migration, feel free to take an unchecked directory and convert the files within the immediate directory from JavaScript/JSX to TypeScript/TSX. #9162 and #9180 provide some tips for performing the migration. Once completed and PR'ed out, feel free to tag myself (@etr2460) or anyone else in the file's git-blame for review.\r\n\r\nWe really appreciate any work done here, and are happy to help resolve any issues you may come across!\r\n\r\n## Tracker\r\n\r\n- [x] `src`\r\n  - [ ] `src/CRUD`\r\n  - [ ] `src/SqlLab`\r\n    - [ ] `src/SqlLab/actions`\r\n    - [ ] `src/SqlLab/components`\r\n      - [x] `src/SqlLab/components/SouthPane`\r\n    - [ ] `src/SqlLab/reducers`\r\n    - [ ] `src/SqlLab/utils`\r\n  - [x] `src/addSlice`\r\n  - [x] `src/api`\r\n  - [ ] `src/chart`\r\n    - [x] `src/common/components`\r\n      - [x] `src/common/components/Collapse`\r\n      - [x] `src/common/components/CronPicker`\r\n      - [x] `src/common/components/Dropdown`\r\n      - [x] `src/common/components/DropdownButton`\r\n      - [x] `src/common/components/InfoTooltip`\r\n      - [x] `src/common/components/Modal`\r\n      - [x] `src/common/components/Radio`\r\n      - [x] `src/common/components/Switch`\r\n      - [x] `src/common/components/Tabs`\r\n      - [x] `src/common/components/Tooltip`\r\n    - [x] `src/common/hooks`\r\n      - [x] `src/common/hooks/apiResources`\r\n      - [x] `src/common/hooks/useChangeEffect`\r\n      - [x] `src/common/hooks/useComponentDidMount`\r\n      - [x] `src/common/hooks/usePrevious`\r\n  - [x] `src/components`\r\n    - [x] `src/components/Alert`\r\n    - [ ] `src/components/AlteredSliceTag`\r\n    - [ ] `src/components/AnchorLink`\r\n    - [x] `src/components/AsyncAceEditor`\r\n    - [x] `src/components/AsyncEsmComponent`\r\n    - [ ] `src/components/AsyncSelect`\r\n    - [x] `src/components/Badge`\r\n    - [x] `src/components/Button`\r\n    - [x] `src/components/ButtonGroup`\r\n    - [x] `src/components/CachedLabel`\r\n    - [x] `src/components/CertifiedIcon`\r\n    - [x] `src/components/ChartIcon`\r\n    - [ ] `src/components/Checkbox`\r\n    - [ ] `src/components/ConfirmStatusChange`\r\n    - [ ] `src/components/CopyToClipboard`\r\n    - [x] `src/components/DatabaseSelector`\r\n    - [x] `src/components/DatePicker`\r\n    - [x] `src/components/DeleteModal`\r\n    - [x] `src/components/DynamicPlugins`\r\n    - [x] `src/components/EditableTitle`\r\n    - [ ] `src/components/ErrorBoundary`\r\n    - [x] `src/components/ErrorMessage`\r\n    - [x] `src/components/FacePile`\r\n    - [x] `src/components/FaveStar`\r\n    - [x] `src/components/FilterableTable`\r\n    - [x] `src/components/FlashProvider`\r\n    - [x] `src/components/FormLabel`\r\n    - [ ] `src/components/FormRow`\r\n    - [ ] `src/components/Icon`\r\n    - [ ] `src/components/IconTooltip`\r\n    - [ ] `src/components/Icons`\r\n    - [x] `src/components/ImportModal`\r\n    - [x] `src/components/IndeterminateCheckbox`\r\n    - [x] `src/components/Label`\r\n    - [x] `src/components/LastUpdated`\r\n    - [ ] `src/components/ListView`\r\n      - [x] `src/components/ListView/Filters`\r\n    - [ ] `src/components/ListViewCard`\r\n    - [x] `src/components/Loading`\r\n    - [x] `src/components/Menu`\r\n    - [ ] `src/components/ModalTrigger`\r\n    - [x] `src/components/NavDropdown`\r\n    - [x] `src/components/OmniContainer`\r\n    - [x] `src/components/Pagination`\r\n    - [x] `src/components/Popover`\r\n    - [x] `src/components/PopoverDropdown`\r\n    - [x] `src/components/PopoverSection`\r\n    - [x] `src/components/ProgressBar`\r\n    - [x] `src/components/RefreshLabel`\r\n    - [ ] `src/components/SearchInput`\r\n    - [ ] `src/components/Select`\r\n      - [x] `src/components/Select/WindowedSelect`\r\n    - [x] `src/components/Slider`\r\n    - [x] `src/components/SupersetResourceSelect`\r\n    - [x] `src/components/TableLoader`\r\n    - [ ] `src/components/TableSelector`\r\n    - [x] `src/components/TableView`\r\n    - [x] `src/components/Timer`\r\n    - [ ] `src/components/URLShortLinkButton`\r\n    - [x] `src/components/dataViewCommon`\r\n  - [ ] `src/dashboard`\r\n    - [ ] `src/dashboard/actions`\r\n    - [ ] `src/dashboard/components`\r\n      - [x] `src/dashboard/components/CrossFilterScopingModal`\r\n        - [x] `src/dashboard/components/CrossFilterScopingModal/CrossFilterScopingForm`\r\n        - [x] `src/dashboard/components/CrossFilterScopingModal/utils`\r\n      - [ ] `src/dashboard/components/CssEditor`\r\n      - [x] `src/dashboard/components/DashboardBuilder`\r\n      - [x] `src/dashboard/components/FiltersBadge`\r\n        - [x] `src/dashboard/components/FiltersBadge/DetailsPanel`\r\n        - [x] `src/dashboard/components/FiltersBadge/FilterIndicator`\r\n      - [ ] `src/dashboard/components/Header`\r\n        - [ ] `src/dashboard/components/Header/HeaderActionsDropdown`\r\n      - [ ] `src/dashboard/components/PropertiesModal`\r\n      - [ ] `src/dashboard/components/PublishedStatus`\r\n      - [x] `src/dashboard/components/SliceHeader`\r\n      - [ ] `src/dashboard/components/SliceHeaderControls`\r\n      - [ ] `src/dashboard/components/UndoRedoKeyListeners`\r\n      - [ ] `src/dashboard/components/dnd`\r\n        - [x] `src/dashboard/components/dnd/handleScroll`\r\n      - [ ] `src/dashboard/components/filterscope`\r\n      - [ ] `src/dashboard/components/gridComponents`\r\n        - [ ] `src/dashboard/components/gridComponents/new`\r\n      - [x] `src/dashboard/components/menu`\r\n        - [x] `src/dashboard/components/menu/ShareMenuItems`\r\n      - [x] `src/dashboard/components/nativeFilters`\r\n        - [x] `src/dashboard/components/nativeFilters/FilterBar`\r\n          - [x] `src/dashboard/components/nativeFilters/FilterBar/CascadeFilters`\r\n            - [x] `src/dashboard/components/nativeFilters/FilterBar/CascadeFilters/CascadeFilterControl`\r\n            - [x] `src/dashboard/components/nativeFilters/FilterBar/CascadeFilters/CascadePopover`\r\n          - [x] `src/dashboard/components/nativeFilters/FilterBar/FilterConfigurationLink`\r\n          - [x] `src/dashboard/components/nativeFilters/FilterBar/FilterControls`\r\n          - [x] `src/dashboard/components/nativeFilters/FilterBar/FilterSets`\r\n            - [x] `src/dashboard/components/nativeFilters/FilterBar/FilterSets/utils`\r\n          - [x] `src/dashboard/components/nativeFilters/FilterBar/Header`\r\n        - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal`\r\n          - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal/FiltersConfigForm`\r\n            - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal/FiltersConfigForm/FilterScope`\r\n          - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal/Footer`\r\n      - [ ] `src/dashboard/components/resizable`\r\n    - [ ] `src/dashboard/containers`\r\n    - [ ] `src/dashboard/fixtures`\r\n    - [ ] `src/dashboard/reducers`\r\n    - [ ] `src/dashboard/util`\r\n      - [x] `src/dashboard/util/charts`\r\n      - [ ] `src/dashboard/util/logging`\r\n  - [x] `src/dataMask`\r\n  - [ ] `src/datasource`\r\n  - [ ] `src/explore`\r\n    - [ ] `src/explore/actions`\r\n    - [ ] `src/explore/components`\r\n      - [x] `src/explore/components/DataTableControl`\r\n      - [x] `src/explore/components/DataTablesPane`\r\n      - [x] `src/explore/components/DatasourcePanel`\r\n        - [x] `src/explore/components/DatasourcePanel/DatasourcePanelDragWrapper`\r\n      - [ ] `src/explore/components/DisplayQueryButton`\r\n      - [x] `src/explore/components/PropertiesModal`\r\n      - [ ] `src/explore/components/controls`\r\n        - [ ] `src/explore/components/controls/AnnotationLayerControl`\r\n        - [ ] `src/explore/components/controls/CollectionControl`\r\n        - [x] `src/explore/components/controls/CustomListItem`\r\n        - [ ] `src/explore/components/controls/DatasourceControl`\r\n        - [x] `src/explore/components/controls/DateFilterControl`\r\n          - [x] `src/explore/components/controls/DateFilterControl/components`\r\n          - [x] `src/explore/components/controls/DateFilterControl/utils`\r\n        - [x] `src/explore/components/controls/DndColumnSelectControl`\r\n          - [x] `src/explore/components/controls/DndColumnSelectControl/utils`\r\n        - [ ] `src/explore/components/controls/FilterBoxItemControl`\r\n        - [ ] `src/explore/components/controls/FilterControl`\r\n          - [ ] `src/explore/components/controls/FilterControl/AdhocFilter`\r\n          - [ ] `src/explore/components/controls/FilterControl/AdhocFilterControl`\r\n          - [ ] `src/explore/components/controls/FilterControl/AdhocFilterEditPopover`\r\n          - [ ] `src/explore/components/controls/FilterControl/AdhocFilterEditPopoverSimpleTabContent`\r\n          - [ ] `src/explore/components/controls/FilterControl/AdhocFilterEditPopoverSqlTabContent`\r\n          - [ ] `src/explore/components/controls/FilterControl/AdhocFilterOption`\r\n          - [x] `src/explore/components/controls/FilterControl/AdhocFilterPopoverTrigger`\r\n        - [ ] `src/explore/components/controls/FixedOrMetricControl`\r\n        - [ ] `src/explore/components/controls/MetricControl`\r\n          - [ ] `src/explore/components/controls/MetricControl/AdhocMetricEditPopover`\r\n        - [x] `src/explore/components/controls/OptionControls`\r\n        - [ ] `src/explore/components/controls/SelectAsyncControl`\r\n        - [x] `src/explore/components/controls/TextControl`\r\n        - [ ] `src/explore/components/controls/TimeSeriesColumnControl`\r\n    - [ ] `src/explore/controlPanels`\r\n    - [x] `src/explore/controlUtils`\r\n    - [ ] `src/explore/exploreUtils`\r\n    - [ ] `src/explore/reducers`\r\n  - [x] `src/filters`\r\n    - [x] `src/filters/components`\r\n      - [x] `src/filters/components/Range`\r\n      - [x] `src/filters/components/Select`\r\n      - [x] `src/filters/components/Time`\r\n      - [x] `src/filters/components/TimeColumn`\r\n      - [x] `src/filters/components/TimeGrain`\r\n  - [x] `src/logger`\r\n    - [x] `src/logger/actions`\r\n  - [x] `src/messageToasts`\r\n    - [x] `src/messageToasts/actions`\r\n    - [x] `src/messageToasts/components`\r\n    - [ ] `src/messageToasts/containers`\r\n    - [x] `src/messageToasts/enhancers`\r\n    - [ ] `src/messageToasts/reducers`\r\n    - [ ] `src/messageToasts/utils`\r\n  - [ ] `src/middleware`\r\n  - [ ] `src/modules`\r\n  - [x] `src/profile`\r\n    - [x] `src/profile/components`\r\n  - [x] `src/setup`\r\n  - [ ] `src/showSavedQuery`\r\n  - [x] `src/types`\r\n  - [ ] `src/utils`\r\n  - [x] `src/views`\r\n    - [x] `src/views/CRUD`\r\n      - [x] `src/views/CRUD/alert`\r\n        - [x] `src/views/CRUD/alert/components`\r\n      - [x] `src/views/CRUD/annotation`\r\n      - [x] `src/views/CRUD/annotationlayers`\r\n      - [x] `src/views/CRUD/chart`\r\n      - [x] `src/views/CRUD/csstemplates`\r\n      - [x] `src/views/CRUD/dashboard`\r\n      - [x] `src/views/CRUD/data`\r\n          - [x] `src/views/CRUD/data/components/SyntaxHighlighterCopy`\r\n        - [x] `src/views/CRUD/data/database`\r\n        - [x] `src/views/CRUD/data/dataset`\r\n        - [x] `src/views/CRUD/data/query`\r\n        - [x] `src/views/CRUD/data/savedquery`\r\n      - [x] `src/views/CRUD/welcome`\r\n    - [ ] `src/visualizations/FilterBox`\r\n    - [ ] `src/visualizations/TimeTable`\r\n    - [ ] `src/visualizations/presets`\r\n  - [ ] `spec/__mocks__`\r\n  - [ ] `spec/fixtures`\r\n  - [ ] `spec/helpers`\r\n    - [ ] `spec/javascripts/CRUD`\r\n    - [x] `spec/javascripts/addSlice`\r\n    - [ ] `spec/javascripts/chart`\r\n    - [ ] `spec/javascripts/components`\r\n      - [ ] `spec/javascripts/dashboard/actions`\r\n      - [ ] `spec/javascripts/dashboard/components`\r\n        - [ ] `spec/javascripts/dashboard/components/dnd`\r\n        - [ ] `spec/javascripts/dashboard/components/gridComponents`\r\n          - [ ] `spec/javascripts/dashboard/components/gridComponents/new`\r\n        - [ ] `spec/javascripts/dashboard/components/menu`\r\n        - [x] `spec/javascripts/dashboard/components/nativeFilters`\r\n        - [ ] `spec/javascripts/dashboard/components/resizable`\r\n      - [x] `spec/javascripts/dashboard/fixtures`\r\n      - [ ] `spec/javascripts/dashboard/reducers`\r\n      - [ ] `spec/javascripts/dashboard/util`\r\n    - [ ] `spec/javascripts/datasource`\r\n    - [ ] `spec/javascripts/explore`\r\n      - [ ] `spec/javascripts/explore/components`\r\n        - [x] `spec/javascripts/explore/components/DateFilterControl`\r\n    - [x] `spec/javascripts/filters`\r\n    - [ ] `spec/javascripts/messageToasts`\r\n      - [ ] `spec/javascripts/messageToasts/components`\r\n      - [ ] `spec/javascripts/messageToasts/reducers`\r\n      - [ ] `spec/javascripts/messageToasts/utils`\r\n    - [ ] `spec/javascripts/middleware`\r\n    - [ ] `spec/javascripts/modules`\r\n    - [x] `spec/javascripts/profile`\r\n    - [ ] `spec/javascripts/showSavedQuery`\r\n    - [ ] `spec/javascripts/sqllab`\r\n      - [ ] `spec/javascripts/sqllab/actions`\r\n      - [ ] `spec/javascripts/sqllab/reducers`\r\n      - [ ] `spec/javascripts/sqllab/utils`\r\n    - [ ] `spec/javascripts/utils`\r\n        - [ ] `spec/javascripts/views/CRUD/alert`\r\n        - [ ] `spec/javascripts/views/CRUD/annotation`\r\n        - [ ] `spec/javascripts/views/CRUD/annotationlayers`\r\n        - [ ] `spec/javascripts/views/CRUD/chart`\r\n        - [ ] `spec/javascripts/views/CRUD/csstemplates`\r\n        - [ ] `spec/javascripts/views/CRUD/dashboard`\r\n          - [ ] `spec/javascripts/views/CRUD/data/database`\r\n          - [ ] `spec/javascripts/views/CRUD/data/dataset`\r\n          - [ ] `spec/javascripts/views/CRUD/data/savedquery`\r\n        - [x] `spec/javascripts/views/CRUD/welcome`"], ["This should be linked to from the page added in #14662, and describe how we aim to support all modern browsers, including Chrome, Firefox, and Edge, etc.\r\n\r\nSomething like this in terms of content:\r\n\r\nhttps://support.discordapp.com/hc/en-us/articles/213491697-What-are-the-OS-system-requirements-for-Discord-\r\n\r\nWe can also link there from `/apps`.\r\n\r\nTagging this as difficult because I think it requires some careful writing to end up with a result that's better than having nothing."], ['Current, in Explore left data panel Columns section, user see \r\n    1) actual column name that without a label e.g.` job_intr_dataengn`(column name, no lable)\r\nor 2) label name of columns e.g. `yt_codingtuts360 label`(label name of `yt_codingtuts360`)\r\nfrom the view. \r\nwhen user hover on the text, exact same information\r\nchange: display actual column name in tooltip, when there\'s a label \r\n<img width="278" alt="Screen Shot 2021-02-19 at 5 13 13 PM" src="https://user-images.githubusercontent.com/67837651/108577451-ff320200-72d5-11eb-871a-1f08ee6df1b6.png">\r\n\r\n<img width="293" alt="Screen Shot 2021-02-19 at 5 08 39 PM" src="https://user-images.githubusercontent.com/67837651/108577236-2805c780-72d5-11eb-8377-5a157f0122a1.png">\r\n'], ['Currently, we rely on `AllGatherGrad` to compute gather for GPUs.\r\n\r\nTODO:\r\n- [] Extend this class to support TPU\r\n- [] Add tests\r\n'], ["<!--\r\n  Hi there! Thank you for wanting to make Poetry better.\r\n\r\n  Before you submit this; let's make sure of a few things.\r\n  Please make sure the following boxes are ticked if they are correct.\r\n  If not, please try and fulfill these first.\r\n-->\r\n\r\n<!-- Checked checkbox should look like this: [x] -->\r\n- [x] I have searched the [issues](https://github.com/python-poetry/poetry/issues) of this repo and believe that this is not a duplicate.\r\n- [x] I have searched the [documentation](https://python-poetry.org/docs/) and believe that my question is not covered.\r\n\r\n## Feature Request\r\n<!-- Now feel free to write your idea for improvement. Thanks again 🙌 ❤️ -->\r\nCurrently the prompt is either way too long in the case of generated global venvs, or useless in the case of `.venv`.  \r\nVirtualenv already supports passing `--prompt` to customize this, so I propose using the project name.  \r\n\r\nI do not use `poetry env` or anything other than one venv per project, so I'm open to propositions on other customization options."], ["Content from the 'For Developers' content section of the wiki should be included in the pandas documentation where relevant. \r\n\r\n- [ ] Code Style and Conventions\r\n- [ ] Testing #20501\r\n- [ ] Documenting new features and bug fixes\r\n- [ ] Using Git\r\n- [ ] Choosing Exceptions to Raise\r\n- [ ] Tips & Tricks for pandas dev\r\n- [ ] A quick overview of pandas.util\r\n- [ ] Git Workflows\r\n- [ ] Pandas Development FAQ\r\n- [ ] Building On Windows\r\n\r\nPRs welcome for any of the above sections."], ["When merging a dask dataframe, the resulting index is duplicated - seems to be because of the number of partitions. See example below:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\na = dd.from_pandas(pd.DataFrame({'a': [1,2,3,4]}), npartitions=2)\r\nb = pd.DataFrame({'a': [1,2,3,4], 'b': [2,3,4,5]})\r\n\r\na.merge(b, on='a').compute()\r\n```\r\nReturns\r\n\r\n|   | a | b |\r\n|---|---|---|\r\n| 0 | 1 | 2 |\r\n| 1 | 2 | 3 |\r\n| 0 | 3 | 4 |\r\n| 1 | 4 | 5 |\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.27.0\r\n- Python version: 3.8.5\r\n- Operating System: Mac OS\r\n- Install method (conda, pip, source): pip\r\n"], ['## Screenshot\r\n\r\n![Présentation sans titre](https://user-images.githubusercontent.com/79460908/110811066-a75c3a80-8286-11eb-80f4-0d3e8cffcc78.png)\r\n\r\n\r\n## Description\r\n\r\nI\'ve set custom colors in the Dashboard properties.\r\nThose colors seems to be pretty well taken into account but bar charts kind of convert them into "pastel colors" (see screenshot).\r\n\r\nBy the way, the pie chart colors are not exactly the good ones (#f97f44 instead of #ff7f44 in this example) but the difference is not visible, contrary to bar charts.  \r\n\r\n## Environment\r\n\r\nsuperset version: 1.0.1\r\nbrowser : chrome 89.0'], ['## 🚀 Feature\r\n\r\nadd/fix typing in all PL codebase, this will yield in multiple PR as we prefer to split the work into smaller peace\r\n\r\n### Additional context\r\n\r\nin #5021  we introduced a new ignore list so after it is merged, take out a single ignored item and fix typing for that part \r\n> each item + its fix = one PR\r\n\r\n### Sections\r\n\r\n- [ ] `pytorch_lightning.callbacks.*` #7035\r\n- [ ] `pytorch_lightning.core.*`  #7035\r\n- [ ] `pytorch_lightning.accelerators.*`  #7035\r\n- [ ] `pytorch_lightning.loggers.*` #7035\r\n- [ ] `pytorch_lightning.logging.*` #7035\r\n- [ ] `pytorch_lightning.metrics.*` <---| @hassiahk\r\n- [ ] `pytorch_lightning.overrides.*`\r\n- [ ] `pytorch_lightning.profiler.*`\r\n- [ ] `pytorch_lightning.pt_overrides.*`\r\n- [ ] `pytorch_lightning.plugins.*`  #7022\r\n- [ ] `pytorch_lightning.root_module.*`\r\n- [ ] `pytorch_lightning.trainer.*`\r\n- [ ] `pytorch_lightning.distributed.*`\r\n- [ ] `pytorch_lightning.tuner.*`\r\n- [ ] `pytorch_lightning.utilities.*`\r\n- [ ] `pl_examples.* by` <---| @janhenriklambrechts\r\n- [ ] `benchmarks.*`\r\n- [ ] `tests.*`\r\n\r\n_This sections can be split even to smaller, most likely still stay per package/folder_'], ["<!--\r\n\r\nWelcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.\r\nDon't worry if they're not all applicable; just try to include what you can :-)\r\n\r\nIf you need to include code snippets or logs, please put them in fenced code\r\nblocks.  If they're super-long, please use the details tag like\r\n<details><summary>super-long log</summary> lots of stuff </details>\r\n\r\nPlease delete these comment blocks before submitting the issue.\r\n\r\n-->\r\n\r\n**Description**\r\nThe `Variable` feature caused us some issue because of the performance impact it can have if badly used. See [here](https://issues.apache.org/jira/browse/AIRFLOW-29).\r\nTo a rather new user, it is not obvious that how the DAG is written can have a considerable impact in executing it.\r\nIn our case we were generating 100s of tasks each using `Variable.get` in its configuration, making loading the Dag extermely slow (and timeout).\r\n\r\n**Use case / motivation**\r\n\r\nOur suggestion would be that emphasis is put in the docs in the Concept/Variables section of this coupling between how Variable is used and performance/stability.\r\n\r\n\r\n**Are you willing to submit a PR?**\r\n\r\nI'll try to suggest something.\r\n<!--- We accept contributions! -->\r\n\r\n**Related Issues**\r\n\r\nhttps://issues.apache.org/jira/browse/AIRFLOW-29\r\n\r\nhttps://github.com/apache/airflow/issues/1380\r\n"], ['### Problem\r\nI have a hard time grasping what happens when jumping between 2d and 3d scatters because the y changes from the vertical direction to the depth direction.\r\n\r\n![image](https://user-images.githubusercontent.com/14371165/112721976-30ba7080-8f07-11eb-972b-dea2a3f16128.png)\r\n\r\n### Proposed Solution\r\nA simple way to tell matplotlib to put the y axis vertically instead of the z axis. Either by improving `zdir` or adding a new parameter.\r\nThis will make it easier to visualize what happens since we are rotating this block of data as little as possible and it makes it easy to continue working with the plot as all x,y,z-methods continues to make sense.\r\n\r\nThe hack of interchanging the data and labels works if all you want is to get the plot working. But it gets rather unintuitive for downstream users if they want to further process the plot with for example `set_ylim` or `set_zlim`.\r\n\r\nRotating with ax.view_init() is very difficult or impossible to get good as it\'s difficult to get the same init angles and in interactive mode it feels completely opposite to rotate compared to the normal settings.\r\n\r\n### Example code\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nx = np.array([0, 1, 2, 4])\r\ny = np.array([5, 10])\r\nz = np.array([100, 150, 200])\r\nX, Y, Z = np.meshgrid(*[x, y, z], indexing="ij")\r\nc = np.arange(0, X.size)\r\n\r\n\r\nax = plt.subplot(1, 3, 1)\r\nax.scatter(X, Y, c=c)\r\nax.set_title("x, y-plot, difficult to tell what\'s going on.")\r\nax.set_xlabel("x")\r\nax.set_ylabel("y")\r\n\r\nax = plt.subplot(1, 3, 2, projection="3d")\r\nax.scatter(X, Y, Z, c=c)\r\nax.set_title(\r\n    "3d plot, but now y is going in the depthwise direction.\\nHave to relearn the plot."\r\n)\r\nax.set_xlabel("x")\r\nax.set_ylabel("y")\r\nax.set_zlabel("z")\r\n\r\nax = plt.subplot(1, 3, 3, projection="3d")\r\nax.scatter(X, Z, Y, c=c)\r\nax.set_title(\r\n    (\r\n        "Interchange y and z, intuitive and builds on top of 2dplot\\n"\r\n        "But now set_*labels doesn\'t make sense."\r\n    )\r\n)\r\nax.set_xlabel("x")\r\nax.set_ylabel("z")\r\nax.set_zlabel("y")\r\n```'], ['Estimator has too many undocumented attributes:\r\n- [ ] test_fit_docstring_attributes[OrthogonalMatchingPursuit-OrthogonalMatchingPursuit]\r\n- [ ] test_fit_docstring_attributes[Lasso-Lasso]\r\n- [ ] test_fit_docstring_attributes[LarsCV-LarsCV]\r\n- [ ] test_fit_docstring_attributes[Birch-Birch]\r\n\r\n\r\n'], ['Apache version of #3873'], ['Right now:\n\n```\n$ ipython notebook\n[... snip]\n        raise ImportError("%s requires pyzmq >= %s"%(module, minimum_version))\nImportError: IPython.zmq requires pyzmq >= 2.1.4\n```\n\nThis is not super helpful. More helpful would be to have a message like:\n\n```\n$ ipython notebook\nYou\'re missing some dependencies required to run the IPython notebook. You can\ninstall them by doing:\n\n    easy_install ipython[zmq,qtconsole,notebook,test]\n\n Or see read this for more information: http://ipython.org/ipython-doc/dev/install/install.html#installnotebook\n```\n\nBonus points if there was a PyPI metapackage which just pulls in the right dependencies, such as `ipython-notebook`.\n'], ['Falcon modules import a number of helper functions that do not begin with an underscore, although such functions are not meant to be part of the public API. Remedy this in the next major version of the framework (as it would be a breaking change).'], ['Hi there,\r\n\r\nI create a unofficial winston-zulip for log : https://github.com/huneau/winston-zulip'], ['**Description**\r\nAdd an example DAG to the [example_dags ](https://github.com/apache/airflow/tree/master/airflow/providers/microsoft/azure/example_dags) directory of the azure providers.\r\n\r\n**Use case / motivation**\r\nThe [Data Factory hooks](https://github.com/apache/airflow/blob/99c74968180ab7bc6d7152ec4233440b62a07969/airflow/providers/microsoft/azure/hooks/azure_data_factory.py) are new with [apache-airflow-providers-microsoft-azure 1.2.0](https://pypi.org/project/apache-airflow-providers-microsoft-azure/1.2.0/), but there are no examples or documentation on how to use them.\r\n\r\nAs a new user, it is helpful to have a known working example, especially for troubleshooting config/setup (missing providers, etc).\r\n\r\n'], ['### Cannot pass kwargs to pkg.upgrade function in aptpkg.py. It does not allow to set --allow-downgrades option for apt command.\r\nSee code block for explanation:\r\nhttps://github.com/saltstack/salt/blob/develop/salt/modules/aptpkg.py#L1047-L1054\r\n\r\n### Setup\r\nsalt-call 2017.7.8 (Nitrogen)\r\nUbuntu 16.04 Xenial\r\n\r\n### Steps to Reproduce Issue\r\nsalt-call pkg.upgrade dist_upgrade=true kwargs=["allow_downgrades"]\r\n\r\n### Versions Report\r\nmaster = minion = Nitrogen\r\n\r\nExpected:\r\nkwargs=["allow_downgrades"]\r\n\r\n    if kwargs.get(\'allow_downgrades\', False):\r\n        cmd.append(\'--allow-downgrades\')\r\n\r\nOnce fixed, please backport to older releases as well: 2016.x/2017.x/2018.x'], ["I have been creating many spiders recently, and I noticed I had to add below line every time\r\n\r\n`from scrapy.http import Request`\r\n\r\nI think for most cases we need the Request to crawl to other pages, only for very simple spider we don't need that. Is it possible to include that line to the template?"], ['## 📚 Documentation\r\nThis is only a suggestion, but I find it useful to have what exceptions functions/classes can raise in the docs, so I\'d like to suggest adding the `Raises:` section to public functions/classes. \r\n\r\n### Progress\r\n- [ ] `pytorch_lightning/core/*`\r\n- [ ] `pytorch_lightning/plugins/*`\r\n- [ ] `pytorch_lightning/trainer/*`\r\n- [ ] `pytorch_lightning/utilities/*`\r\n- [x] `pytorch_lightning/accelerators/*` (#6327 @AlKun25)\r\n- [x] `pytorch_lightning/callbacks/*` (#5541 @akihironitta)\r\n- [x] ~`pytorch_lightning/distributed/*`~ (no exceptions included)\r\n- [x] `pytorch_lightning/loggers/*` (#6171 @AlKun25)\r\n- [x] `pytorch_lightning/metrics/classification/*` (#6190 @dipam7)\r\n- [x] `pytorch_lightning/metrics/functional/*` (#6273 @dipam7)\r\n- [x] `pytorch_lightning/metrics/regression/*` (#6202 @prajakta0111)\r\n- [x] ~`pytorch_lightning/overrides/*`~ (no exceptions included)\r\n- [x] `pytorch_lightning/profiler/*`(#6229 @AlKun25)\r\n- [x] `pytorch_lightning/tuner/*` (#6264 @AlKun25)\r\n\r\n\r\n<details>\r\n<summary>list of lines raising exceptions (very long)</summary>\r\n\r\nThe output of `grep -rn "raise " pytorch_lightning | grep -v NotImplementedError` can include false positives (where exception docs may not be needed).\r\n\r\n```\r\npytorch_lightning/callbacks/lr_monitor.py:72:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/lr_monitor.py:92:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/finetuning.py:202:        raise MisconfigurationException(\r\npytorch_lightning/callbacks/model_checkpoint.py:246:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/model_checkpoint.py:252:                raise MisconfigurationException(\r\npytorch_lightning/callbacks/model_checkpoint.py:291:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/model_checkpoint.py:330:            raise ValueError(".save_function() not set")\r\npytorch_lightning/callbacks/model_checkpoint.py:484:            raise MisconfigurationException(m)\r\npytorch_lightning/callbacks/gradient_accumulation_scheduler.py:58:            raise TypeError("Empty dict cannot be interpreted correct")\r\npytorch_lightning/callbacks/gradient_accumulation_scheduler.py:62:                raise TypeError("All epoches and accumulation factor must be integers")\r\npytorch_lightning/callbacks/gradient_accumulation_scheduler.py:66:            raise IndexError(\r\npytorch_lightning/callbacks/early_stopping.py:101:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/early_stopping.py:130:                raise RuntimeError(error_msg)\r\npytorch_lightning/callbacks/gpu_stats_monitor.py:88:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/gpu_stats_monitor.py:103:            raise MisconfigurationException(\r\npytorch_lightning/callbacks/gpu_stats_monitor.py:108:            raise MisconfigurationException(\r\npytorch_lightning/loggers/wandb.py:99:            raise ImportError(\'You want to use `wandb` logger which is not installed yet,\'  # pragma: no-cover\r\npytorch_lightning/loggers/test_tube.py:95:            raise ImportError(\'You want to use `test_tube` logger which is not installed yet,\'\r\npytorch_lightning/loggers/comet.py:136:            raise ImportError(\r\npytorch_lightning/loggers/comet.py:158:            # If neither api_key nor save_dir are passed as arguments, raise an exception\r\npytorch_lightning/loggers/comet.py:159:            raise MisconfigurationException("CometLogger requires either api_key or save_dir during initialization.")\r\npytorch_lightning/loggers/mlflow.py:97:            raise ImportError(\'You want to use `mlflow` logger which is not installed yet,\'\r\npytorch_lightning/loggers/neptune.py:191:            raise ImportError(\'You want to use `neptune` logger which is not installed yet,\'\r\npytorch_lightning/utilities/device_parser.py:78:        raise MisconfigurationException("GPUs requested but none are available.")\r\npytorch_lightning/utilities/device_parser.py:108:        raise MisconfigurationException("`tpu_cores` can only be 1, 8 or [<1-8>]")\r\npytorch_lightning/utilities/device_parser.py:111:        raise MisconfigurationException(\'No TPU devices were found.\')\r\npytorch_lightning/utilities/device_parser.py:140:            raise MisconfigurationException(f"""\r\npytorch_lightning/utilities/device_parser.py:179:        raise MisconfigurationException("Device ID\'s (GPU/TPU) must be int, string or sequence of ints or None.")\r\npytorch_lightning/utilities/apply_func.py:35:        raise MisconfigurationException(\r\npytorch_lightning/utilities/apply_func.py:43:        raise MisconfigurationException(\r\npytorch_lightning/utilities/apply_func.py:155:        raise MisconfigurationException(\r\npytorch_lightning/utilities/parsing.py:55:    raise ValueError(f\'invalid truth value {val}\')\r\npytorch_lightning/utilities/parsing.py:185:            raise AttributeError(f\'Missing attribute "{key}"\') from exp\r\npytorch_lightning/utilities/parsing.py:239:        raise ValueError(f\'{attribute} is neither stored in the model namespace\'\r\npytorch_lightning/utilities/parsing.py:250:        raise ValueError(f\'{attribute} is neither stored in the model namespace\'\r\npytorch_lightning/utilities/data.py:35:            raise ValueError(\'`Dataloader` returned 0 length.\'\r\npytorch_lightning/utilities/device_dtype_mixin.py:36:        raise RuntimeError(\'Cannot set the dtype explicitly. Please use module.to(new_dtype).\')\r\npytorch_lightning/utilities/device_dtype_mixin.py:51:        raise RuntimeError(\'Cannot set the device explicitly. Please use module.to(new_device).\')\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:146:            raise MisconfigurationException(\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:168:            raise MisconfigurationException(\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:193:            raise MisconfigurationException(\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:201:            raise MisconfigurationException(\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:231:                raise MisconfigurationException(\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:263:            raise MisconfigurationException(\r\npytorch_lightning/plugins/ddp_sequential_plugin.py:330:            raise MisconfigurationException(\r\npytorch_lightning/plugins/plugin_connector.py:68:                    raise MisconfigurationException(m)\r\npytorch_lightning/plugins/plugin_connector.py:83:                    raise MisconfigurationException(m)\r\npytorch_lightning/plugins/plugin_connector.py:97:                    raise MisconfigurationException(m)\r\npytorch_lightning/plugins/plugin_connector.py:117:                raise MisconfigurationException(\r\npytorch_lightning/plugins/sharded_plugin.py:50:            raise MisconfigurationException(\r\npytorch_lightning/plugins/sharded_plugin.py:91:            raise MisconfigurationException(\r\npytorch_lightning/plugins/apex.py:62:            raise Exception(\'apex unscale error\')\r\npytorch_lightning/tuner/auto_gpu_select.py:21:        raise MisconfigurationException(\r\npytorch_lightning/tuner/auto_gpu_select.py:46:    raise RuntimeError("No GPUs available.")\r\npytorch_lightning/tuner/lr_finder.py:58:            raise MisconfigurationException(\r\npytorch_lightning/tuner/lr_finder.py:67:            raise MisconfigurationException(\r\npytorch_lightning/tuner/lr_finder.py:289:                raise MisconfigurationException(\r\npytorch_lightning/tuner/batch_size_scaling.py:77:        raise MisconfigurationException(\r\npytorch_lightning/tuner/batch_size_scaling.py:87:        raise MisconfigurationException(\'The batch scaling feature cannot be used with dataloaders\'\r\npytorch_lightning/tuner/batch_size_scaling.py:111:        raise ValueError(\'mode in method `scale_batch_size` can only be `power` or `binsearch\')\r\npytorch_lightning/tuner/batch_size_scaling.py:192:                raise  # some other error not memory related\r\npytorch_lightning/tuner/batch_size_scaling.py:238:                raise  # some other error not memory related\r\npytorch_lightning/profiler/profilers.py:146:            raise ValueError(\r\npytorch_lightning/profiler/profilers.py:154:            raise ValueError(\r\npytorch_lightning/profiler/profilers.py:253:            raise ValueError(  # pragma: no-cover\r\npytorch_lightning/core/step_result.py:888:            raise ValueError(f"Expected str for \'name\' but got {type(name)}")\r\npytorch_lightning/core/step_result.py:890:            raise ValueError(f"Expected str for \'filename\' but got {type(name)}")\r\npytorch_lightning/core/optimizer.py:46:            raise MisconfigurationException(\r\npytorch_lightning/core/optimizer.py:161:            raise MisconfigurationException(\r\npytorch_lightning/core/optimizer.py:273:                raise MisconfigurationException("When closure is provided, it should be a function")\r\npytorch_lightning/core/saving.py:139:                raise ValueError(\'.csv, .yml or .yaml is required for `hparams_file`\')\r\npytorch_lightning/core/saving.py:312:        raise RuntimeError(f"Missing folder: {os.path.dirname(tags_csv)}.")\r\npytorch_lightning/core/saving.py:355:        raise RuntimeError(f"Missing folder: {os.path.dirname(config_yaml)}.")\r\npytorch_lightning/core/lightning.py:230:                raise MisconfigurationException(m)\r\npytorch_lightning/core/lightning.py:234:                raise MisconfigurationException(m)\r\npytorch_lightning/core/lightning.py:252:                raise MisconfigurationException(\r\npytorch_lightning/core/lightning.py:1347:            raise MisconfigurationException(\r\npytorch_lightning/core/lightning.py:1432:            raise MisconfigurationException(\r\npytorch_lightning/core/lightning.py:1542:            raise ValueError(f"Primitives {PRIMITIVE_TYPES} are not allowed.")\r\npytorch_lightning/core/lightning.py:1544:            raise ValueError(f"Unsupported config type of {type(hp)}.")\r\npytorch_lightning/core/lightning.py:1586:                raise ValueError(\r\npytorch_lightning/core/lightning.py:1661:                    raise ValueError(\r\npytorch_lightning/core/lightning.py:1671:            raise ValueError("The \'method\' parameter only supports \'script\' or \'trace\',"\r\npytorch_lightning/trainer/logging.py:150:                    raise RuntimeError(\r\npytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:520:            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/logger_connector/callback_hook_validator.py:37:                raise MisconfigurationException(msg)\r\npytorch_lightning/trainer/connectors/logger_connector/callback_hook_validator.py:41:                raise MisconfigurationException(msg)\r\npytorch_lightning/trainer/connectors/logger_connector/callback_hook_validator.py:43:            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py:41:        raise RuntimeError(f"Invalid stage {stage_or_testing} of type {type(stage_or_testing)} given")\r\npytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py:126:            raise Exception("The provided opt_metric should be a Result Object. Something is wrong")\r\npytorch_lightning/trainer/connectors/precision_connector.py:73:            raise ModuleNotFoundError(\r\npytorch_lightning/trainer/connectors/profiler_connector.py:36:            raise MisconfigurationException("Only None, bool, str and subclasses of `BaseProfiler`"\r\npytorch_lightning/trainer/connectors/profiler_connector.py:51:                raise ValueError("When passing string value for the `profiler` parameter of"\r\npytorch_lightning/trainer/connectors/training_trick_connector.py:39:            raise MisconfigurationException("track_grad_norm can be an int, a float or \'inf\' (infinity norm).")\r\npytorch_lightning/trainer/connectors/training_trick_connector.py:55:            raise TypeError("Gradient accumulation supports only int and dict types")\r\npytorch_lightning/trainer/connectors/data_connector.py:88:            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/checkpoint_connector.py:129:            raise KeyError(\r\npytorch_lightning/trainer/connectors/checkpoint_connector.py:135:            raise ValueError(\r\npytorch_lightning/trainer/connectors/checkpoint_connector.py:160:            raise MisconfigurationException(m)\r\npytorch_lightning/trainer/connectors/optimizer_connector.py:56:                            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/debugging_connector.py:37:            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/debugging_connector.py:43:            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/debugging_connector.py:92:        raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/callback_connector.py:70:            raise MisconfigurationException(\r\npytorch_lightning/trainer/connectors/callback_connector.py:89:            raise MisconfigurationException(\r\npytorch_lightning/trainer/data_loading.py:99:                raise MisconfigurationException(\r\npytorch_lightning/trainer/data_loading.py:167:            raise MisconfigurationException(\r\npytorch_lightning/trainer/data_loading.py:178:                raise ValueError(\r\npytorch_lightning/trainer/data_loading.py:187:                    raise MisconfigurationException(\r\npytorch_lightning/trainer/data_loading.py:265:                    raise MisconfigurationException(\r\npytorch_lightning/trainer/data_loading.py:272:                    raise MisconfigurationException(\r\npytorch_lightning/trainer/training_loop.py:85:            raise MisconfigurationException(\r\npytorch_lightning/trainer/training_loop.py:330:                raise MisconfigurationException("In manual optimization, `training_step` should not return a Tensor")\r\npytorch_lightning/trainer/training_loop.py:483:            raise MisconfigurationException(\r\npytorch_lightning/trainer/training_loop.py:500:            raise MisconfigurationException(\r\npytorch_lightning/trainer/training_loop.py:910:                raise ValueError(\r\npytorch_lightning/trainer/training_tricks.py:50:            raise ValueError(\r\npytorch_lightning/trainer/training_tricks.py:57:                raise ValueError(\r\npytorch_lightning/trainer/configuration_validator.py:46:            raise MisconfigurationException(\r\npytorch_lightning/trainer/configuration_validator.py:56:            raise MisconfigurationException(\r\npytorch_lightning/trainer/configuration_validator.py:66:            raise MisconfigurationException(\r\npytorch_lightning/trainer/configuration_validator.py:90:            raise MisconfigurationException(\r\npytorch_lightning/trainer/configuration_validator.py:97:            raise MisconfigurationException(\r\npytorch_lightning/trainer/supporters.py:173:                raise ValueError(\r\npytorch_lightning/trainer/supporters.py:236:            raise StopIteration\r\npytorch_lightning/trainer/supporters.py:270:            raise MisconfigurationException(\r\npytorch_lightning/trainer/supporters.py:300:            raise MisconfigurationException(f"Invalid Mode: {mode}")\r\npytorch_lightning/trainer/supporters.py:368:            raise MisconfigurationException(f"Invalid Mode: {mode}")\r\npytorch_lightning/trainer/supporters.py:415:            raise ValueError(f\'Invalid Datatype for loaders: {type(self.loaders).__name__}\')\r\npytorch_lightning/trainer/supporters.py:447:        raise TypeError(f\'Got Type {type(all_lengths).__name__}, but expected one of Sequence, int or Mapping\')\r\npytorch_lightning/trainer/optimizers.py:63:                raise ValueError("A frequency must be given to each optimizer.")\r\npytorch_lightning/trainer/optimizers.py:69:            raise MisconfigurationException(\r\npytorch_lightning/trainer/optimizers.py:113:                    raise MisconfigurationException(\r\npytorch_lightning/trainer/optimizers.py:120:                    raise MisconfigurationException(\r\npytorch_lightning/trainer/optimizers.py:128:                    raise MisconfigurationException(\r\npytorch_lightning/trainer/optimizers.py:139:                raise ValueError(f\'The provided lr scheduler "{scheduler}" is invalid\')\r\npytorch_lightning/trainer/optimizers.py:193:        raise MisconfigurationException(\r\npytorch_lightning/trainer/evaluation_loop.py:189:            raise MisconfigurationException(m)\r\npytorch_lightning/trainer/trainer.py:754:            raise MisconfigurationException(\r\npytorch_lightning/trainer/trainer.py:775:            raise MisconfigurationException(\r\npytorch_lightning/overrides/data_parallel.py:76:                raise RuntimeError("module must have its parameters and buffers "\r\npytorch_lightning/overrides/data_parallel.py:133:                    raise ValueError(\'All dicts must have the same number of keys\')\r\npytorch_lightning/overrides/data_parallel.py:328:            raise output\r\npytorch_lightning/accelerators/tpu_accelerator.py:66:            raise MisconfigurationException(\'PyTorch XLA not installed.\')\r\npytorch_lightning/accelerators/tpu_accelerator.py:182:            raise MisconfigurationException(\r\npytorch_lightning/accelerators/ddp_accelerator.py:116:            raise MisconfigurationException(\'you selected (distribute_backend = ddp) but did not set Trainer(gpus=?)\')\r\npytorch_lightning/accelerators/ddp_accelerator.py:185:            raise RuntimeError(\r\npytorch_lightning/accelerators/dp_accelerator.py:96:            raise MisconfigurationException(\r\npytorch_lightning/accelerators/horovod_accelerator.py:155:            raise ValueError(\r\npytorch_lightning/accelerators/horovod_accelerator.py:175:            raise ValueError(\r\npytorch_lightning/accelerators/horovod_accelerator.py:185:            raise ValueError(f"unrecognized `reduce_op`: {reduce_op}")\r\npytorch_lightning/accelerators/cpu_accelerator.py:42:            raise MisconfigurationException(\'amp + cpu is not supported.  Please use a GPU option\')\r\npytorch_lightning/accelerators/accelerator_connector.py:129:                raise MisconfigurationException(\r\npytorch_lightning/accelerators/accelerator_connector.py:289:            raise MisconfigurationException(\r\npytorch_lightning/accelerators/accelerator_connector.py:360:            raise MisconfigurationException(\r\npytorch_lightning/accelerators/accelerator_connector.py:387:            raise MisconfigurationException(\r\npytorch_lightning/accelerators/accelerator_connector.py:393:            raise MisconfigurationException(\r\npytorch_lightning/metrics/metric.py:129:            raise ValueError("state variable must be a tensor or any empty list (where you can append tensors)")\r\npytorch_lightning/metrics/metric.py:138:            raise ValueError("`dist_reduce_fx` must be callable or one of [\'mean\', \'sum\', \'cat\', None]")\r\npytorch_lightning/metrics/metric.py:287:                raise TypeError(\r\npytorch_lightning/metrics/metric.py:351:                    raise ValueError(f\'Value {metric} belonging to key {name}\'\r\npytorch_lightning/metrics/metric.py:357:                    raise ValueError(f\'Input {metric} to `MetricCollection` is not a instance\'\r\npytorch_lightning/metrics/metric.py:361:                    raise ValueError(f\'Encountered two metrics both named {name}\')\r\npytorch_lightning/metrics/metric.py:364:            raise ValueError(\'Unknown input to MetricCollection.\')\r\npytorch_lightning/metrics/regression/r2score.py:107:            raise ValueError(\'`adjusted` parameter should be an integer larger or\'\r\npytorch_lightning/metrics/regression/r2score.py:113:            raise ValueError(\r\npytorch_lightning/metrics/regression/explained_variance.py:93:            raise ValueError(\r\npytorch_lightning/metrics/functional/r2score.py:28:        raise ValueError(\'Expected both prediction and target to be 1D or 2D tensors,\'\r\npytorch_lightning/metrics/functional/r2score.py:31:        raise ValueError(\'Needs atleast two samples to calculate r2 score.\')\r\npytorch_lightning/metrics/functional/r2score.py:59:        raise ValueError(\'Argument `multioutput` must be either `raw_values`,\'\r\npytorch_lightning/metrics/functional/r2score.py:63:        raise ValueError(\'`adjusted` parameter should be an integer larger or\'\r\npytorch_lightning/metrics/functional/classification.py:162:        raise ValueError("reduction type %s not supported" % reduction)\r\npytorch_lightning/metrics/functional/classification.py:394:        raise ValueError("No negative samples in targets, false positive value should be meaningless")\r\npytorch_lightning/metrics/functional/classification.py:399:        raise ValueError("No positive samples in targets, true positive value should be meaningless")\r\npytorch_lightning/metrics/functional/classification.py:483:            raise ValueError(f"The \'x\' array is neither increasing or decreasing: {x}. Reorder is not supported.")\r\npytorch_lightning/metrics/functional/classification.py:547:        raise ValueError(\'AUROC metric is meant for binary classification, but\'\r\npytorch_lightning/metrics/functional/classification.py:555:        raise ValueError(f"`max_fpr` should be a float in range (0, 1], got: {max_fpr}")\r\npytorch_lightning/metrics/functional/classification.py:557:        raise RuntimeError(\'`max_fpr` argument requires `torch.bucketize` which\'\r\npytorch_lightning/metrics/functional/classification.py:609:        raise ValueError(\r\npytorch_lightning/metrics/functional/classification.py:614:        raise ValueError(\r\npytorch_lightning/metrics/functional/classification.py:621:        raise ValueError(\r\npytorch_lightning/metrics/functional/image_gradients.py:23:        raise TypeError(f"The `img` expects a value of <torch.Tensor> type but got {type(img)}")\r\npytorch_lightning/metrics/functional/image_gradients.py:25:        raise RuntimeError(f"The `img` expects a 4D tensor but got {img.ndim}D tensor")\r\npytorch_lightning/metrics/functional/stat_scores.py:90:        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {preds.shape[0]} classes")\r\npytorch_lightning/metrics/functional/stat_scores.py:93:        raise ValueError("You can not use `ignore_index` with binary data.")\r\npytorch_lightning/metrics/functional/stat_scores.py:97:            raise ValueError(\r\npytorch_lightning/metrics/functional/stat_scores.py:259:        raise ValueError(f"The `reduce` {reduce} is not valid.")\r\npytorch_lightning/metrics/functional/stat_scores.py:262:        raise ValueError(f"The `mdmc_reduce` {mdmc_reduce} is not valid.")\r\npytorch_lightning/metrics/functional/stat_scores.py:265:        raise ValueError("When you set `reduce` as \'macro\', you have to provide the number of classes.")\r\npytorch_lightning/metrics/functional/stat_scores.py:268:        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")\r\npytorch_lightning/metrics/functional/accuracy.py:27:        raise ValueError("You can not use the `top_k` parameter to calculate accuracy for multi-label inputs.")\r\npytorch_lightning/metrics/functional/roc.py:56:            raise ValueError("No negative samples in targets, false positive value should be meaningless")\r\npytorch_lightning/metrics/functional/roc.py:60:            raise ValueError("No positive samples in targets, true positive value should be meaningless")\r\npytorch_lightning/metrics/functional/ssim.py:42:        raise TypeError(\r\npytorch_lightning/metrics/functional/ssim.py:48:        raise ValueError(\r\npytorch_lightning/metrics/functional/ssim.py:66:        raise ValueError(\r\npytorch_lightning/metrics/functional/ssim.py:72:        raise ValueError(f"Expected `kernel_size` to have odd positive number. Got {kernel_size}.")\r\npytorch_lightning/metrics/functional/ssim.py:75:        raise ValueError(f"Expected `sigma` to have positive number. Got {sigma}.")\r\npytorch_lightning/metrics/functional/precision_recall_curve.py:72:        raise ValueError(\r\npytorch_lightning/metrics/functional/precision_recall_curve.py:78:            raise ValueError(\'Preds and target have equal shape, but number of classes is different from 1\')\r\npytorch_lightning/metrics/functional/precision_recall_curve.py:92:            raise ValueError(f\'Argument `num_classes` was set to {num_classes} in\'\r\npytorch_lightning/metrics/classification/hamming_distance.py:85:            raise ValueError("The `threshold` should lie in the (0,1) interval.")\r\npytorch_lightning/metrics/classification/f_beta.py:104:            raise ValueError(\'Argument `average` expected to be one of the following:\'\r\npytorch_lightning/metrics/classification/helpers.py:28:        raise ValueError("The `target` has to be an integer tensor.")\r\npytorch_lightning/metrics/classification/helpers.py:30:        raise ValueError("The `target` has to be a non-negative tensor.")\r\npytorch_lightning/metrics/classification/helpers.py:34:        raise ValueError("If `preds` are integers, they have to be non-negative.")\r\npytorch_lightning/metrics/classification/helpers.py:37:        raise ValueError("The `preds` and `target` should have the same first dimension.")\r\npytorch_lightning/metrics/classification/helpers.py:40:        raise ValueError("The `preds` should be probabilities, but values were detected outside of [0,1] range.")\r\npytorch_lightning/metrics/classification/helpers.py:43:        raise ValueError(f"The `threshold` should be a float in the (0,1) interval, got {threshold}")\r\npytorch_lightning/metrics/classification/helpers.py:46:        raise ValueError("If you set `is_multiclass=False`, then `target` should not exceed 1.")\r\npytorch_lightning/metrics/classification/helpers.py:49:        raise ValueError("If you set `is_multiclass=False` and `preds` are integers, then `preds` should not exceed 1.")\r\npytorch_lightning/metrics/classification/helpers.py:69:            raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:74:            raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:92:            raise ValueError("If `preds` have one dimension more than `target`, `preds` should be a float tensor.")\r\npytorch_lightning/metrics/classification/helpers.py:94:            raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:106:        raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:121:        raise ValueError("Your data is binary, but `num_classes` is larger than 2.")\r\npytorch_lightning/metrics/classification/helpers.py:123:        raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:128:        raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:144:        raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:152:                raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:160:            raise ValueError("The highest label in `target` should be smaller than `num_classes`.")\r\npytorch_lightning/metrics/classification/helpers.py:162:            raise ValueError("The highest label in `preds` should be smaller than `num_classes`.")\r\npytorch_lightning/metrics/classification/helpers.py:164:            raise ValueError("The size of C dimension of `preds` does not match `num_classes`.")\r\npytorch_lightning/metrics/classification/helpers.py:174:        raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:180:        raise ValueError("The implied number of classes (from shape of inputs) does not match num_classes.")\r\npytorch_lightning/metrics/classification/helpers.py:185:        raise ValueError("You can not use `top_k` parameter with binary data.")\r\npytorch_lightning/metrics/classification/helpers.py:187:        raise ValueError("The `top_k` has to be an integer larger than 0.")\r\npytorch_lightning/metrics/classification/helpers.py:189:        raise ValueError("You have set `top_k`, but you do not have probability predictions.")\r\npytorch_lightning/metrics/classification/helpers.py:191:        raise ValueError("If you set `is_multiclass=False`, you can not set `top_k`.")\r\npytorch_lightning/metrics/classification/helpers.py:193:        raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:198:        raise ValueError("The `top_k` has to be strictly smaller than the `C` dimension of `preds`.")\r\npytorch_lightning/metrics/classification/helpers.py:270:            raise ValueError("Probabilities in `preds` must sum up to 1 accross the `C` dimension.")\r\npytorch_lightning/metrics/classification/helpers.py:275:            raise ValueError(\r\npytorch_lightning/metrics/classification/helpers.py:280:            raise ValueError(\r\npytorch_lightning/metrics/classification/stat_scores.py:149:            raise ValueError(f"The `threshold` should be a float in the (0,1) interval, got {threshold}")\r\npytorch_lightning/metrics/classification/stat_scores.py:152:            raise ValueError(f"The `reduce` {reduce} is not valid.")\r\npytorch_lightning/metrics/classification/stat_scores.py:155:            raise ValueError(f"The `mdmc_reduce` {mdmc_reduce} is not valid.")\r\npytorch_lightning/metrics/classification/stat_scores.py:158:            raise ValueError("When you set `reduce` as \'macro\', you have to provide the number of classes.")\r\npytorch_lightning/metrics/classification/stat_scores.py:161:            raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")\r\npytorch_lightning/metrics/classification/accuracy.py:119:            raise ValueError(f"The `threshold` should be a float in the (0,1) interval, got {threshold}")\r\npytorch_lightning/metrics/classification/accuracy.py:122:            raise ValueError(f"The `top_k` should be an integer larger than 0, got {top_k}")\r\npytorch_lightning/metrics/utils.py:41:    """ Check that predictions and target have the same shape, else raise error """\r\npytorch_lightning/metrics/utils.py:43:        raise RuntimeError("Predictions and targets are expected to have the same shape")\r\npytorch_lightning/metrics/utils.py:64:        raise ValueError("preds and target must have same number of dimensions, or one additional dimension for preds")\r\npytorch_lightning/metrics/utils.py:221:    raise ValueError("Reduction parameter unknown.")\r\npytorch_lightning/metrics/utils.py:264:    raise ValueError(\r\n```\r\n\r\n</details>'], ["Most functions in `scipy.linalg` functions (e.g. `svd`, `qr`, `eig`, `eigh`, `pinv`, `pinv2` ...) have a default kwarg `check_finite=True` that we  typically leave to the default value in scikit-learn.\r\n\r\nAs we already validate the input data for most estimators in scikit-learn, this check is redundant and can cause significant overhead, especially at predict / transform time. We should probably always call those method with an explicit `check_finite=False` in scikit-learn.\r\n\r\nThis issue shall probably be addressed in many PRs, probably one per module that imports  `scipy.linalg`.\r\n\r\nWe should still make sure that the estimators raise a `ValueError` with the expected error message when fed with numpy arrays with infinite some values (`-np.inf`, `np.inf` or `np.nan`). This can be done manually by calling `sklearn.utils.estimator_checks.check_estimators_nan_inf` on the estimator, which should be automatically be called by `sklearn.tests.test_common` but we need to check that it's actually the case when reviewing such PRs."], ['```\r\nIn [12]: pd.set_option(\'mode.use_inf_as_na\',False)\r\n\r\nIn [13]: df\r\nOut[13]:\r\n                            B\r\n2018-09-25 14:47:31.500   0.0\r\n2018-04-03 22:55:13.000   1.0\r\n2018-09-04 09:10:07.500   2.0\r\n2018-11-30 10:32:08.000   NaN\r\n2018-03-27 14:21:12.500   4.0\r\n2018-09-28 14:30:36.500   6.0\r\n2018-07-16 21:56:48.000   7.0\r\n2018-12-05 23:53:59.500   8.0\r\n2018-09-04 09:46:58.500   9.0\r\n2018-11-09 09:14:08.500  10.0\r\n\r\nIn [14]: df.sort_index()\r\nOut[14]:\r\n                            B\r\n2018-03-27 14:21:12.500   4.0\r\n2018-04-03 22:55:13.000   1.0\r\n2018-07-16 21:56:48.000   7.0\r\n2018-09-04 09:10:07.500   2.0\r\n2018-09-04 09:46:58.500   9.0\r\n2018-09-25 14:47:31.500   0.0\r\n2018-09-28 14:30:36.500   6.0\r\n2018-11-09 09:14:08.500  10.0\r\n2018-11-30 10:32:08.000   NaN\r\n2018-12-05 23:53:59.500   8.0\r\n\r\nIn [15]: pd.set_option(\'mode.use_inf_as_na\',True)\r\n\r\nIn [16]: df.sort_index()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-16-02a473b4132d> in <module>\r\n----> 1 df.sort_index()\r\n\r\ng:\\py372\\lib\\site-packages\\pandas\\core\\frame.py in sort_index(self, axis, lev\r\n ascending, inplace, kind, na_position, sort_remaining, by)\r\n   5088\r\n   5089             indexer = nargsort(\r\n-> 5090                 labels, kind=kind, ascending=ascending, na_position=n\r\nosition\r\n   5091             )\r\n   5092\r\n\r\ng:\\py372\\lib\\site-packages\\pandas\\core\\sorting.py in nargsort(items, kind, as\r\nding, na_position)\r\n    244\r\n    245     items = extract_array(items)\r\n--> 246     mask = np.asarray(isna(items))\r\n    247\r\n    248     if is_extension_array_dtype(items):\r\n\r\ng:\\py372\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py in isna(obj)\r\n    120     Name: 1, dtype: bool\r\n    121     """\r\n--> 122     return _isna(obj)\r\n    123\r\n    124\r\n\r\ng:\\py372\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py in _isna_old(obj)\r\n    177         return _isna_ndarraylike_old(obj)\r\n    178     elif isinstance(obj, ABCGeneric):\r\n--> 179         return obj._constructor(obj._data.isna(func=_isna_old))\r\n    180     elif isinstance(obj, list):\r\n    181         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\r\n\r\nAttributeError: \'DatetimeArray\' object has no attribute \'_constructor\'\r\n\r\n```'], ["I have a question about `jax.jvp`. From my understanding, given `f: R^n -> R^m`, `jax.jvp(f, x, v)` will compute the pushforward at `x` evaluated on tangent vector `v`, i.e. `df(x)[v]`. In this case we can think about this as a matrix multiplicatoin where `df(x)` as a `m x n` matrix and `v` to be the same shape as `x`.\r\n\r\nHowever, it seems that `jax.jvp` allow `v` to be of a different shape than `x` which confuses me:\r\n```python\r\nimport jax\r\ndef f(x):\r\n  # Note that f: R^n -> R^(n-1)\r\n  return x[:-1]\r\n\r\nn = 10\r\nkey = jax.random.PRNGKey(0)\r\nx = jax.random.normal(key, (n,))\r\nv = jax.random.normal(key, (2*n,))\r\njax.jvp(f, (x,), (v,))\r\n```\r\nwill run properly and return\r\n```python\r\n(DeviceArray([-0.372111  ,  0.26423106, -0.18252774, -0.7368198 ,\r\n              -0.44030386, -0.15214427, -0.6713536 , -0.5908642 ,\r\n               0.73168874], dtype=float32),\r\n DeviceArray([ 1.054516  , -0.9692889 , -0.59460217, -0.0318858 ,\r\n               2.410932  , -1.8784496 , -0.7847697 , -0.3137085 ,\r\n               0.33370885], dtype=float32))\r\n```\r\ndespite the fact that `v` is not the same shape as `x`. Shouldn't this fail since `x` and `v` are not the same shape?\r\n"], ["<!--\r\n  Hi there! Thank you for discovering and submitting an issue.\r\n\r\n  Before you submit this; let's make sure of a few things.\r\n  Please make sure the following boxes are ticked if they are correct.\r\n  If not, please try and fulfill these first.\r\n-->\r\n\r\n<!-- Checked checkbox should look like this: [x] -->\r\n- [x] I am on the [latest](https://github.com/python-poetry/poetry/releases/latest) Poetry version.\r\n- [x] I have searched the [issues](https://github.com/python-poetry/poetry/issues) of this repo and believe that this is not a duplicate.\r\n- [x] If an exception occurs when executing a command, I executed it again in debug mode (`-vvv` option).\r\n\r\n<!--\r\n  Once those are done, if you're able to fill in the following list with your information,\r\n  it'd be very helpful to whoever handles the issue.\r\n-->\r\n\r\n- **OS version and name**: MacOS 10.15\r\n- **Poetry version**: 1.1.2\r\n- **pyproject.toml** / **setup.py**: https://gist.github.com/Fizzadar/a1b98304a2b86ee966dedea6479320bd\r\n\r\n## Issue\r\n<!-- Now feel free to write your issue, but please be descriptive! Thanks again 🙌 ❤️ -->\r\n\r\nThe `extras_require` in the generated `setup.py` ignores the version specified in the `pyproject.toml`, leading to inconsistent/broken builds when installing a package with extras.\r\n\r\nGist above contains full info, but specifically:\r\n\r\n```toml\r\n[tool.poetry.dependencies]\r\nkombu = {version = '^=4.5', optional = true}\r\n\r\n[tool.poetry.extras]\r\nqueue = ['kombu']\r\n```\r\n\r\nBecomes:\r\n\r\n```py\r\nextras_require = \\\r\n{'queue': ['kombu']}\r\n```"], ["<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nWhen I have a cluster already running, I sometimes want to re-run the setup commands even if there's no changes, without having to shut down the cluster. For example if I am installing a package via a Github repo, I want to trigger the install again after pushing new changes.\r\n\r\nHowever, calling `ray up` doesn't actually re-run the setup commands if it doesn't detect any changes in the commands. I am getting around this for now by adding a `echo` command and modifying it every time I want to re-trigger the setup_commands. But it would be nice to have a flag to do this via `ray up`.\r\n"], ['### Versions\r\n\r\n<details>\r\n  <summary>pd.show_versions()</summary>\r\n  \r\n  ```\r\n   INSTALLED VERSIONS\r\n  ------------------\r\n  commit           : f2c8480af2f25efdbd803218b9d87980f416563e\r\n  python           : 3.8.7.final.0\r\n  python-bits      : 64\r\n  OS               : Windows\r\n  OS-release       : 10\r\n  Version          : 10.0.18362\r\n  machine          : AMD64\r\n  processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\r\n  byteorder        : little\r\n  LC_ALL           : None\r\n  LANG             : None\r\n  LOCALE           : English\r\n  pandas           : 1.2.3\r\n  numpy            : 1.19.5\r\n  pytz             : 2021.1\r\n  dateutil         : 2.8.1\r\n  pip              : 21.0.1\r\n  setuptools       : 49.2.1\r\n  Cython           : None\r\n  pytest           : 6.2.2\r\n  hypothesis       : None\r\n  sphinx           : None\r\n  blosc            : None\r\n  feather          : None\r\n  xlsxwriter       : None\r\n  lxml.etree       : None\r\n  html5lib         : None\r\n  pymysql          : None\r\n  psycopg2         : None\r\n  jinja2           : 2.11.3\r\n  IPython          : 7.21.0\r\n  pandas_datareader: None\r\n  bs4              : None\r\n  bottleneck       : None\r\n  fsspec           : None\r\n  fastparquet      : None\r\n  gcsfs            : None\r\n  matplotlib       : None\r\n  numexpr          : None\r\n  odfpy            : None\r\n  openpyxl         : None\r\n  pandas_gbq       : None\r\n  pyarrow          : 3.0.0\r\n  pyxlsb           : None\r\n  s3fs             : None\r\n  scipy            : 1.6.1\r\n  sqlalchemy       : None\r\n  tables           : None\r\n  tabulate         : None\r\n  xarray           : None\r\n  xlrd             : None\r\n  xlwt             : None\r\n  numba            : None\r\n\r\n  ```\r\n</details>\r\n\r\n### Description\r\nWhen doing a groupby and then a fillna on a column of dtype StringDType, pandas throws a `ValueError: StringArray requires a sequence of strings or pandas.NA`. This does not seem to happen with other (extension) dtypes afaik. \r\nThe error only occurs if there are still NAs left after the `fillna`. It does not occur without the `groupby`.\r\n\r\n### Example\r\n```python3\r\n>> pd.DataFrame({"a": pd.array([None, "a"], dtype="string"), "b": [0, 0]}).groupby("b").ffill()\r\n\r\nTraceback (most recent call last):\r\n  File "C:\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py", line 3437, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File "<ipython-input-22-a665c967b563>", line 1, in <module>\r\n    pd.DataFrame({"a": pd.array([None, "a"], dtype="string"), "b": [0, 0]}).groupby("b").ffill()\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py", line 1953, in pad\r\n    return self._fill("ffill", limit=limit)\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py", line 1919, in _fill\r\n    return self._get_cythonized_result(\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py", line 2673, in _get_cythonized_result\r\n    result = algorithms.take_nd(values, result)\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\algorithms.py", line 1699, in take_nd\r\n    return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py", line 78, in take\r\n    return self._from_backing_data(new_data)\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py", line 190, in _from_backing_data\r\n    return type(self)(arr)\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\arrays\\string_.py", line 195, in __init__\r\n    self._validate()\r\n  File "C:\\venv\\lib\\site-packages\\pandas\\core\\arrays\\string_.py", line 200, in _validate\r\n    raise ValueError("StringArray requires a sequence of strings or pandas.NA")\r\nValueError: StringArray requires a sequence of strings or pandas.NA\r\n\r\n>> pd.DataFrame({"a": pd.array(["a", None], dtype="string"), "b": [0, 0]}).groupby("b").ffill()\r\nOut[31]: \r\n   a\r\n0  a\r\n1  a\r\n```\r\n\r\nEdit: Updated pandas, still an issue in 1.2.3'], ["# 🚀 Feature request\r\n\r\nIn `pipelines` tests and documentation they are recurringly named `nlp`, the goal is to rename them to something more appropriate.\r\n\r\n<!-- A clear and concise description of the feature proposal.\r\n     Please provide a link to the paper and code in case they exist. -->\r\n\r\n## Motivation\r\n\r\n\r\n```python\r\nnlp = pipeline(task='conversational', model='XXX')\r\n```\r\nThis is a bit pretentious as it does cover all NLP, and a better name help understanding to users too.\r\nFor instance the `conversational` task pipeline could be named `conversational_agent`. Or maybe still a generic name but less\r\npretentious `pipe`, `pipeline` (caveat: those are less clear in what they intend to achieve)\r\n\r\nThe goal is to rename all occurences of `nlp` by better names within both tests and the documentation.\r\n\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request\r\n     related to a problem? e.g., I'm always frustrated when [...]. If this is related\r\n     to another GitHub issue, please link here too. -->\r\n\r\n## Your contribution\r\n\r\nThe better names could be discussed here.\r\n\r\n<!-- Is there any way that you could help, e.g. by submitting a PR?\r\n     Make sure to read the CONTRIBUTING.MD readme:\r\n     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->\r\n"], ['I have trained a EncoderDecoderModel from huggging face to do english-German translation task. I tried to overfit a small dataset (100 parallel sentences), and use `model.generate()` then `tokenizer.decode()` to perform the translation. However, the output seems to be proper German sentences, but it is definitely not the correct translation.\r\n\r\nHere are the code for building the model\r\n\r\n```\r\nencoder_config = BertConfig()\r\ndecoder_config = BertConfig()\r\nconfig = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\r\nmodel = EncoderDecoderModel(config=config)\r\n```\r\n\r\nHere are the code for testing the model\r\n\r\n```\r\nmodel.eval()\r\ninput_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0)\r\noutput_ids = model.generate(input_ids.to(\'cuda\'), decoder_start_token_id=model.config.decoder.pad_token_id)\r\noutput_text = tokenizer.decode(output_ids[0])\r\n```\r\n\r\nExample input: "iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould ."\r\n\r\nGround truth translation: "iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird ."\r\n\r\nWhat the model outputs after trained 100 epochs: "[S] wenn sie den unten stehenden link anklicken, sehen sie ein video uber die erstellung ansprechender illustrationen in quarkxpress" which is totally nonesense.\r\n\r\nWhere is the problem?'], ['\r\n### Description\r\n\r\nUsage of [`scrapy.utils.response.response_httprepr` ](https://github.com/scrapy/scrapy/blob/2.4.1/scrapy/utils/response.py#L45)inside[ `DownloaderStats`](https://github.com/scrapy/scrapy/blob/2.4.1/scrapy/downloadermiddlewares/stats.py) middleware causing application to make unnecessary memory allocation.\r\n`response_httprepr` used only one time - to calculate response sizes for `downloader/response_bytes` stats in that middleware.\r\n\r\nIn current implementation `response_httprepr` return `bytes` (immutable type) - in order to calculate `downloader/response_bytes` application will additionally allocate nearly the same memory amount as for original response (only for calculating `len` inside middleware).\r\nhttps://github.com/scrapy/scrapy/blob/26836c4e1ae9588ee173c5977fc6611364ca7cc7/scrapy/utils/response.py#L45-L60\r\n\r\n\r\n### Steps to Reproduce\r\nIn order to demonstrate influence of this i made this spider:\r\n<details>\r\n  <summary>spider code</summary>\r\n\r\n```python\r\nimport sys\r\nfrom importlib import import_module\r\nimport scrapy\r\n\r\n\r\nclass MemoryHttpreprSpider(scrapy.Spider):\r\n    name = \'memory_httprepr\'\r\n    custom_settings = {\r\n        \'DOWNLOADER_MIDDLEWARES\':{\r\n            \'scrapy.downloadermiddlewares.stats.DownloaderStats\': None\r\n        }\r\n    }\r\n# the same as in MemoryUsage extension:\r\n    def get_virtual_size(self):\r\n        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\r\n        if sys.platform != \'darwin\':\r\n            # on macOS ru_maxrss is in bytes, on Linux it is in KB\r\n            size *= 1024\r\n        return size\r\n\r\n    def start_requests(self):\r\n        try:\r\n            self.resource = import_module(\'resource\')\r\n        except ImportError:\r\n            pass\r\n        self.logger.info(f"used memory on start: {str(self.get_virtual_size())}")\r\n        yield scrapy.Request(url=\'https://speed.hetzner.de/100MB.bin\', callback=self.parse)\r\n        #yield scrapy.Request(url=\'http://quotes.toscrape.com\', callback=self.parse)\r\n\r\n    def parse(self, response, **kwargs):\r\n        self.logger.info(f"used memory after downloading response: {str(self.get_virtual_size())}")\r\n```\r\n\r\n\r\n</details>\r\nIt include:\r\n\r\n- usage of `get_virtual_size` method - directly the same as on [`MemoryUsage`](https://github.com/scrapy/scrapy/blob/2.4.1/scrapy/extensions/memusage.py) extension\r\n- url with 100Mb binary response https://speed.hetzner.de/100MB.bin\r\n- disabled `DownloaderStats` middleware that uses `response_httprepr` and `request_httprepr`\r\n- usual html url http://quotes.toscrape.com (for comparison)\r\n\r\n| log output | 100Mb default settings | 100Mb no downloader stats | quotes default |\r\n| --- | --- | --- | --- |\r\n| `[memory_httprepr] used memory on start:` | `61 587 456` | `61 521 920` | `61 558 784` |\r\n| `[memory_httprepr] used memory after downloading response:` | `375 910 400` | `271 179 776` | `61 558 784` |\r\n\r\n### Versions\r\n\r\n`[scrapy.utils.log] Scrapy 2.4.0 started (bot: httprepr)`\r\n`[scrapy.utils.log] Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.2 (default, Apr 23 2020, 14:32:57) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h 22 Sep 2020), cryptography 3.1.1, Platform Linux-4.15.0-76-generic-x86_64-with-glibc2.2.5`\r\n'], ["**Required skills**: Python\r\n\r\n**Difficulty**: Medium\r\n\r\nAt the moment, the converter has basic support for the Age of Empires 1: Definitive Edition (DE1) release. The modpack generator reuses the converter for the original 1998 release (AoE1 + Rise of Rome). [New features](https://ageofempires.fandom.com/wiki/Age_of_Empires:_Definitive_Edition#New_features) from DE1, e.g. walkable farms, still need to be supported. A lot of the new features were taken over from AoC, so you can probably reuse functionality from its conversion processor.\r\n\r\nAdditionally, not all of DE1's media formats (terrain textures, UI, particle effects, etc.) are exported yet. Adding those to the converter would be optional for now. The data conversion is muc more important.\r\n\r\n**Further reading**:\r\n\r\n* [DE1 converter code](https://github.com/SFTtech/openage/blob/master/openage/convert/processor/conversion/de1/processor.py)\r\n* [RoR converter code](https://github.com/SFTtech/openage/blob/master/openage/convert/processor/conversion/ror/processor.py)\r\n* [Converter architecture](https://github.com/SFTtech/openage/blob/master/doc/code/converter/architecture_overview.md)\r\n* [Converter workflow](https://github.com/SFTtech/openage/blob/master/doc/code/converter/workflow.md)\r\n"], ['4.  **Feature Request:** then please do a quick search of existing issues to\r\n    make sure that this has not been asked before.\r\nThis is a feature request. I have been convert my pandas code for dask and couldnt find a substitution for numpy.select(). My current workaround is to use .apply() with a custom function, but it would be nice to have native dask.select() method.\r\n\r\n        cond = [(self._sell_shift == 1), (self._buy_shift == 1)]\r\n        out = ["Sell", "Buy"]\r\n        df = np.select(cond, out, default=0) # now 2 arrays become 1\r\n\r\n\r\nThank you.\r\n'], ['https://github.com/pandas-dev/pandas/issues/34226#issuecomment-690887774 is the example, should just add to window.rst\r\n\r\nalso would be good to show using https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.api.indexers.FixedForwardWindowIndexer.html?highlight=forward#pandas.api.indexers.FixedForwardWindowIndexer'], ['### ALL software version info (bokeh, python, notebook, OS, browser, any other relevant packages)\r\nBokeh 2.3.0dev13 (Python 3.8.6, Win10)\r\n\r\n### Description of expected behavior and the observed behavior\r\nExpected: Defining and using colors should be consistent across all different ways to define colors (as much as possible)\r\nObserved: Some ways of defining colors work different than others\r\n\r\n### Complete, minimal, self-contained example code that reproduces the issue\r\n\r\n**Example 1: defining single colors with CSS color strings**\r\n\r\nA single `rgb()` color string works:\r\n```\r\nfrom bokeh.plotting import figure, show\r\n\r\nx = [1,2,3]\r\ny = [1,2,3]\r\ncol = "rgb(200 0 0)"\r\n\r\np = figure(title = "Using single rgb() color value works")\r\np.circle(x, y, radius=.25, color=col)\r\nshow(p)\r\n```\r\nHowever, a single `hsl()` color string does not work:\r\n```\r\nfrom bokeh.plotting import figure, show\r\n\r\nx = [1,2,3]\r\ny = [1,2,3]\r\ncol = "hsl(60deg 100% 50% / 1.0)"\r\n\r\np = figure(title = "Using single hsl() color value fails")\r\np.circle(x, y, radius=.25, color=col)\r\nshow(p)\r\n```\r\nhsl() color strings as items in a list / array do work, therefore I guess that adding a hsl regex to `__init__()` in Bokeh.core.properties.Color might resolve this problem.\r\n\r\n**Example 2: defining single colors with a 32-bit unsigned integer**\r\n\r\nSimilarly, using a single 32-bit unsigned integer does not work:\r\n```\r\nfrom bokeh.plotting import figure, show\r\n\r\nx = [1,2,3]\r\ny = [1,2,3]\r\ncol = 0xff0000ff\r\n\r\np = figure(title = "Using single 32-bit int color fails")\r\np.circle(x, y, radius=.25, color=col)\r\nshow(p)\r\n```\r\n32-bit unsigned integers as items in a list do work, so the rexpective data type might also have to be added to `__init__()` in Bokeh.core.properties.Color \r\n\r\nEDIT: I used to have a third example here, but as @mattpap correctly pointed out, this behaviour is consistent with CSS4 syntax.'], ['Can be reproduced at https://chat.zulip.org/#organization/organization-profile.\r\n\r\nI think this is a regression ... I remember the button being disabled before.'], ['Pipenv version: 2018.7.1\r\nThe first error is OK (the `PermissionError`), but then there\'s a lot more.\r\n\r\n```\r\n$ WORKON_HOME=/opt/ pipenv sync\r\nCreating a virtualenv for this project...\r\nPipfile: /home/me/project/Pipfile\r\nUsing /usr/bin/python3.6m (3.6.5) to create virtualenv...\r\n⠋Running virtualenv with interpreter /usr/bin/python3.6m\r\nUsing base prefix \'/usr\'\r\nTraceback (most recent call last):\r\n  File "/home/me/.local/lib/python3.6/site-packages/virtualenv.py", line 2343, in <module>\r\n    main()\r\n  File "/home/me/.local/lib/python3.6/site-packages/virtualenv.py", line 712, in main\r\n    symlink=options.symlink)\r\n  File "/home/me/.local/lib/python3.6/site-packages/virtualenv.py", line 927, in create_environment\r\n    site_packages=site_packages, clear=clear, symlink=symlink))\r\n  File "/home/me/.local/lib/python3.6/site-packages/virtualenv.py", line 1112, in install_python\r\n    mkdir(lib_dir)\r\n  File "/home/me/.local/lib/python3.6/site-packages/virtualenv.py", line 324, in mkdir\r\n    os.makedirs(path)\r\n  File "/usr/lib/python3.6/os.py", line 210, in makedirs\r\n    makedirs(head, mode, exist_ok)\r\n  File "/usr/lib/python3.6/os.py", line 210, in makedirs\r\n    makedirs(head, mode, exist_ok)\r\n  File "/usr/lib/python3.6/os.py", line 220, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: \'/opt/clump-D6KMH_ut\'\r\nError while trying to remove the /opt/clump-D6KMH_ut env:\r\nNo such file or directory\r\n\r\nVirtualenv location:                                                                                                                            Traceback (most recent call last):\r\n  File "/home/me/.local/bin/pipenv", line 11, in <module>\r\n    sys.exit(cli())\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/vendor/click/core.py", line 722, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/vendor/click/core.py", line 697, in main\r\n    rv = self.invoke(ctx)\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/vendor/click/core.py", line 1066, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/vendor/click/core.py", line 895, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/vendor/click/core.py", line 535, in invoke\r\n    return callback(*args, **kwargs)\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/vendor/click/decorators.py", line 17, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/cli.py", line 1049, in sync\r\n    pypi_mirror=pypi_mirror,\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/core.py", line 2504, in do_sync\r\n    ensure_project(three=three, python=python, validate=False, deploy=deploy, pypi_mirror=pypi_mirror)\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/core.py", line 614, in ensure_project\r\n    path_to_python = which(\'python\') or which(\'py\')\r\n  File "/home/me/.local/lib/python3.6/site-packages/pipenv/core.py", line 115, in which\r\n    p = os.path.join(location, \'bin\', command)\r\n  File "/usr/lib/python3.6/posixpath.py", line 80, in join\r\n    a = os.fspath(a)\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n'], ['#### Versions\r\nbokeh 2.2.3 (via conda 4.8.3); Mac OS X 10.15.7 \r\n\r\n#### Description of expected behavior and the observed behavior\r\nScatter points jittered on a categorical axis don\'t move when the categorical factor range (provided as `range` parameter) is modified. In a [Discourse thread](https://discourse.bokeh.org/t/6848), Bryan put this down to a "deficiency in the current caching behavior of the jitter transform".\r\n\r\nBelow is a brief example where a button reverses a categorical axis: red jittered points move, blue jittered ones don’t.\r\n\r\n#### Example\r\n\r\n```python\r\nfrom bokeh.transform import jitter\r\nfrom bokeh.models import ColumnDataSource, FactorRange, CustomJS\r\nfrom bokeh.models.widgets import Button\r\nfrom bokeh.layouts import column\r\nfrom bokeh.plotting import figure\r\nfrom bokeh.io import show\r\n\r\nsource = ColumnDataSource({\r\n    \'name\': [\'a\', \'a\', \'a\', \'a\', \'a\'],\r\n    \'val\': [1, 1, 1, 1, 1],\r\n})\r\n\r\np = figure(x_range=FactorRange(factors=[\'a\', \'b\']),  \r\n           y_range=(0, 2), plot_width=200, plot_height=200)\r\n\r\np.scatter(x=jitter(\'name\', 0.5, range=p.x_range), y=\'val\', size=5, alpha=0.5, color=\'blue\', source=source)\r\np.scatter(x=\'name\', y=jitter(\'val\', 0.5), size=5, color=\'red\', alpha=0.5, source=source)\r\n\r\nbutton = Button(label=\'Reorder factors\')\r\nbutton.js_on_click(CustomJS(args={\'x_range\': p.x_range}, code="""\r\n    const factors = [...x_range.factors];\r\n    const reversed = factors.reverse();\r\n    x_range.factors = reversed;\r\n"""))\r\ncolumn = column(button, p)\r\nshow(column)\r\n```\r\n\r\n#### Screenshots or screencasts of the bug in action\r\nBefore click (original x axis):\r\n<img width="215" alt="before_click" src="https://user-images.githubusercontent.com/5185412/101223477-0c026a80-3684-11eb-9fcf-bd2c890c7243.png">\r\nAfter click (x axis factors reversed):\r\n<img width="213" alt="after_click" src="https://user-images.githubusercontent.com/5185412/101223500-145aa580-3684-11eb-90c9-387d551c04bc.png">\r\n'], ['Currently, https://docs.saltproject.io/en/latest/topics/development/pull_requests.html mentions the step of installing a pre-commit hook, but does not talk at all about the pyupgrade solution that is implemented.\r\n\r\nThis is potentially confusing for community contributors. Pyupgrade will add py2->3 code to any file they touch. The contributors will see a bunch of changes to salt code that they did not actually touch, as part of their commit.'], ["**Apache Airflow version**: 2.0.1\r\n\r\n**What happened**:\r\n\r\n`OdbcHook` returns `None` for non-boolean-like string values in `connect_kwargs` dict arg\r\n\r\n\r\n**What you expected to happen**:\r\n\r\n`connect_kwarg` values should remain as is.\r\n\r\n\r\n**How to reproduce it**:\r\n\r\n```python\r\n>>> from airflow.providers.odbc.hooks.odbc import OdbcHook\r\n>>> OdbcHook('my_conn', connect_kwargs={'CurrentSchema': 'SCHEMA'}).connect_kwargs\r\n{'CurrentSchema': None}\r\n```\r\n\r\n**Anything else we need to know**:\r\n\r\nThe issue lies in: https://github.com/apache/airflow/blob/db9febdb3be97832679d2ced8028fd7f1c21cd4e/airflow/providers/odbc/hooks/odbc.py#L170-L173\r\n\r\nThere's no `else` block that returns `val`. As it is, any string value will instead return `None`.\r\n\r\nPardon my ignorance on the subject, but this raises a question: Why is `clean_bool` being called in the first place for a user-provided dictionary? I'm not sure how this is necessary because the user can provide a literal boolean value in the dictionary if needed, no? If in the event that a driver needs to take a case-sensitive boolean string for some parameter, then `clean_bool` would make it impossible to provide such a value."], ["Versioneer version looks like the following (from Modin master):\r\n\r\n```\r\nIn [1]: import modin\r\n   ...: modin.__version__\r\nOut[1]: '0.7.2+15.ga461df3\r\n```\r\n\r\nWith uncommited updates, version string looks like this:\r\n\r\n```\r\nIn [1]: import modin\r\n   ...: modin.__version__\r\nOut[1]: '0.7.2+15.ga461df3.dirty'\r\n```\r\n\r\nThe format of the string is:\r\n```\r\n<most recent release>+<number of commits since release>.<commit hash>\r\n```\r\n\r\nThis should help debugging from master and installation from nightly wheels will be more informative for which commit is being used by the user/issue reporter.\r\n\r\nHere's the commit that added this to Modin: https://github.com/modin-project/modin/commit/840fd813a6d893e32607b5adc33dfeb0f756924a\r\n\r\ncc @simon-mo "], ['<!--To help us understand and resolve your issue, please fill out the form to the best of your ability.-->\r\n<!--You can feel free to delete the sections that do not apply.-->\r\n\r\n### Bug report\r\n\r\n**Bug summary**\r\n\r\nGenerating `np.random.randn(1000)` values, visualizing them with `plt.hist()`. Works fine with Numpy.\r\n\r\nWhen I replace Numpy with tensorflow.experimental.numpy, Matplotlib 3.3.4 fails to display the histogram correctly. Matplotlib 3.2.2 works fine.\r\n\r\n**Code for reproduction**\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.experimental.numpy as tnp\r\n\r\n# bad image\r\nlabels1 = 15 + 2 * tnp.random.randn(1000)\r\n_ = plt.hist(labels1)\r\n\r\n# good image\r\nlabels2 = 15 + 2 * np.random.randn(1000)\r\n_ = plt.hist(labels2)\r\n```\r\n\r\n**Actual outcome**\r\n\r\n![np-bad](https://user-images.githubusercontent.com/901867/109069736-7b499280-76a6-11eb-87c4-2880aaf759f5.png)\r\n\r\n**Expected outcome**\r\n\r\n![np-good](https://user-images.githubusercontent.com/901867/109069769-843a6400-76a6-11eb-9d2e-149fc615c583.png)\r\n\r\n**Matplotlib version**\r\n<!--Please specify your platform and versions of the relevant libraries you are using:-->\r\n  * Operating system: Windows 10\r\n  * Matplotlib version (`import matplotlib; print(matplotlib.__version__)`): 3.3.4\r\n  * Matplotlib backend (`print(matplotlib.get_backend())`): module://ipykernel.pylab.backend_inline\r\n  * Python version: 3.8.7\r\n  * Jupyter version (if applicable): see below\r\n  * Other libraries: see below\r\n\r\nTensorFlow 2.4.1\r\n\r\n```\r\njupyter --version\r\njupyter core     : 4.7.0\r\njupyter-notebook : 6.1.6\r\nqtconsole        : 5.0.1\r\nipython          : 7.20.0\r\nipykernel        : 5.4.2\r\njupyter client   : 6.1.7\r\njupyter lab      : not installed\r\nnbconvert        : 6.0.7\r\nipywidgets       : 7.6.3\r\nnbformat         : 5.0.8\r\ntraitlets        : 5.0.5\r\n```\r\n\r\nPython installed from python.org as an exe installer. Everything else is `pip install --user`\r\n\r\nBug opened with TensorFlow on this same issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/46274'], ['Be sure to check the existing issues (both open and closed!), and make sure you are running the latest version of Pipenv.\r\n\r\nCheck the [diagnose documentation](https://docs.pipenv.org/diagnose/) for common issues before posting! We may close your issue if it is very similar to one of them. Please be considerate, or be on your way.\r\n\r\nMake sure to mention your debugging experience if the documented solution failed.\r\n\r\n\r\n### Issue description\r\n\r\nI was trying to get pyenv and pipenv working on my machine (mac OS), and what resulted was that I could still run `pipenv shell` but any `pipenv run <command>` attempt failed.\r\n\r\nAfter some deep debugging, it turns out I had a terminal error appearing, that wasn\'t an obvious problem, but caused the subprocess.POpen call in delegator to fail as if it had not found the file, rather than an unknown error. \r\n\r\n```\r\n> /Users/abartlett/Library/Python/3.7/lib/python/site-packages/pipenv/core.py(1470)system_which()\r\n-> c = delegator.run("{0} {1}".format(_which, command))\r\n(Pdb) l\r\n1465 \t    os.environ = {\r\n1466 \t        vistir.compat.fs_str(k): vistir.compat.fs_str(val)\r\n1467 \t        for k, val in os.environ.items()\r\n1468 \t    }\r\n1469 \t    try:\r\n1470B->\t        c = delegator.run("{0} {1}".format(_which, command))\r\n1471 \t        try:\r\n1472 \t            # Which Not found…\r\n1473 \t            if c.return_code == 127:\r\n1474 \t                click.echo(\r\n1475 \t                    "{}: the {} system utility is required for Pipenv to find Python installations properly."\r\n> /Users/abartlett/Library/Python/3.7/lib/python/site-packages/pipenv/vendor/delegator.py(206)run()->None\r\n-> self.was_run = True\r\n(Pdb) l\r\n201  \t                pexpect_kwargs["env"].update(env)\r\n202  \t            # Enable Python subprocesses to work with expect functionality.\r\n203  \t            pexpect_kwargs["env"]["PYTHONUNBUFFERED"] = "1"\r\n204  \t            s = PopenSpawn(self._popen_args, **pexpect_kwargs)\r\n205  \t        self.subprocess = s\r\n206  ->\t        self.was_run = True\r\n207\r\n208  \t    def expect(self, pattern, timeout=-1):\r\n209  \t        """Waits on the given pattern to appear in std_out"""\r\n210\r\n211  \t        if self.blocking:\r\n(Pdb) p s.stderr.readlines()\r\n[\'dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicui18n.62.dylib\\n\', \'  Referenced from: /usr/local/bin/node\\n\', \'  Reason: image not found\\n\']\r\n```\r\n\r\n\r\n### Expected result\r\n\r\nI would expect the actual error to bubble up to the console, so that I know it failed to run the which command, rather than it failing silently. \r\n\r\n### Actual result\r\n\r\n```\r\n> pipenv run pip freeze\r\nLoading .env environment variables…\r\nError: the command pip could not be found within PATH or Pipfile\'s [scripts].\r\n```\r\n\r\n### Steps to replicate\r\nunsure how to replicate this, I suspect something that intentionally fails in your .bash_profile or .zshrc file. \r\n\r\nHave spent last day on this, and am unable to reproduce right now, will try next week. \r\n-------------------------------------------------------------------------------\r\n\r\n<details><summary>$ pipenv --support</summary>\r\n\r\nPipenv version: `\'2018.11.26\'`\r\n\r\nPipenv location: `\'/Users/abartlett/Library/Python/3.7/lib/python/site-packages/pipenv\'`\r\n\r\nPython location: `\'/usr/local/opt/python/bin/python3.7\'`\r\n\r\nPython installations found:\r\n\r\n  - `3.7.2`: `/usr/local/opt/python/libexec/bin/python`\r\n  - `3.7.2`: `/usr/local/bin/python3.7m`\r\n  - `2.7.10`: `/usr/bin/python`\r\n  - `2.7.10`: `/usr/bin/pythonw`\r\n  - `2.7.10`: `/usr/bin/python2.7`\r\n\r\nPEP 508 Information:\r\n\r\n```\r\n{\'implementation_name\': \'cpython\',\r\n \'implementation_version\': \'3.7.2\',\r\n \'os_name\': \'posix\',\r\n \'platform_machine\': \'x86_64\',\r\n \'platform_python_implementation\': \'CPython\',\r\n \'platform_release\': \'18.2.0\',\r\n \'platform_system\': \'Darwin\',\r\n \'platform_version\': \'Darwin Kernel Version 18.2.0: Fri Oct  5 19:41:49 PDT \'\r\n                     \'2018; root:xnu-4903.221.2~2/RELEASE_X86_64\',\r\n \'python_full_version\': \'3.7.2\',\r\n \'python_version\': \'3.7\',\r\n \'sys_platform\': \'darwin\'}\r\n```\r\n\r\nSystem environment variables:\r\n\r\n  - `TERM_SESSION_ID`\r\n  - `SSH_AUTH_SOCK`\r\n  - `Apple_PubSub_Socket_Render`\r\n  - `COLORFGBG`\r\n  - `ITERM_PROFILE`\r\n  - `XPC_FLAGS`\r\n  - `LANG`\r\n  - `PWD`\r\n  - `SHELL`\r\n  - `TERM_PROGRAM_VERSION`\r\n  - `TERM_PROGRAM`\r\n  - `PATH`\r\n  - `COLORTERM`\r\n  - `TERM`\r\n  - `HOME`\r\n  - `TMPDIR`\r\n  - `USER`\r\n  - `XPC_SERVICE_NAME`\r\n  - `LOGNAME`\r\n  - `__CF_USER_TEXT_ENCODING`\r\n  - `ITERM_SESSION_ID`\r\n  - `SHLVL`\r\n  - `OLDPWD`\r\n  - `BROWSER`\r\n  - `EDITOR`\r\n  - `VISUAL`\r\n  - `PAGER`\r\n  - `LESS`\r\n  - `LESS_TERMCAP_mb`\r\n  - `LESS_TERMCAP_md`\r\n  - `LESS_TERMCAP_me`\r\n  - `LESS_TERMCAP_se`\r\n  - `LESS_TERMCAP_so`\r\n  - `LESS_TERMCAP_ue`\r\n  - `LESS_TERMCAP_us`\r\n  - `LSCOLORS`\r\n  - `LS_COLORS`\r\n  - `GREP_COLOR`\r\n  - `GREP_COLORS`\r\n  - `WORKON_HOME`\r\n  - `PROJECT_HOME`\r\n  - `VIRTUALENVWRAPPER_PYTHON`\r\n  - `VIRTUALENVWRAPPER_PROJECT_FILENAME`\r\n  - `VIRTUALENVWRAPPER_WORKON_CD`\r\n  - `VIRTUALENVWRAPPER_SCRIPT`\r\n  - `VIRTUALENVWRAPPER_HOOK_DIR`\r\n  - `_`\r\n  - `PIP_DISABLE_PIP_VERSION_CHECK`\r\n  - `PYTHONDONTWRITEBYTECODE`\r\n  - `PIP_SHIMS_BASE_MODULE`\r\n  - `PIP_PYTHON_PATH`\r\n  - `PYTHONFINDER_IGNORE_UNSUPPORTED`\r\n\r\nPipenv–specific environment variables:\r\n\r\n\r\nDebug–specific environment variables:\r\n\r\n  - `PATH`: `/usr/local/opt/python/libexec/bin:/usr/local/opt/icu4c/sbin:/usr/local/opt/icu4c/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/abartlett/Library/Python/3.7/bin:/Applications/MacVim.app/Contents/bin`\r\n  - `SHELL`: `/usr/local/bin/zsh`\r\n  - `EDITOR`: `mvim -f --nomru -c "au VimLeave * !open -a iTerm2"`\r\n  - `LANG`: `en_AU.UTF-8`\r\n  - `PWD`: `/Users/abartlett/src/relist/backend`\r\n\r\n\r\n---------------------------\r\n</details>\r\n'], ['## My operating system is (include version):\r\nUbuntu 16.04 Xenial\r\n\r\n## I installed Certbot with (certbot-auto, OS package manager, pip, etc):\r\nOS Package Manager\r\n\r\n## I ran this command and it produced this output:\r\nI ran certbot update_symlinks and I\'m getting "Excepted (path to my cert.pem) to be a symlink".\r\n\r\n## Certbot\'s behavior differed from what I expected because:\r\nI wanted to fix my symlinks which were erased after transferring all my LE data, so I ran certbot update_symlinks, figuring it would fix them, but I keep getting the error (on a far off certificate in the alphabetical order, like not on the first ones, yet they do not differ) that it excepted this certificate to have a symlink.\r\n\r\n## Here is a Certbot log showing the issue (if available):\r\n###### Logs are stored in `/var/log/letsencrypt` by default. Feel free to redact domains, e-mail and IP addresses as you see fit.\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File "/usr/bin/certbot", line 11, in <module>\r\n    load_entry_point(\'certbot==0.26.1\', \'console_scripts\', \'certbot\')()\r\n  File "/usr/lib/python3/dist-packages/certbot/main.py", line 1364, in main\r\n    return config.func(config, plugins)\r\n  File "/usr/lib/python3/dist-packages/certbot/main.py", line 1000, in update_symlinks\r\n    cert_manager.update_live_symlinks(config)\r\n  File "/usr/lib/python3/dist-packages/certbot/cert_manager.py", line 39, in update_live_symlinks\r\n    storage.RenewableCert(renewal_file, config, update_symlinks=True)\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 438, in __init__\r\n    self._update_symlinks()\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 508, in _update_symlinks\r\n    previous_link = get_link_target(link)\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 211, in get_link_target\r\n    "Expected {0} to be a symlink".format(link))\r\ncertbot.errors.CertStorageError: Expected [Path_To_My_Certificate] to be a symlink\r\n2018-08-05 03:13:44,586:DEBUG:certbot.main:certbot version: 0.26.1\r\n2018-08-05 03:13:44,587:DEBUG:certbot.main:Arguments: [\'-v\']\r\n2018-08-05 03:13:44,587:DEBUG:certbot.main:Discovered plugins: PluginsRegistry(PluginEntryPoint#apache,PluginEntryPoint#dns-cloudflare,PluginEntryPoint#manual,PluginEntryPoint#null,PluginEntryPoint#standalone,PluginEntryPoint#webroot)\r\n2018-08-05 03:13:44,596:DEBUG:certbot.log:Root logging level set at 10\r\n2018-08-05 03:13:44,596:INFO:certbot.log:Saving debug log to /var/log/letsencrypt/letsencrypt.log\r\n2018-08-05 03:13:44,598:DEBUG:certbot.log:Exiting abnormally:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 208, in get_link_target\r\n    target = os.readlink(link)\r\nOSError: [Errno 22] Invalid argument: \'[Path_To_My_Certificate]\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File "/usr/bin/certbot", line 11, in <module>\r\n    load_entry_point(\'certbot==0.26.1\', \'console_scripts\', \'certbot\')()\r\n  File "/usr/lib/python3/dist-packages/certbot/main.py", line 1364, in main\r\n    return config.func(config, plugins)\r\n  File "/usr/lib/python3/dist-packages/certbot/main.py", line 1000, in update_symlinks\r\n    cert_manager.update_live_symlinks(config)\r\n  File "/usr/lib/python3/dist-packages/certbot/cert_manager.py", line 39, in update_live_symlinks\r\n    storage.RenewableCert(renewal_file, config, update_symlinks=True)\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 438, in __init__\r\n    self._update_symlinks()\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 508, in _update_symlinks\r\n    previous_link = get_link_target(link)\r\n  File "/usr/lib/python3/dist-packages/certbot/storage.py", line 211, in get_link_target\r\n    "Expected {0} to be a symlink".format(link))\r\ncertbot.errors.CertStorageError: Expected [Path_To_My_Certificate] to be a symlink\r\n```\r\n'], ["I hope scrapy's log can provide more convenience, such as crawlers running for a long time, log can be cut by day, so that the log file is not too large."], ["# 🚀 Feature request\r\n\r\nHello I was thinking it would be of great help if I can get the time offsets of start and end of each word .\r\n\r\n## Motivation\r\nI was going through Google Speech to text documentation and found this  [feature](https://cloud.google.com/speech-to-text/docs/async-time-offsets) and thought will be really amazing if i can have something similar here. \r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request\r\n     related to a problem? e.g., I'm always frustrated when [...]. If this is related\r\n     to another GitHub issue, please link here too. -->\r\n\r\n## Your contribution\r\n\r\nI can really use some help in this task and would love to implement something similar. \r\n\r\n<!-- Is there any way that you could help, e.g. by submitting a PR?\r\n     Make sure to read the CONTRIBUTING.MD readme:\r\n     https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md -->\r\n"], ['### Description of Issue\r\nWhen following the instructions on https://docs.saltstack.com/en/master/topics/development/hacking.html and trying to run `pip -e ./salt`, the pip command fails with:\r\n```\r\n$ pip install -e ./\r\nObtaining file:///home/xian/tech/gits/salt\r\nERROR: Exception:\r\nTraceback (most recent call last):\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/cli/base_command.py", line 186, in _main\r\n    status = self.run(options, args)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/commands/install.py", line 331, in run\r\n    resolver.resolve(requirement_set)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py", line 177, in resolve\r\n    discovered_reqs.extend(self._resolve_one(requirement_set, req))\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py", line 333, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py", line 265, in _get_abstract_dist_for\r\n    return self.preparer.prepare_editable_requirement(req)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/operations/prepare.py", line 555, in prepare_editable_requirement\r\n    req, self.req_tracker, self.finder, self.build_isolation,\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/operations/prepare.py", line 95, in _get_prepared_distribution\r\n    abstract_dist.prepare_distribution_metadata(finder, build_isolation)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/distributions/sdist.py", line 33, in prepare_distribution_metadata\r\n    self.req.load_pyproject_toml()\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/req/req_install.py", line 516, in load_pyproject_toml\r\n    str(self)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_internal/pyproject.py", line 75, in load_pyproject_toml\r\n    pp_toml = pytoml.load(f)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_vendor/pytoml/parser.py", line 11, in load\r\n    return loads(fin.read(), translate=translate, object_pairs_hook=object_pairs_hook, filename=getattr(fin, \'name\', repr(fin)))\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_vendor/pytoml/parser.py", line 24, in loads\r\n    ast = _p_toml(src, object_pairs_hook=object_pairs_hook)\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_vendor/pytoml/parser.py", line 341, in _p_toml\r\n    s.expect_eof()\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_vendor/pytoml/parser.py", line 123, in expect_eof\r\n    return self._expect(self.consume_eof())\r\n  File "/home/xian/.local/share/virtualenvs/salt-csy8NuFj/lib/python3.7/site-packages/pip/_vendor/pytoml/parser.py", line 163, in _expect\r\n    raise TomlError(\'msg\', self._pos[0], self._pos[1], self._filename)\r\npip._vendor.pytoml.core.TomlError: /home/xian/tech/gits/salt/pyproject.toml(2, 1): msg\r\n```\r\nmoving pyproject.toml out of the way allows `pip -e .` to continue\r\n\r\n\r\n### Versions Report\r\ngit checkout\r\n'], ['can you also create an issue on the pandas side so we know what we need to update once dask is at a certain version (you can just point to this line here)\r\n\r\n_Originally posted by @jreback in https://github.com/_render_node/MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1MjM4MzI0NA==/comments/review_comment_\r\n\r\n**Note:** This will be on hold for quite a while (first merging my PR, then waiting until dask has silenced the warning on their side, then bumping our dask dependency)'], ['Follow up for #12532 .\r\n\r\nWe need to fully move the Timeseries Table plugin to [`@superset-ui/legacy-plugin-chart-time-table`](https://github.com/apache-superset/superset-ui/blob/84a6d592b9470883150c1085dada740b69e46fe6/plugins/legacy-plugin-chart-time-table/) and delete related files in `superset-frontend`.'], ['When trying to load a 15MB GeoJSON file using `GeoJSONDataSource(geojson=geojson)` I accidentally passed a list instead of a string. This triggered `ValueError expected a value of type str, got ...` which then dumped the WHOLE GeoJSON data into the notebook error output. Notebook file ballooned to 30MB from 400kB and crashed the browser because of the volume of output. Since the browser crashed on open, it was impossible to recover the notebook from within Jupyter. Was only able to recover by opening raw ipynb in SublimeText and deleting output from cell.\r\n\r\nBokeh should take care never to dump whole inputs, especially with GIS data which can be very large.\r\n'], ["**Apache Airflow version**: 1.10.12\r\n\r\n**Environment**:  Centos 7 Host\r\n\r\n- **Cloud provider or hardware configuration**: \r\n- **OS** (e.g. from /etc/os-release): Ubuntu 20.04 Docker Container\r\n- **Kernel** (e.g. `uname -a`): Linux c6c6e8230c17 3.10.0-1127.18.2.el7.x86_64 #1 SMP Sun Jul 26 15:27:06 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n- **Install tools**: Docker Compose\r\n- **Others** ?\r\n\r\n**What happened**:\r\n\r\nThe schema file generated by MSSQLToGCSOperator specifies that a field defined as **bit** in MSSQL, is defined as **INTEGER**.  However, the JSON file defines the value as **true**/**false**.  When attempting to import into BigQuery using the GCSOperatorToBigQuery operator, the job fails:\r\n`Error while reading data, error message: JSON parsing error in row starting at position 0: Could not convert value 'boolean_value: false' to integer. Field: manager; Value: 0`\r\n\r\n**What you expected to happen**:\r\n\r\nValue should be defined as **BOOLEAN** in the schema.\r\n\r\n**How to reproduce it**:\r\nEnsure `dbo.Customers` table has a **bit** column:\r\n\r\n```\r\nexport_customers = MsSqlToGoogleCloudStorageOperator(\r\n                task_id='export_customers',\r\n                sql='SELECT * FROM dbo.Customers;',\r\n                bucket='mssql-export',\r\n                filename='data/customers/export.json',\r\n                schema_filename='schemas/export.json',\r\n                mssql_conn_id='mssql_default',\r\n                google_cloud_storage_conn_id='google_cloud_default',\r\n                dag=dag\r\n            )\r\n```"], ['We shouldn\'t really be patching in `setUp` and saving the patched mocks.\r\n\r\n"If you use this technique you must ensure that the patching is “undone” by calling stop. This can be fiddlier than you might think, because if an exception is raised in the setUp then tearDown is not called. unittest.TestCase.addCleanup() makes this easier"\r\nfrom https://docs.python.org/3.5/library/unittest.mock-examples.html'], ['**Description**\r\nSupport of [AssumeRoleWithWebIdentity](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html#STS.Client.assume_role_with_web_identity) for AWS provider when running Airflow workers in EKS. \r\n\r\n**Use case / motivation**\r\nThis feature will allow us to use [IRSA](https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/) with Airflow Pods running on EKS and x-account _assumeRole_.\r\n\r\nA connection type aws with empty username & password and the following extra parameters\r\n```json\r\n{\r\n"role_arn": "<role_arn>",\r\n"region_name": "<region>",\r\n"aws_session_token": "file://$AWS_WEB_IDENTITY_TOKEN_FILE"\r\n}\r\n```\r\nwill retrieve temporary credentials using method `sts-assume-role-with-web-identity` (see [boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html#STS.Client.assume_role_with_web_identity) of this method)\r\n'], ["As part of #5082, we added a basic dropdown menu for making it possible to set a bot as an incoming webhook, with limited permissions to only send messages.\r\n\r\nI think it's probably worth doing a pass at making that look really nice, keeping in mind that we'll eventually have at least 4 choices:\r\n* Incoming webook\r\n* Outgoing webhook (with sub-options for format)\r\n* Generic bot\r\n* Embedded bot (with sub-options for which one)\r\n"]];
            var data = [['<div><a href=https://github.com/apache/airflow/issues/11673 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Issue with GCS and GCStoGCSOperator when dropping file', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop0>apache/airflow</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/17716 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Bot Email Field', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop1>zulip/zulip</div>', 8, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/17731 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Improve time widget text', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop2>zulip/zulip</div>', 14, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/38990 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'DOC: add comparison to spreadsheets', '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop3>pandas-dev/pandas</div>', 8, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/18062 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'help center: Improve formatting for labels in stream permissions table', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop4>zulip/zulip</div>', 8, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/dask/dask/issues/4855 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Flattening Dask Arrays without spreading out chunks', '<span class="badge badge-info">dask</span> <span class="badge badge-info">python</span> <span class="badge badge-info">pydata</span> <span class="badge badge-info">numpy</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">scikit-learn</span> <span class="badge badge-info">scipy</span>', '<div id=pop6>dask/dask</div>', 16, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/SFTtech/openage/issues/815 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Add development guide with tips and tricks', '<span class="badge badge-info">game</span> <span class="badge badge-info">engine</span> <span class="badge badge-info">clone</span> <span class="badge badge-info">python</span> <span class="badge badge-info">c-plus-plus</span> <span class="badge badge-info">qt</span> <span class="badge badge-info">nyan</span> <span class="badge badge-info">opengl</span> <span class="badge badge-info">openage</span> <span class="badge badge-info">age-of-empires</span> <span class="badge badge-info">cmake</span> <span class="badge badge-info">multiplayer</span> <span class="badge badge-info">linux</span> <span class="badge badge-info">game-engine</span> <span class="badge badge-info">entity-component-system</span> <span class="badge badge-info">rts-engine</span> <span class="badge badge-info">cpp</span> <span class="badge badge-info">cpp17</span> <span class="badge badge-info">game-development</span>', '<div id=pop7>SFTtech/openage</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/13419 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[TODO] Clean up `controlPanels/sections`  and `controls` in superset-frontend', '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop8>apache/superset</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/streamlit/streamlit/issues/2953 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Comment out config options in `streamlit config show` if they are set to their default values', '<span class="badge badge-info">python</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">streamlit</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">developer-tools</span>', '<div id=pop9>streamlit/streamlit</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/19958 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Incorrect coloring of patch diff - make yellow', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop10>saltstack/salt</div>', 16, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/mlflow/mlflow/issues/2931 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[FR] Check and truncate string length', '<span class="badge badge-info">machine-learning</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">ml</span> <span class="badge badge-info">mlflow</span> <span class="badge badge-info">apache-spark</span> <span class="badge badge-info">model-management</span>', '<div id=pop11>mlflow/mlflow</div>', 9, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/15005 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '`GCSToLocalFilesystemOperator` unnecessarily downloads objects when it checks object size', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop12>apache/airflow</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/53529 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'SaltStack removing / character from Debian APT URIs', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop13>saltstack/salt</div>', 17, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/bokeh/bokeh/issues/10376 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'ColorBar should display low/high color values of LinearColorMapper', '<span class="badge badge-info">bokeh</span> <span class="badge badge-info">python</span> <span class="badge badge-info">interactive-plots</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">visualization</span> <span class="badge badge-info">plotting</span> <span class="badge badge-info">plots</span> <span class="badge badge-info">data-visualisation</span> <span class="badge badge-info">notebooks</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">visualisation</span> <span class="badge badge-info">numfocus</span>', '<div id=pop14>bokeh/bokeh</div>', 4, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/13542 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', "Safari: 'explore-container' div has more height than 'app' div on Dataset Explore page ", '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop16>apache/superset</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/PyTorchLightning/pytorch-lightning/issues/5001 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[Enhance] Update reload_dataloaders_every_epoch to reload_dataloaders_every_n_epoch', '<span class="badge badge-info">python</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">artificial-intelligence</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">machine-learning</span>', '<div id=pop17>PyTorchLightning/pytorch-lightning</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ray-project/ray/issues/9316 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[rllib] h5 export format is not supported', '<span class="badge badge-info">ray</span> <span class="badge badge-info">distributed</span> <span class="badge badge-info">parallel</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">reinforcement-learning</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">rllib</span> <span class="badge badge-info">hyperparameter-search</span> <span class="badge badge-info">optimization</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">automl</span> <span class="badge badge-info">hyperparameter-optimization</span> <span class="badge badge-info">model-selection</span> <span class="badge badge-info">java</span> <span class="badge badge-info">serving</span> <span class="badge badge-info">deployment</span>', '<div id=pop18>ray-project/ray</div>', 9, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/SFTtech/openage/issues/1273 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Replace jinja2 with mako', '<span class="badge badge-info">game</span> <span class="badge badge-info">engine</span> <span class="badge badge-info">clone</span> <span class="badge badge-info">python</span> <span class="badge badge-info">c-plus-plus</span> <span class="badge badge-info">qt</span> <span class="badge badge-info">nyan</span> <span class="badge badge-info">opengl</span> <span class="badge badge-info">openage</span> <span class="badge badge-info">age-of-empires</span> <span class="badge badge-info">cmake</span> <span class="badge badge-info">multiplayer</span> <span class="badge badge-info">linux</span> <span class="badge badge-info">game-engine</span> <span class="badge badge-info">entity-component-system</span> <span class="badge badge-info">rts-engine</span> <span class="badge badge-info">cpp</span> <span class="badge badge-info">cpp17</span> <span class="badge badge-info">game-development</span>', '<div id=pop19>SFTtech/openage</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/12012 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Add your Company to Airflow Users list', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop20>apache/airflow</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/mlflow/mlflow/issues/315 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Filter out UID, GID etc when tarring project for upload to DBFS during Databricks execution', '<span class="badge badge-info">machine-learning</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">ml</span> <span class="badge badge-info">mlflow</span> <span class="badge badge-info">apache-spark</span> <span class="badge badge-info">model-management</span>', '<div id=pop21>mlflow/mlflow</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/13386 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', "[SQL Lab] Copy partition to clipboard doesn't work in left SQL Lab panel", '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop22>apache/superset</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/12401 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Duplicate connections UI', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop23>apache/airflow</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/streamlit/streamlit/issues/1880 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Improve text justification in dataframe', '<span class="badge badge-info">python</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">streamlit</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">developer-tools</span>', '<div id=pop24>streamlit/streamlit</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scikit-learn/scikit-learn/issues/13117 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Possible error in documentation of ElasticNetCV()', '<span class="badge badge-info">machine-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">statistics</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-analysis</span>', '<div id=pop25>scikit-learn/scikit-learn</div>', 6, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ray-project/ray/issues/14998 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Feature Request: Python API to get current Ray cluster information', '<span class="badge badge-info">ray</span> <span class="badge badge-info">distributed</span> <span class="badge badge-info">parallel</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">reinforcement-learning</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">rllib</span> <span class="badge badge-info">hyperparameter-search</span> <span class="badge badge-info">optimization</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">automl</span> <span class="badge badge-info">hyperparameter-optimization</span> <span class="badge badge-info">model-selection</span> <span class="badge badge-info">java</span> <span class="badge badge-info">serving</span> <span class="badge badge-info">deployment</span>', '<div id=pop26>ray-project/ray</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ray-project/ray/issues/5639 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[rllib] Figure out why DDPG refactor caused exploding Q values, undo revert', '<span class="badge badge-info">ray</span> <span class="badge badge-info">distributed</span> <span class="badge badge-info">parallel</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">reinforcement-learning</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">rllib</span> <span class="badge badge-info">hyperparameter-search</span> <span class="badge badge-info">optimization</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">automl</span> <span class="badge badge-info">hyperparameter-optimization</span> <span class="badge badge-info">model-selection</span> <span class="badge badge-info">java</span> <span class="badge badge-info">serving</span> <span class="badge badge-info">deployment</span>', '<div id=pop27>ray-project/ray</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/56645 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[BUG] gpg.encrypt text=foo output=/some/file does not write to a file', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop28>saltstack/salt</div>', 4, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scrapy/scrapy/issues/4991 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Use transport.loseConnection()', '<span class="badge badge-info">python</span> <span class="badge badge-info">scraping</span> <span class="badge badge-info">crawling</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">crawler</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop29>scrapy/scrapy</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/bokeh/bokeh/issues/10028 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[FEATURE] More default theme for Bokeh', '<span class="badge badge-info">bokeh</span> <span class="badge badge-info">python</span> <span class="badge badge-info">interactive-plots</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">visualization</span> <span class="badge badge-info">plotting</span> <span class="badge badge-info">plots</span> <span class="badge badge-info">data-visualisation</span> <span class="badge badge-info">notebooks</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">visualisation</span> <span class="badge badge-info">numfocus</span>', '<div id=pop30>bokeh/bokeh</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/psf/requests-html/issues/226 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Need method get all children', '<span class="badge badge-info">html</span> <span class="badge badge-info">scraping</span> <span class="badge badge-info">python</span> <span class="badge badge-info">requests</span> <span class="badge badge-info">http</span> <span class="badge badge-info">kennethreitz</span> <span class="badge badge-info">lxml</span> <span class="badge badge-info">pyquery</span> <span class="badge badge-info">css-selectors</span> <span class="badge badge-info">beautifulsoup</span>', '<div id=pop32>psf/requests-html</div>', 4, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/falconry/falcon/issues/1853 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Remove all deprecated behaviors and shims from 3.x', '<span class="badge badge-info">python</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">rest</span> <span class="badge badge-info">microservices</span> <span class="badge badge-info">web</span> <span class="badge badge-info">api</span> <span class="badge badge-info">http</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop33>falconry/falcon</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/bokeh/bokeh/issues/9223 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[FEATURE] Make INLINE resources configurable for Sphinx', '<span class="badge badge-info">bokeh</span> <span class="badge badge-info">python</span> <span class="badge badge-info">interactive-plots</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">visualization</span> <span class="badge badge-info">plotting</span> <span class="badge badge-info">plots</span> <span class="badge badge-info">data-visualisation</span> <span class="badge badge-info">notebooks</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">visualisation</span> <span class="badge badge-info">numfocus</span>', '<div id=pop34>bokeh/bokeh</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/python-poetry/poetry/issues/3060 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '📚 Contribute to documentation', '<span class="badge badge-info">python</span> <span class="badge badge-info">dependency-manager</span> <span class="badge badge-info">package-manager</span> <span class="badge badge-info">packaging</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop35>python-poetry/poetry</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/huggingface/transformers/issues/11246 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Enable Wav2Vec2 Pretraining', '<span class="badge badge-info">nlp</span> <span class="badge badge-info">natural-language-processing</span> <span class="badge badge-info">natural-language-understanding</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">language-model</span> <span class="badge badge-info">natural-language-generation</span> <span class="badge badge-info">tensorflow</span> <span class="badge badge-info">bert</span> <span class="badge badge-info">gpt</span> <span class="badge badge-info">xlnet</span> <span class="badge badge-info">language-models</span> <span class="badge badge-info">xlm</span> <span class="badge badge-info">transformer-xl</span> <span class="badge badge-info">pytorch-transformers</span> <span class="badge badge-info">hacktoberfest-accepted</span> <span class="badge badge-info">nlp-library</span> <span class="badge badge-info">transformer</span> <span class="badge badge-info">model-hub</span> <span class="badge badge-info">pretrained-models</span>', '<div id=pop36>huggingface/transformers</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/PyTorchLightning/pytorch-lightning/issues/5925 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Add pl.Trainer types', '<span class="badge badge-info">python</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">artificial-intelligence</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">machine-learning</span>', '<div id=pop37>PyTorchLightning/pytorch-lightning</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scrapy/scrapy/issues/4842 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Document files inside JOBDIR', '<span class="badge badge-info">python</span> <span class="badge badge-info">scraping</span> <span class="badge badge-info">crawling</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">crawler</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop38>scrapy/scrapy</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/17929 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Fix test suites that do not use `sanity_check` and/or `assert_provisioning_status_ok`', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop39>zulip/zulip</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/15269 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Update GoogleAdsHook to authenticate via OAuth2', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop40>apache/airflow</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/18319 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Create incoming webhook integration for Open Collective', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop41>zulip/zulip</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ray-project/ray/issues/13465 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Unify linting of clang-format and *.proto files', '<span class="badge badge-info">ray</span> <span class="badge badge-info">distributed</span> <span class="badge badge-info">parallel</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">reinforcement-learning</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">rllib</span> <span class="badge badge-info">hyperparameter-search</span> <span class="badge badge-info">optimization</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">automl</span> <span class="badge badge-info">hyperparameter-optimization</span> <span class="badge badge-info">model-selection</span> <span class="badge badge-info">java</span> <span class="badge badge-info">serving</span> <span class="badge badge-info">deployment</span>', '<div id=pop42>ray-project/ray</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ansible/awx/issues/3988 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Jobs canceled before starting should not show a "finished" time', '<span class="badge badge-info">python</span> <span class="badge badge-info">ansible</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">django</span> <span class="badge badge-info">django-rest-framework</span> <span class="badge badge-info">awx</span> <span class="badge badge-info">automation</span> <span class="badge badge-info">reactjs</span>', '<div id=pop43>ansible/awx</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ipython/ipython/issues/9891 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', "feature request: redirect magic's result to pager", '<span class="badge badge-info">ipython</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">notebook</span> <span class="badge badge-info">python</span> <span class="badge badge-info">repl</span> <span class="badge badge-info">closember</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop44>ipython/ipython</div>', 13, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/networkx/networkx/issues/3443 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'draw_networkx_nodes misshandles ax (easy fix).', '<span class="badge badge-info">python</span> <span class="badge badge-info">complex-networks</span> <span class="badge badge-info">graph-theory</span> <span class="badge badge-info">graph-algorithms</span> <span class="badge badge-info">graph-analysis</span> <span class="badge badge-info">graph-generation</span> <span class="badge badge-info">graph-visualization</span>', '<div id=pop45>networkx/networkx</div>', 15, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/55983 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Human-readable expire in user.present', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop46>saltstack/salt</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/38482 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'DOC: get_indexer returns non-matching with -1 positional', '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop47>pandas-dev/pandas</div>', 11, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/10004 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[TRACKER] Migrate JavaScript files to TypeScript', '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop48>apache/superset</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/14732 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Add a /help/ article documenting what browsers we support', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop49>zulip/zulip</div>', 10, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/13252 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[Explore]show actual column name in tooltips on hover ', '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop50>apache/superset</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/PyTorchLightning/pytorch-lightning/issues/6295 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', "all_gather for TPU doesn't support backward gradients.", '<span class="badge badge-info">python</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">artificial-intelligence</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">machine-learning</span>', '<div id=pop51>PyTorchLightning/pytorch-lightning</div>', 1, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/python-poetry/poetry/issues/3055 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Pass --prompt to be project name', '<span class="badge badge-info">python</span> <span class="badge badge-info">dependency-manager</span> <span class="badge badge-info">package-manager</span> <span class="badge badge-info">packaging</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop52>python-poetry/poetry</div>', 4, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/30232 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', "Move 'For Developers' content from wiki to contributing docs.", '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop53>pandas-dev/pandas</div>', 19, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/dask/dask/issues/6659 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Merge returns duplicate indices', '<span class="badge badge-info">dask</span> <span class="badge badge-info">python</span> <span class="badge badge-info">pydata</span> <span class="badge badge-info">numpy</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">scikit-learn</span> <span class="badge badge-info">scipy</span>', '<div id=pop54>dask/dask</div>', 6, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/13579 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Wrong color for bar charts', '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop55>apache/superset</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/PyTorchLightning/pytorch-lightning/issues/5023 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'fix typing in PL codebase - multiple PRs', '<span class="badge badge-info">python</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">artificial-intelligence</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">machine-learning</span>', '<div id=pop56>PyTorchLightning/pytorch-lightning</div>', 18, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/14392 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Refer Best Practices of using Variables in the Concepts/Variables section', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop57>apache/airflow</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scikit-learn/scikit-learn/issues/19781 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Failed xtests on test_docstring_parameters.py (documentation issues)', '<span class="badge badge-info">machine-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">statistics</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-analysis</span>', '<div id=pop59>scikit-learn/scikit-learn</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/certbot/certbot/issues/4338 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Refactor Apache plugin unit tests to decouple test config files', '<span class="badge badge-info">acme</span> <span class="badge badge-info">acme-client</span> <span class="badge badge-info">certbot</span> <span class="badge badge-info">certificate</span> <span class="badge badge-info">letsencrypt</span> <span class="badge badge-info">python</span>', '<div id=pop60>certbot/certbot</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ipython/ipython/issues/2569 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Add install instructions when dependencies are missing', '<span class="badge badge-info">ipython</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">notebook</span> <span class="badge badge-info">python</span> <span class="badge badge-info">repl</span> <span class="badge badge-info">closember</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop61>ipython/ipython</div>', 15, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/falconry/falcon/issues/1457 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Refactor: Privatize helpers, etc. that really should only be used internally by the framework', '<span class="badge badge-info">python</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">rest</span> <span class="badge badge-info">microservices</span> <span class="badge badge-info">web</span> <span class="badge badge-info">api</span> <span class="badge badge-info">http</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop62>falconry/falcon</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/4549 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Unofficial winston zulip contribution', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop63>zulip/zulip</div>', 6, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/14758 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Example DAG for New Azure Data Factory Hooks', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop64>apache/airflow</div>', 8, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/52977 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'pkg.upgrade does not handle --allow-downgrades for apt dist-upgrade', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop65>saltstack/salt</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scrapy/scrapy/issues/2733 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Import Request in the Template file', '<span class="badge badge-info">python</span> <span class="badge badge-info">scraping</span> <span class="badge badge-info">crawling</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">crawler</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop66>scrapy/scrapy</div>', 13, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/PyTorchLightning/pytorch-lightning/issues/5434 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Document exceptions', '<span class="badge badge-info">python</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">artificial-intelligence</span> <span class="badge badge-info">ai</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">machine-learning</span>', '<div id=pop67>PyTorchLightning/pytorch-lightning</div>', 15, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scikit-learn/scikit-learn/issues/18837 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Duplicate check_finite when calling scipy.linalg functions', '<span class="badge badge-info">machine-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">statistics</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-analysis</span>', '<div id=pop68>scikit-learn/scikit-learn</div>', 27, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/29687 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '\'sort_index"does not work with \'pd.set_option(\'mode.use_inf_as_na\',True)\'', '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop69>pandas-dev/pandas</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/python-poetry/poetry/issues/3163 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '`extras_require` ignores version specifier', '<span class="badge badge-info">python</span> <span class="badge badge-info">dependency-manager</span> <span class="badge badge-info">package-manager</span> <span class="badge badge-info">packaging</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop71>python-poetry/poetry</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ray-project/ray/issues/12325 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[Autoscaler] Add option to force setup_commands to re-run', '<span class="badge badge-info">ray</span> <span class="badge badge-info">distributed</span> <span class="badge badge-info">parallel</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">reinforcement-learning</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">rllib</span> <span class="badge badge-info">hyperparameter-search</span> <span class="badge badge-info">optimization</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">automl</span> <span class="badge badge-info">hyperparameter-optimization</span> <span class="badge badge-info">model-selection</span> <span class="badge badge-info">java</span> <span class="badge badge-info">serving</span> <span class="badge badge-info">deployment</span>', '<div id=pop72>ray-project/ray</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/40250 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'BUG: ValueError when calling groupby + fillna on StringDType column', '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop73>pandas-dev/pandas</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/huggingface/transformers/issues/9455 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Rename `nlp` variables into more appropriate names', '<span class="badge badge-info">nlp</span> <span class="badge badge-info">natural-language-processing</span> <span class="badge badge-info">natural-language-understanding</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">language-model</span> <span class="badge badge-info">natural-language-generation</span> <span class="badge badge-info">tensorflow</span> <span class="badge badge-info">bert</span> <span class="badge badge-info">gpt</span> <span class="badge badge-info">xlnet</span> <span class="badge badge-info">language-models</span> <span class="badge badge-info">xlm</span> <span class="badge badge-info">transformer-xl</span> <span class="badge badge-info">pytorch-transformers</span> <span class="badge badge-info">hacktoberfest-accepted</span> <span class="badge badge-info">nlp-library</span> <span class="badge badge-info">transformer</span> <span class="badge badge-info">model-hub</span> <span class="badge badge-info">pretrained-models</span>', '<div id=pop74>huggingface/transformers</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/huggingface/transformers/issues/8944 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'how to use EncoderDecoderModel to do en-de translation?', '<span class="badge badge-info">nlp</span> <span class="badge badge-info">natural-language-processing</span> <span class="badge badge-info">natural-language-understanding</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">language-model</span> <span class="badge badge-info">natural-language-generation</span> <span class="badge badge-info">tensorflow</span> <span class="badge badge-info">bert</span> <span class="badge badge-info">gpt</span> <span class="badge badge-info">xlnet</span> <span class="badge badge-info">language-models</span> <span class="badge badge-info">xlm</span> <span class="badge badge-info">transformer-xl</span> <span class="badge badge-info">pytorch-transformers</span> <span class="badge badge-info">hacktoberfest-accepted</span> <span class="badge badge-info">nlp-library</span> <span class="badge badge-info">transformer</span> <span class="badge badge-info">model-hub</span> <span class="badge badge-info">pretrained-models</span>', '<div id=pop75>huggingface/transformers</div>', 11, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scrapy/scrapy/issues/4964 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'response_httprepr in DownloaderStats middleware causing scrapy to make unnecessary memory allocation.', '<span class="badge badge-info">python</span> <span class="badge badge-info">scraping</span> <span class="badge badge-info">crawling</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">crawler</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop76>scrapy/scrapy</div>', 4, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/SFTtech/openage/issues/1288 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Age of Empires 1: Definitive Edition new features support', '<span class="badge badge-info">game</span> <span class="badge badge-info">engine</span> <span class="badge badge-info">clone</span> <span class="badge badge-info">python</span> <span class="badge badge-info">c-plus-plus</span> <span class="badge badge-info">qt</span> <span class="badge badge-info">nyan</span> <span class="badge badge-info">opengl</span> <span class="badge badge-info">openage</span> <span class="badge badge-info">age-of-empires</span> <span class="badge badge-info">cmake</span> <span class="badge badge-info">multiplayer</span> <span class="badge badge-info">linux</span> <span class="badge badge-info">game-engine</span> <span class="badge badge-info">entity-component-system</span> <span class="badge badge-info">rts-engine</span> <span class="badge badge-info">cpp</span> <span class="badge badge-info">cpp17</span> <span class="badge badge-info">game-development</span>', '<div id=pop77>SFTtech/openage</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/dask/dask/issues/6067 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Please implement numpy.select()', '<span class="badge badge-info">dask</span> <span class="badge badge-info">python</span> <span class="badge badge-info">pydata</span> <span class="badge badge-info">numpy</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">scikit-learn</span> <span class="badge badge-info">scipy</span>', '<div id=pop78>dask/dask</div>', 8, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/38627 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'DOC: example for doing a reversed rolling window', '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop79>pandas-dev/pandas</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/bokeh/bokeh/issues/10902 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[BUG] defining colors is inconsistent', '<span class="badge badge-info">bokeh</span> <span class="badge badge-info">python</span> <span class="badge badge-info">interactive-plots</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">visualization</span> <span class="badge badge-info">plotting</span> <span class="badge badge-info">plots</span> <span class="badge badge-info">data-visualisation</span> <span class="badge badge-info">notebooks</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">visualisation</span> <span class="badge badge-info">numfocus</span>', '<div id=pop80>bokeh/bokeh</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/16080 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Deactivate organization button is not disabled for regular users', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop81>zulip/zulip</div>', 12, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pypa/pipenv/issues/2553 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Errors if WORKON_HOME is not writable could be better', '<span class="badge badge-info">pip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">packaging</span> <span class="badge badge-info">virtualenv</span> <span class="badge badge-info">pipfile</span>', '<div id=pop82>pypa/pipenv</div>', 12, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/bokeh/bokeh/issues/10756 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[BUG] Jitter caching too aggressively', '<span class="badge badge-info">bokeh</span> <span class="badge badge-info">python</span> <span class="badge badge-info">interactive-plots</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">visualization</span> <span class="badge badge-info">plotting</span> <span class="badge badge-info">plots</span> <span class="badge badge-info">data-visualisation</span> <span class="badge badge-info">notebooks</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">visualisation</span> <span class="badge badge-info">numfocus</span>', '<div id=pop83>bokeh/bokeh</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/59853 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[BUG] Missing documentation on pyupgrade in pull request docs', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop84>saltstack/salt</div>', 2, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/15016 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'OdbcHook string values in connect_kwargs dict converts to None', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop85>apache/airflow</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/ray-project/ray/issues/7639 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[Suggestion] Use versioneer to create the version string for Ray', '<span class="badge badge-info">ray</span> <span class="badge badge-info">distributed</span> <span class="badge badge-info">parallel</span> <span class="badge badge-info">machine-learning</span> <span class="badge badge-info">reinforcement-learning</span> <span class="badge badge-info">deep-learning</span> <span class="badge badge-info">python</span> <span class="badge badge-info">rllib</span> <span class="badge badge-info">hyperparameter-search</span> <span class="badge badge-info">optimization</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">automl</span> <span class="badge badge-info">hyperparameter-optimization</span> <span class="badge badge-info">model-selection</span> <span class="badge badge-info">java</span> <span class="badge badge-info">serving</span> <span class="badge badge-info">deployment</span>', '<div id=pop86>ray-project/ray</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pypa/pipenv/issues/3454 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Pipenv run swallows terminal errors, and shows standard command <command> could not be found', '<span class="badge badge-info">pip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">packaging</span> <span class="badge badge-info">virtualenv</span> <span class="badge badge-info">pipfile</span>', '<div id=pop88>pypa/pipenv</div>', 5, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/certbot/certbot/issues/6284 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Remove --update-symlinks flag', '<span class="badge badge-info">acme</span> <span class="badge badge-info">acme-client</span> <span class="badge badge-info">certbot</span> <span class="badge badge-info">certificate</span> <span class="badge badge-info">letsencrypt</span> <span class="badge badge-info">python</span>', '<div id=pop89>certbot/certbot</div>', 7, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/scrapy/scrapy/issues/3628 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Crawler logs are cut by day', '<span class="badge badge-info">python</span> <span class="badge badge-info">scraping</span> <span class="badge badge-info">crawling</span> <span class="badge badge-info">framework</span> <span class="badge badge-info">crawler</span> <span class="badge badge-info">hacktoberfest</span>', '<div id=pop90>scrapy/scrapy</div>', 15, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/huggingface/transformers/issues/11307 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Getting time offsets of beginning and end of each word in Wav2Vec2', '<span class="badge badge-info">nlp</span> <span class="badge badge-info">natural-language-processing</span> <span class="badge badge-info">natural-language-understanding</span> <span class="badge badge-info">pytorch</span> <span class="badge badge-info">language-model</span> <span class="badge badge-info">natural-language-generation</span> <span class="badge badge-info">tensorflow</span> <span class="badge badge-info">bert</span> <span class="badge badge-info">gpt</span> <span class="badge badge-info">xlnet</span> <span class="badge badge-info">language-models</span> <span class="badge badge-info">xlm</span> <span class="badge badge-info">transformer-xl</span> <span class="badge badge-info">pytorch-transformers</span> <span class="badge badge-info">hacktoberfest-accepted</span> <span class="badge badge-info">nlp-library</span> <span class="badge badge-info">transformer</span> <span class="badge badge-info">model-hub</span> <span class="badge badge-info">pretrained-models</span>', '<div id=pop91>huggingface/transformers</div>', 10, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/saltstack/salt/issues/56579 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Broken pyproject.toml', '<span class="badge badge-info">python</span> <span class="badge badge-info">configuration-management</span> <span class="badge badge-info">remote-execution</span> <span class="badge badge-info">infrastructure-management</span> <span class="badge badge-info">zeromq</span> <span class="badge badge-info">event-stream</span> <span class="badge badge-info">event-management</span> <span class="badge badge-info">cloud-providers</span> <span class="badge badge-info">cloud-management</span> <span class="badge badge-info">cloud-provisioning</span> <span class="badge badge-info">infrasructure</span> <span class="badge badge-info">infrastructure-automation</span> <span class="badge badge-info">infrastructure-as-code</span> <span class="badge badge-info">infrastructure-as-a-code</span> <span class="badge badge-info">iot</span> <span class="badge badge-info">edge</span> <span class="badge badge-info">cloud</span>', '<div id=pop92>saltstack/salt</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/pandas-dev/pandas/issues/29960 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'DOC: Remove :okwarning: from scale guide when used dask version is not raising FutureWarning anymore', '<span class="badge badge-info">data-analysis</span> <span class="badge badge-info">pandas</span> <span class="badge badge-info">flexible</span> <span class="badge badge-info">alignment</span> <span class="badge badge-info">python</span>', '<div id=pop93>pandas-dev/pandas</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/superset/issues/12533 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', '[code hygiene] Fully move Timeseries Table plugin to superset-ui ', '<span class="badge badge-info">superset</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-superset</span> <span class="badge badge-info">data-visualization</span> <span class="badge badge-info">data-viz</span> <span class="badge badge-info">analytics</span> <span class="badge badge-info">business-intelligence</span> <span class="badge badge-info">data-science</span> <span class="badge badge-info">data-engineering</span> <span class="badge badge-info">asf</span> <span class="badge badge-info">bi</span> <span class="badge badge-info">business-analytics</span> <span class="badge badge-info">data-analytics</span> <span class="badge badge-info">data-analysis</span> <span class="badge badge-info">python</span> <span class="badge badge-info">react</span> <span class="badge badge-info">sql-editor</span> <span class="badge badge-info">flask</span>', '<div id=pop94>apache/superset</div>', 0, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/bokeh/bokeh/issues/5464 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Truncate error messages to a maximum length', '<span class="badge badge-info">bokeh</span> <span class="badge badge-info">python</span> <span class="badge badge-info">interactive-plots</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">visualization</span> <span class="badge badge-info">plotting</span> <span class="badge badge-info">plots</span> <span class="badge badge-info">data-visualisation</span> <span class="badge badge-info">notebooks</span> <span class="badge badge-info">jupyter</span> <span class="badge badge-info">visualisation</span> <span class="badge badge-info">numfocus</span>', '<div id=pop95>bokeh/bokeh</div>', 10, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/11874 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'MSSQLToBigQuery incorrect type conversion bit-->integer', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop96>apache/airflow</div>', 3, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/certbot/certbot/issues/3874 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Refactor tests to patch more correctly', '<span class="badge badge-info">acme</span> <span class="badge badge-info">acme-client</span> <span class="badge badge-info">certbot</span> <span class="badge badge-info">certificate</span> <span class="badge badge-info">letsencrypt</span> <span class="badge badge-info">python</span>', '<div id=pop97>certbot/certbot</div>', 4, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/apache/airflow/issues/12035 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Support AssumeRoleWithWebIdentity for AWS provider', '<span class="badge badge-info">airflow</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">apache-airflow</span> <span class="badge badge-info">python</span> <span class="badge badge-info">scheduler</span> <span class="badge badge-info">workflow</span>', '<div id=pop98>apache/airflow</div>', 10, '<span class="badge badge-info">Python</span>'], ['<div><a href=https://github.com/zulip/zulip/issues/5273 target="_blank">🔗</a><span class="open_desc_modal"> 🔎</span></div>', 'Make the UI for setting a bot as an incoming webhook bot more fancy/self-explanatory', '<span class="badge badge-info">zulip</span> <span class="badge badge-info">python</span> <span class="badge badge-info">python3</span> <span class="badge badge-info">chat</span> <span class="badge badge-info">javascript</span> <span class="badge badge-info">collaboration</span> <span class="badge badge-info">slack</span> <span class="badge badge-info">foss</span> <span class="badge badge-info">free</span> <span class="badge badge-info">apache</span> <span class="badge badge-info">react-native</span> <span class="badge badge-info">electron</span>', '<div id=pop99>zulip/zulip</div>', 25, '<span class="badge badge-info">Python</span>']];
            var links = [['https://github.com/apache/airflow/issues/11673'], ['https://github.com/zulip/zulip/issues/17716'], ['https://github.com/zulip/zulip/issues/17731'], ['https://github.com/pandas-dev/pandas/issues/38990'], ['https://github.com/zulip/zulip/issues/18062'], ['https://github.com/matplotlib/matplotlib/issues/8863'], ['https://github.com/dask/dask/issues/4855'], ['https://github.com/SFTtech/openage/issues/815'], ['https://github.com/apache/superset/issues/13419'], ['https://github.com/streamlit/streamlit/issues/2953'], ['https://github.com/saltstack/salt/issues/19958'], ['https://github.com/mlflow/mlflow/issues/2931'], ['https://github.com/apache/airflow/issues/15005'], ['https://github.com/saltstack/salt/issues/53529'], ['https://github.com/bokeh/bokeh/issues/10376'], ['https://github.com/scipy/scipy/issues/8485'], ['https://github.com/apache/superset/issues/13542'], ['https://github.com/PyTorchLightning/pytorch-lightning/issues/5001'], ['https://github.com/ray-project/ray/issues/9316'], ['https://github.com/SFTtech/openage/issues/1273'], ['https://github.com/apache/airflow/issues/12012'], ['https://github.com/mlflow/mlflow/issues/315'], ['https://github.com/apache/superset/issues/13386'], ['https://github.com/apache/airflow/issues/12401'], ['https://github.com/streamlit/streamlit/issues/1880'], ['https://github.com/scikit-learn/scikit-learn/issues/13117'], ['https://github.com/ray-project/ray/issues/14998'], ['https://github.com/ray-project/ray/issues/5639'], ['https://github.com/saltstack/salt/issues/56645'], ['https://github.com/scrapy/scrapy/issues/4991'], ['https://github.com/bokeh/bokeh/issues/10028'], ['https://github.com/matplotlib/matplotlib/issues/7840'], ['https://github.com/psf/requests-html/issues/226'], ['https://github.com/falconry/falcon/issues/1853'], ['https://github.com/bokeh/bokeh/issues/9223'], ['https://github.com/python-poetry/poetry/issues/3060'], ['https://github.com/huggingface/transformers/issues/11246'], ['https://github.com/PyTorchLightning/pytorch-lightning/issues/5925'], ['https://github.com/scrapy/scrapy/issues/4842'], ['https://github.com/zulip/zulip/issues/17929'], ['https://github.com/apache/airflow/issues/15269'], ['https://github.com/zulip/zulip/issues/18319'], ['https://github.com/ray-project/ray/issues/13465'], ['https://github.com/ansible/awx/issues/3988'], ['https://github.com/ipython/ipython/issues/9891'], ['https://github.com/networkx/networkx/issues/3443'], ['https://github.com/saltstack/salt/issues/55983'], ['https://github.com/pandas-dev/pandas/issues/38482'], ['https://github.com/apache/superset/issues/10004'], ['https://github.com/zulip/zulip/issues/14732'], ['https://github.com/apache/superset/issues/13252'], ['https://github.com/PyTorchLightning/pytorch-lightning/issues/6295'], ['https://github.com/python-poetry/poetry/issues/3055'], ['https://github.com/pandas-dev/pandas/issues/30232'], ['https://github.com/dask/dask/issues/6659'], ['https://github.com/apache/superset/issues/13579'], ['https://github.com/PyTorchLightning/pytorch-lightning/issues/5023'], ['https://github.com/apache/airflow/issues/14392'], ['https://github.com/matplotlib/matplotlib/issues/19791'], ['https://github.com/scikit-learn/scikit-learn/issues/19781'], ['https://github.com/certbot/certbot/issues/4338'], ['https://github.com/ipython/ipython/issues/2569'], ['https://github.com/falconry/falcon/issues/1457'], ['https://github.com/zulip/zulip/issues/4549'], ['https://github.com/apache/airflow/issues/14758'], ['https://github.com/saltstack/salt/issues/52977'], ['https://github.com/scrapy/scrapy/issues/2733'], ['https://github.com/PyTorchLightning/pytorch-lightning/issues/5434'], ['https://github.com/scikit-learn/scikit-learn/issues/18837'], ['https://github.com/pandas-dev/pandas/issues/29687'], ['https://github.com/google/jax/issues/5226'], ['https://github.com/python-poetry/poetry/issues/3163'], ['https://github.com/ray-project/ray/issues/12325'], ['https://github.com/pandas-dev/pandas/issues/40250'], ['https://github.com/huggingface/transformers/issues/9455'], ['https://github.com/huggingface/transformers/issues/8944'], ['https://github.com/scrapy/scrapy/issues/4964'], ['https://github.com/SFTtech/openage/issues/1288'], ['https://github.com/dask/dask/issues/6067'], ['https://github.com/pandas-dev/pandas/issues/38627'], ['https://github.com/bokeh/bokeh/issues/10902'], ['https://github.com/zulip/zulip/issues/16080'], ['https://github.com/pypa/pipenv/issues/2553'], ['https://github.com/bokeh/bokeh/issues/10756'], ['https://github.com/saltstack/salt/issues/59853'], ['https://github.com/apache/airflow/issues/15016'], ['https://github.com/ray-project/ray/issues/7639'], ['https://github.com/matplotlib/matplotlib/issues/19574'], ['https://github.com/pypa/pipenv/issues/3454'], ['https://github.com/certbot/certbot/issues/6284'], ['https://github.com/scrapy/scrapy/issues/3628'], ['https://github.com/huggingface/transformers/issues/11307'], ['https://github.com/saltstack/salt/issues/56579'], ['https://github.com/pandas-dev/pandas/issues/29960'], ['https://github.com/apache/superset/issues/12533'], ['https://github.com/bokeh/bokeh/issues/5464'], ['https://github.com/apache/airflow/issues/11874'], ['https://github.com/certbot/certbot/issues/3874'], ['https://github.com/apache/airflow/issues/12035'], ['https://github.com/zulip/zulip/issues/5273']];
            var repo_desc = [['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Zulip server and webapp - powerful open source team chat'], ['Zulip server and webapp - powerful open source team chat'], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['Zulip server and webapp - powerful open source team chat'], ['matplotlib: plotting with Python'], ['Parallel computing with task scheduling'], ['Free (as in freedom) open source clone of the Age of Empires II engine :rocket:'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['Streamlit — The fastest way to build data apps in Python'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Open source platform for the machine learning lifecycle'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Interactive Data Visualization in the browser, from  Python'], ['Scipy library main repository'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.'], ['An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.'], ['Free (as in freedom) open source clone of the Age of Empires II engine :rocket:'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Open source platform for the machine learning lifecycle'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Streamlit — The fastest way to build data apps in Python'], ['scikit-learn: machine learning in Python'], ['An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.'], ['An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Scrapy, a fast high-level web crawling & scraping framework for Python.'], ['Interactive Data Visualization in the browser, from  Python'], ['matplotlib: plotting with Python'], ['Pythonic HTML Parsing for Humans™'], ['The no-nonsense, minimalist REST and app backend framework for Python developers, with a focus on reliability, correctness, and performance at scale.'], ['Interactive Data Visualization in the browser, from  Python'], ['Python dependency management and packaging made easy.'], ['🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'], ['The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.'], ['Scrapy, a fast high-level web crawling & scraping framework for Python.'], ['Zulip server and webapp - powerful open source team chat'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Zulip server and webapp - powerful open source team chat'], ['An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.'], ['AWX Project'], ['Official repository for IPython itself. Other repos in the IPython organization contain things like the website, documentation builds, etc.'], ['Network Analysis in Python'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['Zulip server and webapp - powerful open source team chat'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.'], ['Python dependency management and packaging made easy.'], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['Parallel computing with task scheduling'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['matplotlib: plotting with Python'], ['scikit-learn: machine learning in Python'], ["Certbot is EFF's tool to obtain certs from Let's Encrypt and (optionally) auto-enable HTTPS on your server.  It can also act as a client for any other CA that uses the ACME protocol."], ['Official repository for IPython itself. Other repos in the IPython organization contain things like the website, documentation builds, etc.'], ['The no-nonsense, minimalist REST and app backend framework for Python developers, with a focus on reliability, correctness, and performance at scale.'], ['Zulip server and webapp - powerful open source team chat'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Scrapy, a fast high-level web crawling & scraping framework for Python.'], ['The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.'], ['scikit-learn: machine learning in Python'], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more'], ['Python dependency management and packaging made easy.'], ['An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.'], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'], ['🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'], ['Scrapy, a fast high-level web crawling & scraping framework for Python.'], ['Free (as in freedom) open source clone of the Age of Empires II engine :rocket:'], ['Parallel computing with task scheduling'], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['Interactive Data Visualization in the browser, from  Python'], ['Zulip server and webapp - powerful open source team chat'], [' Python Development Workflow for Humans.'], ['Interactive Data Visualization in the browser, from  Python'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.'], ['matplotlib: plotting with Python'], [' Python Development Workflow for Humans.'], ["Certbot is EFF's tool to obtain certs from Let's Encrypt and (optionally) auto-enable HTTPS on your server.  It can also act as a client for any other CA that uses the ACME protocol."], ['Scrapy, a fast high-level web crawling & scraping framework for Python.'], ['🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'], ['Software to automate the management and configuration of any infrastructure or application at scale. Get access to the Salt software package repository here: '], ['Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more'], ['Apache Superset is a Data Visualization and Data Exploration Platform'], ['Interactive Data Visualization in the browser, from  Python'], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ["Certbot is EFF's tool to obtain certs from Let's Encrypt and (optionally) auto-enable HTTPS on your server.  It can also act as a client for any other CA that uses the ACME protocol."], ['Apache Airflow - A platform to programmatically author, schedule, and monitor workflows'], ['Zulip server and webapp - powerful open source team chat']];
            var stars = [[21391], [13463], [13463], [29590], [13463], [13583], [8290], [10292], [38372], [14419], [11710], [9283], [21391], [11710], [15066], [8213], [38372], [13328], [15831], [10292], [21391], [9283], [38372], [21391], [14419], [45644], [15831], [15831], [11710], [40518], [15066], [13583], [11843], [8371], [15066], [14933], [45574], [13328], [40518], [13463], [21391], [13463], [15831], [9605], [14786], [9033], [11710], [29590], [38372], [13463], [38372], [13328], [14933], [29590], [8290], [38372], [13328], [21391], [13583], [45644], [27944], [14786], [8371], [13463], [21391], [11710], [40518], [13328], [45644], [29590], [12558], [14933], [15831], [29590], [45574], [45574], [40518], [10292], [8290], [29590], [15066], [13463], [21837], [15066], [11710], [21391], [15831], [13583], [21837], [27944], [40518], [45574], [11710], [29590], [38372], [15066], [21391], [27944], [21391], [13463]];

            console.log(data[0])

            var sp = $('#spreadsheet').jexcel({
                data: data,
                csvHeaders: false,
                search: false,
                tableOverflow: true,
                tableWidth: "96vw",
                tableHeight: "80vh",
                defaultColWidth: "100vw",
                minDimensions: [7, 50],
                rowResize: true,
                colHeaders: [
                    '',
                    'Issue',
                    'Topics',
                    'Repository',
                    '# Comments',
                    'Language',
                    '',
                ],
                colWidths: [70, 700, 1000, 200, 110, 100, 2000],
                colAlignments: ['left', 'left', 'left', 'left', 'left', 'left', 'left'],
                columns: [{
                    type: 'html',
                    readOnly: true
                }, {
                    type: 'html',
                    readOnly: true
                }, {
                    type: 'html',
                    readOnly: true
                }, {
                    type: 'html',
                    readOnly: true
                }, {
                    type: 'html',
                    readOnly: true
                }, {
                    type: 'html',
                    readOnly: true
                }, {
                    type: 'html',
                    readOnly: true
                }, ]
            });

            var data_all;
            data_all = sp.getData();


            $(document).ready(function() {


                $(function() {
                    $('[data-toggle="popover"]').popover()
                });

                for (i = 0; i < repo_desc.length; i++) {
                    $('#pop' + i).popover({
                        trigger: 'hover',
                        content: repo_desc[i][0] + '\n' + '⭐' + stars[i].toLocaleString()
                    });
                };

                sp.headers[0].innerHTML = '';
                sp.headers[6].innerHTML = '';
                document.querySelector(".jexcel_content").style.removeProperty("box-shadow");

            });

            var converter = new showdown.Converter();

            $('.open_desc_modal').click(function() {

                var cell = sp.selectedCell;
                var rowix = cell[1];
                var rowData = sp.getRowData(rowix);
                var description = converter.makeHtml(desc[rowix][0]);

                document.getElementById("mtitle").innerHTML = rowData[1];
                document.getElementById("mbody").innerHTML = (
                    '<h6><small class="text-muted" id="msubcat">Link</small><h6>' +
                    '<p style="padding-bottom:30px;font-weight:lighter"><a href="' + links[rowix] + '", target="_blank">' + links[rowix] + '</a></p>' +
                    '<h6><small class="text-muted" id="msubcat">Topics</small><h6>' +
                    '<p style="padding-bottom:30px;">' + rowData[2] + '</p>' +
                    '<h6><small class="text-muted" id="msubcat">Repository</small><h6>' +
                    '<p style="padding-bottom:5px;font-weight:lighter">' + rowData[3].replace(/(<([^>]+)>)/gi, "") + '&nbsp;&nbsp;&nbsp;⭐ ' + stars[rowix].toLocaleString() + '</p>' +
                    '<p style="padding-bottom:30px;font-weight:lighter">' + repo_desc[rowix] + '\n' + '</p>' +
                    '<h6><small class="text-muted" id="msubcat">Language</small><h6>' +
                    '<p style="padding-bottom:30px;">' + rowData[5] + '</p>' +
                    '<h6><small class="text-muted" id="msubcat"># Comments</small><h6>' +
                    '<p style="padding-bottom:30px;font-weight:lighter">' + rowData[4] + '</p>' +
                    '<h6><small class="text-muted" id="msubcat">Description</small><h6>' +
                    '<div style="padding-bottom:30px;font-weight:lighter;padding-top:-30px;">' + description + '</div>'
                )

                $('#desc_modal').modal({
                    'show': true,
                    // 'backdrop': 'static',
                });

            });
        </script>

        <script>
        </script>

</body>

</html>